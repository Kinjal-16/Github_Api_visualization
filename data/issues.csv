,url,repository_url,labels_url,comments_url,events_url,html_url,id,node_id,number,title,labels,state,locked,assignee,assignees,milestone,comments,created_at,updated_at,closed_at,author_association,active_lock_reason,draft,body,timeline_url,performed_via_github_app,repo_name,owner,user.login,user.id,user.node_id,user.avatar_url,user.gravatar_id,user.url,user.html_url,user.followers_url,user.following_url,user.gists_url,user.starred_url,user.subscriptions_url,user.organizations_url,user.repos_url,user.events_url,user.received_events_url,user.type,user.site_admin,pull_request.url,pull_request.html_url,pull_request.diff_url,pull_request.patch_url,pull_request.merged_at,reactions.url,reactions.total_count,reactions.+1,reactions.-1,reactions.laugh,reactions.hooray,reactions.confused,reactions.heart,reactions.rocket,reactions.eyes,assignee.login,assignee.id,assignee.node_id,assignee.avatar_url,assignee.gravatar_id,assignee.url,assignee.html_url,assignee.followers_url,assignee.following_url,assignee.gists_url,assignee.starred_url,assignee.subscriptions_url,assignee.organizations_url,assignee.repos_url,assignee.events_url,assignee.received_events_url,assignee.type,assignee.site_admin
0,https://api.github.com/repos/apache/spark/issues/35082,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/35082/labels{/name},https://api.github.com/repos/apache/spark/issues/35082/comments,https://api.github.com/repos/apache/spark/issues/35082/events,https://github.com/apache/spark/pull/35082,1091550722,PR_kwDOAQXtWs4wa4qQ,35082,[SPARK-37677][PYTHON] Decompress the ZIP file and grant the executable permission to the file,"[{'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,0,2021-12-31T12:18:09Z,2021-12-31T12:37:40Z,,NONE,,False,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
`Unpacking an archive s3a://zjx/python3.6.9.zip#python3 from /tmp/spark-d861d788-bd72-4d3c-88fc-8f79b30b081d/python3.6.9.zip to /opt/spark/work-dir/./python3
Exception in thread ""main"" java.io.IOException: Cannot run program ""python3/bin/python3"": error=13, Permission denied
	at java.base/java.lang.ProcessBuilder.start(Unknown Source)
	at java.base/java.lang.ProcessBuilder.start(Unknown Source)
	at org.apache.spark.deploy.PythonRunner$.main(PythonRunner.scala:97)
	at org.apache.spark.deploy.PythonRunner.main(PythonRunner.scala)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
	at java.base/java.lang.reflect.Method.invoke(Unknown Source)
	at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
	at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:955)
	at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)
	at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)
	at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)
	at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1045)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1054)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Caused by: java.io.IOException: error=13, Permission denied
	at java.base/java.lang.ProcessImpl.forkAndExec(Native Method)
	at java.base/java.lang.ProcessImpl.<init>(Unknown Source)
	at java.base/java.lang.ProcessImpl.start(Unknown Source)
	... 16 more`
When we set parameter ""--archives hdfs:///user/zjx/python3.6.9.zip"" to submit the Spark job, driver will unzip it, but it will lost executable permission after unzipping, so we should keep those permission. In this PR, I add the executable permission for all file after unzipping. It may cost some time to unzip that, but the time cost is controllable.
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->



### Why are the changes needed?
When we submit a py job and want to use our own version of Python，we may add ""--archives hdfs:///user/zjx/python3.6.9.zip"" ,after driver unzip this file, and want to execute the program using python3, it will report permission denied. So we should add executable permission to those script.
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->


### Does this PR introduce _any_ user-facing change?
After the fix, the zip package submitted by the user for Python will run normally
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->


### How was this patch tested?
First, I tested python3.6.9 and Python3.9. I compiled Python3.6.9 (which contains no other third party dependencies) and Python3.6.9 (which contains many third party dependencies) to include 8398 and 63570 files respectively and compressed them into a ZIP file, The zip file upload is then specified in spark-Submit, and the execution permission process is logged to record the time spent. The conclusion is that python3.9.zip took 50 to 65 milliseconds to find permissions, and the other one took 600 to 800 milliseconds.Then upload and test python3.6.9.zip and python3.6.9.tgz 100 times and find that the average unzip time of ZIP file is 15637.45 milliseconds, while the unzip time of TGZ file is 15758.78 milliseconds。That is to say, counting the time of adding execution permission, Zip also decompresses faster than TGZ.

<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.
-->
",https://api.github.com/repos/apache/spark/issues/35082/timeline,,spark,apache,zhongjingxiong,84573424,MDQ6VXNlcjg0NTczNDI0,https://avatars.githubusercontent.com/u/84573424?v=4,,https://api.github.com/users/zhongjingxiong,https://github.com/zhongjingxiong,https://api.github.com/users/zhongjingxiong/followers,https://api.github.com/users/zhongjingxiong/following{/other_user},https://api.github.com/users/zhongjingxiong/gists{/gist_id},https://api.github.com/users/zhongjingxiong/starred{/owner}{/repo},https://api.github.com/users/zhongjingxiong/subscriptions,https://api.github.com/users/zhongjingxiong/orgs,https://api.github.com/users/zhongjingxiong/repos,https://api.github.com/users/zhongjingxiong/events{/privacy},https://api.github.com/users/zhongjingxiong/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/35082,https://github.com/apache/spark/pull/35082,https://github.com/apache/spark/pull/35082.diff,https://github.com/apache/spark/pull/35082.patch,,https://api.github.com/repos/apache/spark/issues/35082/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
1,https://api.github.com/repos/apache/spark/issues/35080,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/35080/labels{/name},https://api.github.com/repos/apache/spark/issues/35080/comments,https://api.github.com/repos/apache/spark/issues/35080/events,https://github.com/apache/spark/pull/35080,1091503003,PR_kwDOAQXtWs4wausK,35080,[SPARK-37792][CORE] Fix the check of custom configuration in SparkShellLoggingFilter,"[{'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,1,2021-12-31T09:55:12Z,2021-12-31T18:20:53Z,,MEMBER,,False,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->

This patch fixes the way we check if the log came from root or some custom configuration in `SparkShellLoggingFilter`.

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->

Previously in log4j 1.x the way we check if the log came from root or some custom configuration in `SparkShellLoggingFilter` does not work with log4j 2 now.

### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->

No

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.
-->

Manual test",https://api.github.com/repos/apache/spark/issues/35080/timeline,,spark,apache,viirya,68855,MDQ6VXNlcjY4ODU1,https://avatars.githubusercontent.com/u/68855?v=4,,https://api.github.com/users/viirya,https://github.com/viirya,https://api.github.com/users/viirya/followers,https://api.github.com/users/viirya/following{/other_user},https://api.github.com/users/viirya/gists{/gist_id},https://api.github.com/users/viirya/starred{/owner}{/repo},https://api.github.com/users/viirya/subscriptions,https://api.github.com/users/viirya/orgs,https://api.github.com/users/viirya/repos,https://api.github.com/users/viirya/events{/privacy},https://api.github.com/users/viirya/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/35080,https://github.com/apache/spark/pull/35080,https://github.com/apache/spark/pull/35080.diff,https://github.com/apache/spark/pull/35080.patch,,https://api.github.com/repos/apache/spark/issues/35080/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
2,https://api.github.com/repos/apache/spark/issues/35079,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/35079/labels{/name},https://api.github.com/repos/apache/spark/issues/35079/comments,https://api.github.com/repos/apache/spark/issues/35079/events,https://github.com/apache/spark/pull/35079,1091495925,PR_kwDOAQXtWs4watQR,35079,[SPARK-30789][SQL][DOCS][FOLLOWUP] Add document for syntax `(IGNORE | RESPECT) NULLS`,"[{'id': 1981471081, 'node_id': 'MDU6TGFiZWwxOTgxNDcxMDgx', 'url': 'https://api.github.com/repos/apache/spark/labels/DOCS', 'name': 'DOCS', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,0,2021-12-31T09:33:02Z,2021-12-31T12:14:59Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?
https://github.com/apache/spark/pull/30943 supports syntax `(IGNORE | RESPECT) NULLS for LEAD/LAG/NTH_VALUE/FIRST_VALUE/LAST_VALUE`, but update document.
The screen snapshot
![screenshot-20211231-174803](https://user-images.githubusercontent.com/8486025/147816336-debca074-0b84-48e8-9ed2-cb13f562cf12.png)


This PR adds document for syntax `(IGNORE | RESPECT) NULLS`


### Why are the changes needed?
Add document for syntax `(IGNORE | RESPECT) NULLS`


### Does this PR introduce _any_ user-facing change?
'Yes'.
The screen snapshot
![screenshot-20211231-195656](https://user-images.githubusercontent.com/8486025/147822144-e0d878a5-739f-4623-aa10-0121017812e6.png)

![screenshot-20211231-195741](https://user-images.githubusercontent.com/8486025/147822166-8a4f5f2d-8e68-4a9f-a3a8-a2c434b092e2.png)


### How was this patch tested?
Manual check.
",https://api.github.com/repos/apache/spark/issues/35079/timeline,,spark,apache,beliefer,8486025,MDQ6VXNlcjg0ODYwMjU=,https://avatars.githubusercontent.com/u/8486025?v=4,,https://api.github.com/users/beliefer,https://github.com/beliefer,https://api.github.com/users/beliefer/followers,https://api.github.com/users/beliefer/following{/other_user},https://api.github.com/users/beliefer/gists{/gist_id},https://api.github.com/users/beliefer/starred{/owner}{/repo},https://api.github.com/users/beliefer/subscriptions,https://api.github.com/users/beliefer/orgs,https://api.github.com/users/beliefer/repos,https://api.github.com/users/beliefer/events{/privacy},https://api.github.com/users/beliefer/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/35079,https://github.com/apache/spark/pull/35079,https://github.com/apache/spark/pull/35079.diff,https://github.com/apache/spark/pull/35079.patch,,https://api.github.com/repos/apache/spark/issues/35079/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
3,https://api.github.com/repos/apache/spark/issues/35078,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/35078/labels{/name},https://api.github.com/repos/apache/spark/issues/35078/comments,https://api.github.com/repos/apache/spark/issues/35078/events,https://github.com/apache/spark/pull/35078,1091436692,PR_kwDOAQXtWs4wahC0,35078,[SPARK-37796][SQL] ByteArrayMethods arrayEquals should fast skip the check of aligning with unaligned platform,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,1,2021-12-31T06:17:10Z,2021-12-31T15:59:07Z,,CONTRIBUTOR,,False,"
<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->
The method `arrayEquals` in `ByteArrayMethods` is critical function which is used in `UTF8String.` `equals`, `indexOf`,`find` etc.

After SPARK-16962, it add the complexity of aligned. It would be better to fast sikip the check of aligning if the platform is unaligned.

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->
Improve the performance.

### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->
no

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.
-->
Pass CI. Run the benchmark using [unaligned-benchmark](https://github.com/ulysses-you/spark/commit/d14d4bfcfeddcf90ccfe7cc3f6cda426d6d6b7e5), and here is the benchmark result:

[JDK8](https://github.com/ulysses-you/spark/actions/runs/1639852573)
```
================================================================================================
byte array equals
================================================================================================

OpenJDK 64-Bit Server VM 1.8.0_312-b07 on Linux 5.11.0-1022-azure
Intel(R) Xeon(R) Platinum 8272CL CPU @ 2.60GHz
Byte Array equals:                        Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)   Relative
------------------------------------------------------------------------------------------------------------------------
Byte Array equals fast                             1322           2222         NaN        121.0           8.3       1.0X
Byte Array equals                                  3378           3381           3         47.4          21.1       0.4X
```

[JDK11](https://github.com/ulysses-you/spark/actions/runs/1639853330)
```
================================================================================================
byte array equals
================================================================================================

OpenJDK 64-Bit Server VM 11.0.13+8-LTS on Linux 5.11.0-1022-azure
Intel(R) Xeon(R) Platinum 8272CL CPU @ 2.60GHz
Byte Array equals:                        Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)   Relative
------------------------------------------------------------------------------------------------------------------------
Byte Array equals fast                             1860           1891          15         86.0          11.6       1.0X
Byte Array equals                                  2913           2921           8         54.9          18.2       0.6X
```

[JDK17](https://github.com/ulysses-you/spark/actions/runs/1639853938)
```
================================================================================================
byte array equals
================================================================================================

OpenJDK 64-Bit Server VM 17.0.1+12-LTS on Linux 5.11.0-1022-azure
Intel(R) Xeon(R) Platinum 8171M CPU @ 2.60GHz
Byte Array equals:                        Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)   Relative
------------------------------------------------------------------------------------------------------------------------
Byte Array equals fast                             1543           1602          39        103.7           9.6       1.0X
Byte Array equals                                  3027           3029           1         52.9          18.9       0.5X
```
",https://api.github.com/repos/apache/spark/issues/35078/timeline,,spark,apache,ulysses-you,12025282,MDQ6VXNlcjEyMDI1Mjgy,https://avatars.githubusercontent.com/u/12025282?v=4,,https://api.github.com/users/ulysses-you,https://github.com/ulysses-you,https://api.github.com/users/ulysses-you/followers,https://api.github.com/users/ulysses-you/following{/other_user},https://api.github.com/users/ulysses-you/gists{/gist_id},https://api.github.com/users/ulysses-you/starred{/owner}{/repo},https://api.github.com/users/ulysses-you/subscriptions,https://api.github.com/users/ulysses-you/orgs,https://api.github.com/users/ulysses-you/repos,https://api.github.com/users/ulysses-you/events{/privacy},https://api.github.com/users/ulysses-you/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/35078,https://github.com/apache/spark/pull/35078,https://github.com/apache/spark/pull/35078.diff,https://github.com/apache/spark/pull/35078.patch,,https://api.github.com/repos/apache/spark/issues/35078/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
4,https://api.github.com/repos/apache/spark/issues/35077,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/35077/labels{/name},https://api.github.com/repos/apache/spark/issues/35077/comments,https://api.github.com/repos/apache/spark/issues/35077/events,https://github.com/apache/spark/pull/35077,1091414553,PR_kwDOAQXtWs4wacg6,35077,[SPARK-37795][BUILD] Add a scalastyle rule to ban `org.apache.log4j` imports,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1406627200, 'node_id': 'MDU6TGFiZWwxNDA2NjI3MjAw', 'url': 'https://api.github.com/repos/apache/spark/labels/BUILD', 'name': 'BUILD', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1427970157, 'node_id': 'MDU6TGFiZWwxNDI3OTcwMTU3', 'url': 'https://api.github.com/repos/apache/spark/labels/YARN', 'name': 'YARN', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,1,2021-12-31T04:40:52Z,2021-12-31T16:14:31Z,,MEMBER,,True,"### What changes were proposed in this pull request?
This PR aims to add a new checkstyle rule to ban `org.apache.log4j` imports. 


### Why are the changes needed?
This will help us remove the log4j bridge in the future. 

### Does this PR introduce _any_ user-facing change?
No.


### How was this patch tested?
Pass the CIs. ",https://api.github.com/repos/apache/spark/issues/35077/timeline,,spark,apache,williamhyun,62487364,MDQ6VXNlcjYyNDg3MzY0,https://avatars.githubusercontent.com/u/62487364?v=4,,https://api.github.com/users/williamhyun,https://github.com/williamhyun,https://api.github.com/users/williamhyun/followers,https://api.github.com/users/williamhyun/following{/other_user},https://api.github.com/users/williamhyun/gists{/gist_id},https://api.github.com/users/williamhyun/starred{/owner}{/repo},https://api.github.com/users/williamhyun/subscriptions,https://api.github.com/users/williamhyun/orgs,https://api.github.com/users/williamhyun/repos,https://api.github.com/users/williamhyun/events{/privacy},https://api.github.com/users/williamhyun/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/35077,https://github.com/apache/spark/pull/35077,https://github.com/apache/spark/pull/35077.diff,https://github.com/apache/spark/pull/35077.patch,,https://api.github.com/repos/apache/spark/issues/35077/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
5,https://api.github.com/repos/apache/spark/issues/35076,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/35076/labels{/name},https://api.github.com/repos/apache/spark/issues/35076/comments,https://api.github.com/repos/apache/spark/issues/35076/events,https://github.com/apache/spark/pull/35076,1091411846,PR_kwDOAQXtWs4wab-B,35076,[SPARK-37793][CORE][SHUFFLE] Fallback to fetch original blocks when noLocalMergedBlockDataError,"[{'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,2,2021-12-31T04:29:31Z,2021-12-31T21:20:51Z,,MEMBER,,False,"### What changes were proposed in this pull request?

When enable push-based shuffle, there is a chance that task hang at 

```
59  Executor task launch worker for task 424.0 in stage 753.0 (TID 106778)
WAITING	Lock(java.util.concurrent.ThreadPoolExecutor$Worker@1660371198})
  sun.misc.Unsafe.park(Native Method)
  java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
  java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2044)
  java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
  org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:756)
  org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:85)
  org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)
  scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)
  scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)
  scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
  org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)
  org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
  scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
  org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage6.sort_addToSorter_0$(Unknown Source)
  org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage6.processNext(Unknown Source)
  org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
  org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)
  org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage7.smj_findNextJoinRows_0$(Unknown Source)
  org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage7.hashAgg_doAggregateWithKeys_1$(Unknown Source)
  org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage7.hashAgg_doAggregateWithKeys_0$(Unknown Source)
  org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage7.hashAgg_doAggregateWithoutKey_0$(Unknown Source)
  org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage7.processNext(Unknown Source)
  org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
  org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$2.hasNext(WholeStageCodegenExec.scala:779)
  scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
  org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)
  org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)
  org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
  org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)
  org.apache.spark.scheduler.Task.run(Task.scala:136)
  org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:507)
  org.apache.spark.executor.Executor$TaskRunner$$Lambda$518/852390142.apply(Unknown Source)
  org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1470)
  org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:510)
  java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
  java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
  java.lang.Thread.run(Thread.java:748)
```

And `org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:756)` is 
```
while (result == null) {
  ...
  result = results.take() // line 756
  ...
}
```

After some investigations, found that the last `FetchResult` put into `result` is `PushMergedLocalMetaFetchResult`, and there is a chance that `bufs` is empty, will cause no `SuccessFetchResult` be added to `results`, and thread hang if no other `FetchResult` is put into `results`.

```scala
while (result == null) {
  ...
  result = results.take()
  ...

  result match {
    case r @ SuccessFetchResult(blockId, mapIndex, address, size, buf, isNetworkReqDone) =>
      ...
      case PushMergedLocalMetaFetchResult(
        shuffleId, shuffleMergeId, reduceId, bitmaps, localDirs) =>
        val shuffleBlockId = ShuffleMergedBlockId(shuffleId, shuffleMergeId, reduceId)
        try {
          val bufs: Seq[ManagedBuffer] = blockManager.getLocalMergedBlockData(shuffleBlockId,
            localDirs)
          // THERE IS A CHANCE THAT bufs.isEmpty!
          ...
          bufs.zipWithIndex.foreach { case (buf, chunkId) =>
            buf.retain()
            val shuffleChunkId = ShuffleBlockChunkId(shuffleId, shuffleMergeId, reduceId,
              chunkId)
            pushBasedFetchHelper.addChunk(shuffleChunkId, bitmaps(chunkId))
            results.put(SuccessFetchResult(shuffleChunkId, SHUFFLE_PUSH_MAP_ID,
              pushBasedFetchHelper.localShuffleMergerBlockMgrId, buf.size(), buf,
              isNetworkReqDone = false))
          }
        } catch {
          case e: Exception =>
            pushBasedFetchHelper.initiateFallbackFetchForPushMergedBlock(
              shuffleBlockId, pushBasedFetchHelper.localShuffleMergerBlockMgrId)
        }
        result = null
    ...
  }
}
```

### Why are the changes needed?

Fallback to fetch original blocks when noLocalMergedBlockDataError to avoid task hang.

### Does this PR introduce _any_ user-facing change?
Bug fix, to make push-based shuffle more stable.

### How was this patch tested?
Pass 1T TPC-DS tests",https://api.github.com/repos/apache/spark/issues/35076/timeline,,spark,apache,pan3793,26535726,MDQ6VXNlcjI2NTM1NzI2,https://avatars.githubusercontent.com/u/26535726?v=4,,https://api.github.com/users/pan3793,https://github.com/pan3793,https://api.github.com/users/pan3793/followers,https://api.github.com/users/pan3793/following{/other_user},https://api.github.com/users/pan3793/gists{/gist_id},https://api.github.com/users/pan3793/starred{/owner}{/repo},https://api.github.com/users/pan3793/subscriptions,https://api.github.com/users/pan3793/orgs,https://api.github.com/users/pan3793/repos,https://api.github.com/users/pan3793/events{/privacy},https://api.github.com/users/pan3793/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/35076,https://github.com/apache/spark/pull/35076,https://github.com/apache/spark/pull/35076.diff,https://github.com/apache/spark/pull/35076.patch,,https://api.github.com/repos/apache/spark/issues/35076/reactions,1,1,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
6,https://api.github.com/repos/apache/spark/issues/35071,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/35071/labels{/name},https://api.github.com/repos/apache/spark/issues/35071/comments,https://api.github.com/repos/apache/spark/issues/35071/events,https://github.com/apache/spark/pull/35071,1091295759,PR_kwDOAQXtWs4waEL9,35071,"[SPARK-37788][PYTHON] Update remaining PySpark functions to use ColumnOrName (over Column), where appropriate","[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1982260031, 'node_id': 'MDU6TGFiZWwxOTgyMjYwMDMx', 'url': 'https://api.github.com/repos/apache/spark/labels/PYTHON', 'name': 'PYTHON', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,2,2021-12-30T20:39:53Z,2021-12-31T22:59:05Z,,CONTRIBUTOR,,False,"
### What changes were proposed in this pull request?
Please see https://issues.apache.org/jira/browse/SPARK-37788

There are a few remaining functions that should but don't yet support ColumnOrName; this PR updates some annotations of functions that do support it, and converts input column string names to columns if not being done already.

### Why are the changes needed?
API consistency in PySpark


### Does this PR introduce _any_ user-facing change?
Yes; namely two array functions:

- array_repeat; can now support `df.select(array_repeat(""data"", ""repeat_n"").alias('r'))`
- slice: can now support `df.select(slice(""data"", ""index"", ""length"").alias('r'))`

### How was this patch tested?
Modification to two existing unit tests",https://api.github.com/repos/apache/spark/issues/35071/timeline,,spark,apache,Daniel-Davies,33356828,MDQ6VXNlcjMzMzU2ODI4,https://avatars.githubusercontent.com/u/33356828?v=4,,https://api.github.com/users/Daniel-Davies,https://github.com/Daniel-Davies,https://api.github.com/users/Daniel-Davies/followers,https://api.github.com/users/Daniel-Davies/following{/other_user},https://api.github.com/users/Daniel-Davies/gists{/gist_id},https://api.github.com/users/Daniel-Davies/starred{/owner}{/repo},https://api.github.com/users/Daniel-Davies/subscriptions,https://api.github.com/users/Daniel-Davies/orgs,https://api.github.com/users/Daniel-Davies/repos,https://api.github.com/users/Daniel-Davies/events{/privacy},https://api.github.com/users/Daniel-Davies/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/35071,https://github.com/apache/spark/pull/35071,https://github.com/apache/spark/pull/35071.diff,https://github.com/apache/spark/pull/35071.patch,,https://api.github.com/repos/apache/spark/issues/35071/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
7,https://api.github.com/repos/apache/spark/issues/35070,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/35070/labels{/name},https://api.github.com/repos/apache/spark/issues/35070/comments,https://api.github.com/repos/apache/spark/issues/35070/events,https://github.com/apache/spark/pull/35070,1091160589,PR_kwDOAQXtWs4wZoXZ,35070,[SPARK-37789][SQL] Add a class to represent general aggregate functions in DS V2,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,2,2021-12-30T15:10:38Z,2021-12-31T11:13:57Z,,CONTRIBUTOR,,False,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->
There are a lot of aggregate functions in SQL and it's a lot of work to add them one by one in the DS v2 API. This PR proposes to add a new `GeneralAggregateFunc` class to represent all the general SQL aggregate functions. Since it's general, Spark doesn't know its aggregation buffer and can only push down the aggregation to the source completely.

As an example, this PR also translates `AVG` to `GeneralAggregateFunc` and pushes it to JDBC V2.

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->
To add aggregate functions in DS v2 easier.

### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->
No

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.
-->
JDBC v2 test",https://api.github.com/repos/apache/spark/issues/35070/timeline,,spark,apache,cloud-fan,3182036,MDQ6VXNlcjMxODIwMzY=,https://avatars.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/35070,https://github.com/apache/spark/pull/35070,https://github.com/apache/spark/pull/35070.diff,https://github.com/apache/spark/pull/35070.patch,,https://api.github.com/repos/apache/spark/issues/35070/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
8,https://api.github.com/repos/apache/spark/issues/35069,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/35069/labels{/name},https://api.github.com/repos/apache/spark/issues/35069/comments,https://api.github.com/repos/apache/spark/issues/35069/events,https://github.com/apache/spark/pull/35069,1090956349,PR_kwDOAQXtWs4wY9PY,35069,[SPARK-35714][FOLLOW-UP][CORE] WorkerWatcher should run System.exit in a thread out of RpcEnv ,"[{'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,0,2021-12-30T08:37:46Z,2021-12-30T08:37:59Z,,MEMBER,,False,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->

This PR proposes to let `WorkerWatcher` run `System.exit` in a separate thread instead of some thread of `RpcEnv`.


### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->
`System.exit` will trigger the shutdown hook to run `executor.stop`, which will result in the same deadlock issue with SPARK-14180. But note that since Spark upgrades to Hadoop 3  recently, each hook now will have a [timeout threshold](https://github.com/apache/hadoop/blob/d4794dd3b2ba365a9d95ad6aafcf43a1ea40f777/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/ShutdownHookManager.java#L205-L209) which forcibly interrupt the hook execution once reaches timeout. So, the deadlock issue doesn't really exist in the master branch. However, it's still critical for previous releases and is a wrong behavior that should be fixed.

### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->

No.


### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.
-->

Tested manually.
",https://api.github.com/repos/apache/spark/issues/35069/timeline,,spark,apache,Ngone51,16397174,MDQ6VXNlcjE2Mzk3MTc0,https://avatars.githubusercontent.com/u/16397174?v=4,,https://api.github.com/users/Ngone51,https://github.com/Ngone51,https://api.github.com/users/Ngone51/followers,https://api.github.com/users/Ngone51/following{/other_user},https://api.github.com/users/Ngone51/gists{/gist_id},https://api.github.com/users/Ngone51/starred{/owner}{/repo},https://api.github.com/users/Ngone51/subscriptions,https://api.github.com/users/Ngone51/orgs,https://api.github.com/users/Ngone51/repos,https://api.github.com/users/Ngone51/events{/privacy},https://api.github.com/users/Ngone51/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/35069,https://github.com/apache/spark/pull/35069,https://github.com/apache/spark/pull/35069.diff,https://github.com/apache/spark/pull/35069.patch,,https://api.github.com/repos/apache/spark/issues/35069/reactions,1,1,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
9,https://api.github.com/repos/apache/spark/issues/35068,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/35068/labels{/name},https://api.github.com/repos/apache/spark/issues/35068/comments,https://api.github.com/repos/apache/spark/issues/35068/events,https://github.com/apache/spark/pull/35068,1090906674,PR_kwDOAQXtWs4wYzBd,35068,[SPARK-37770][SQL][FOLLOWUP] Implement the ConstantColumnVector for the metadata columns performance improvements,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,6,2021-12-30T06:35:04Z,2021-12-31T02:35:33Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?
Implement a new column vector named `ConstantColumnVector`, which avoids copying the same data for all rows but storing only one copy of the data.

### Why are the changes needed?
Performance improvements.


### Does this PR introduce _any_ user-facing change?
Np


### How was this patch tested?
TODO
",https://api.github.com/repos/apache/spark/issues/35068/timeline,,spark,apache,Yaohua628,79476540,MDQ6VXNlcjc5NDc2NTQw,https://avatars.githubusercontent.com/u/79476540?v=4,,https://api.github.com/users/Yaohua628,https://github.com/Yaohua628,https://api.github.com/users/Yaohua628/followers,https://api.github.com/users/Yaohua628/following{/other_user},https://api.github.com/users/Yaohua628/gists{/gist_id},https://api.github.com/users/Yaohua628/starred{/owner}{/repo},https://api.github.com/users/Yaohua628/subscriptions,https://api.github.com/users/Yaohua628/orgs,https://api.github.com/users/Yaohua628/repos,https://api.github.com/users/Yaohua628/events{/privacy},https://api.github.com/users/Yaohua628/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/35068,https://github.com/apache/spark/pull/35068,https://github.com/apache/spark/pull/35068.diff,https://github.com/apache/spark/pull/35068.patch,,https://api.github.com/repos/apache/spark/issues/35068/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
10,https://api.github.com/repos/apache/spark/issues/35067,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/35067/labels{/name},https://api.github.com/repos/apache/spark/issues/35067/comments,https://api.github.com/repos/apache/spark/issues/35067/events,https://github.com/apache/spark/pull/35067,1090858330,PR_kwDOAQXtWs4wYo-P,35067,[SPARK-37423][PYTHON] Inline type hints for fpm.py in python/pyspark/mllib,"[{'id': 1405803268, 'node_id': 'MDU6TGFiZWwxNDA1ODAzMjY4', 'url': 'https://api.github.com/repos/apache/spark/labels/MLLIB', 'name': 'MLLIB', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1982260031, 'node_id': 'MDU6TGFiZWwxOTgyMjYwMDMx', 'url': 'https://api.github.com/repos/apache/spark/labels/PYTHON', 'name': 'PYTHON', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,1,2021-12-30T03:53:40Z,2021-12-30T03:55:05Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?
Inline type hints for fpm.py, test.py in python/pyspark/mllib/


### Why are the changes needed?
We can take advantage of static type checking within the functions by inlining the type hints.

### Does this PR introduce _any_ user-facing change?
No


### How was this patch tested?
Existing tests
",https://api.github.com/repos/apache/spark/issues/35067/timeline,,spark,apache,dchvn,91461145,MDQ6VXNlcjkxNDYxMTQ1,https://avatars.githubusercontent.com/u/91461145?v=4,,https://api.github.com/users/dchvn,https://github.com/dchvn,https://api.github.com/users/dchvn/followers,https://api.github.com/users/dchvn/following{/other_user},https://api.github.com/users/dchvn/gists{/gist_id},https://api.github.com/users/dchvn/starred{/owner}{/repo},https://api.github.com/users/dchvn/subscriptions,https://api.github.com/users/dchvn/orgs,https://api.github.com/users/dchvn/repos,https://api.github.com/users/dchvn/events{/privacy},https://api.github.com/users/dchvn/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/35067,https://github.com/apache/spark/pull/35067,https://github.com/apache/spark/pull/35067.diff,https://github.com/apache/spark/pull/35067.patch,,https://api.github.com/repos/apache/spark/issues/35067/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
11,https://api.github.com/repos/apache/spark/issues/35066,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/35066/labels{/name},https://api.github.com/repos/apache/spark/issues/35066/comments,https://api.github.com/repos/apache/spark/issues/35066/events,https://github.com/apache/spark/pull/35066,1090848526,PR_kwDOAQXtWs4wYnBW,35066,[SPARK-37784][SQL] Correctly handle UDTs in CodeGenerator.addBufferedState(),"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,1,2021-12-30T03:14:19Z,2021-12-30T06:00:05Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?

This PR fixes a correctness issue in the CodeGenerator.addBufferedState() helper method (which is used by the SortMergeJoinExec operator). 

The addBufferedState() method generates code for buffering values that come from a row in an operator's input iterator, performing any necessary copying so that the buffered values remain correct after the input iterator advances to the next row.

The current logic does not correctly handle UDTs: these fall through to the match statement's default branch, causing UDT values to be buffered without copying. This is problematic if the UDT's underlying SQL type is an array, map, struct, or string type (since those types require copying). Failing to copy values can lead to correctness issues or crashes.

This patch's fix is simple: when the dataType is a UDT, use its underlying sqlType for determining whether values need to be copied. I used an existing helper function to perform this type unwrapping.

### Why are the changes needed?

Fix a correctness issue.

### Does this PR introduce _any_ user-facing change?

No.

### How was this patch tested?

I manually tested this change by re-running a workload which failed with a segfault prior to this patch. See JIRA for more details: https://issues.apache.org/jira/browse/SPARK-37784

So far I have been unable to come up with a CI-runnable regression test which would have failed prior to this change (my only working reproduction runs in a pre-production environment and does not fail in my development environment).",https://api.github.com/repos/apache/spark/issues/35066/timeline,,spark,apache,JoshRosen,50748,MDQ6VXNlcjUwNzQ4,https://avatars.githubusercontent.com/u/50748?v=4,,https://api.github.com/users/JoshRosen,https://github.com/JoshRosen,https://api.github.com/users/JoshRosen/followers,https://api.github.com/users/JoshRosen/following{/other_user},https://api.github.com/users/JoshRosen/gists{/gist_id},https://api.github.com/users/JoshRosen/starred{/owner}{/repo},https://api.github.com/users/JoshRosen/subscriptions,https://api.github.com/users/JoshRosen/orgs,https://api.github.com/users/JoshRosen/repos,https://api.github.com/users/JoshRosen/events{/privacy},https://api.github.com/users/JoshRosen/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/35066,https://github.com/apache/spark/pull/35066,https://github.com/apache/spark/pull/35066.diff,https://github.com/apache/spark/pull/35066.patch,,https://api.github.com/repos/apache/spark/issues/35066/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
12,https://api.github.com/repos/apache/spark/issues/35060,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/35060/labels{/name},https://api.github.com/repos/apache/spark/issues/35060/comments,https://api.github.com/repos/apache/spark/issues/35060/events,https://github.com/apache/spark/pull/35060,1090502032,PR_kwDOAQXtWs4wXfia,35060,[SPARK-28137][SQL] Data Type Formatting Functions: `to_number`,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,0,2021-12-29T12:54:50Z,2021-12-31T08:46:42Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?
Many database support the function `to_number` to convert a string to number. 
The implement of `to_number` has many different between `Postgresql` ,`Oracle` and `Phoenix`.
So, this PR follows the implement of `to_number` in `Oracle` that give a strict parameter verification.
So, this PR follows the implement of `to_number` in `Phoenix` that uses BigDecimal.

This PR support the patterns for numeric formatting as follows:

Pattern | Description
-- | --
9 | Value with the specified number of digits
0 | Value with leading zeros
. (period) | Decimal point
, (comma) | Group (thousand) separator
S | Sign anchored to number (uses locale)
$ | a value with a leading dollar sign
D | Decimal point (uses locale)
G | Group separator (uses locale)



There are some mainstream database support the syntax.
**PostgreSQL:**
https://www.postgresql.org/docs/12/functions-formatting.html

**Oracle:**
https://docs.oracle.com/en/database/oracle/oracle-database/19/sqlrf/TO_NUMBER.html#GUID-D4807212-AFD7-48A7-9AED-BEC3E8809866

**Vertica**
https://www.vertica.com/docs/10.0.x/HTML/Content/Authoring/SQLReferenceManual/Functions/Formatting/TO_NUMBER.htm?tocpath=SQL%20Reference%20Manual%7CSQL%20Functions%7CFormatting%20Functions%7C_____7

**Redshift**
https://docs.aws.amazon.com/redshift/latest/dg/r_TO_NUMBER.html

**DB2**
https://www.ibm.com/support/knowledgecenter/SSGU8G_14.1.0/com.ibm.sqls.doc/ids_sqs_1544.htm

**Teradata**
https://docs.teradata.com/r/kmuOwjp1zEYg98JsB8fu_A/TH2cDXBn6tala29S536nqg

**Snowflake:**
https://docs.snowflake.net/manuals/sql-reference/functions/to_decimal.html

**Exasol**
https://docs.exasol.com/sql_references/functions/alphabeticallistfunctions/to_number.htm#TO_NUMBER

**Phoenix**
http://phoenix.incubator.apache.org/language/functions.html#to_number

**Singlestore**
https://docs.singlestore.com/v7.3/reference/sql-reference/numeric-functions/to-number/

**Intersystems**
https://docs.intersystems.com/latest/csp/docbook/DocBook.UI.Page.cls?KEY=RSQL_TONUMBER

The syntax like:
> select to_number('12,454.8-', '99G999D9S');
-12454.8

### Why are the changes needed?
`to_number` is very useful for formatted currency to number conversion.


### Does this PR introduce any user-facing change?
Yes. New feature.


### How was this patch tested?
New tests
",https://api.github.com/repos/apache/spark/issues/35060/timeline,,spark,apache,beliefer,8486025,MDQ6VXNlcjg0ODYwMjU=,https://avatars.githubusercontent.com/u/8486025?v=4,,https://api.github.com/users/beliefer,https://github.com/beliefer,https://api.github.com/users/beliefer/followers,https://api.github.com/users/beliefer/following{/other_user},https://api.github.com/users/beliefer/gists{/gist_id},https://api.github.com/users/beliefer/starred{/owner}{/repo},https://api.github.com/users/beliefer/subscriptions,https://api.github.com/users/beliefer/orgs,https://api.github.com/users/beliefer/repos,https://api.github.com/users/beliefer/events{/privacy},https://api.github.com/users/beliefer/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/35060,https://github.com/apache/spark/pull/35060,https://github.com/apache/spark/pull/35060.diff,https://github.com/apache/spark/pull/35060.patch,,https://api.github.com/repos/apache/spark/issues/35060/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
13,https://api.github.com/repos/apache/spark/issues/35059,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/35059/labels{/name},https://api.github.com/repos/apache/spark/issues/35059/comments,https://api.github.com/repos/apache/spark/issues/35059/events,https://github.com/apache/spark/pull/35059,1090446934,PR_kwDOAQXtWs4wXT7X,35059,[SPARK-37780][SQL] QueryExecutionListener support use SQLConf.get to get corresponding SessionState's SQLConf,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1406627200, 'node_id': 'MDU6TGFiZWwxNDA2NjI3MjAw', 'url': 'https://api.github.com/repos/apache/spark/labels/BUILD', 'name': 'BUILD', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,3,2021-12-29T11:13:20Z,2021-12-30T05:05:20Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?
Current QueryExecutionListener only support add constructor parameter of SparkConf, but if we  start a SparkContext first, many SQL conf won't be applied to SparkContext's SparkConf.
Also it's a SQL component,  related configuration we defined it as SQLConf. But it's hard to use SQLConf in this plugin listener.

This patch, we use pass SQLConf to `ExecutionListenerManager` and use `SQLConf.withExistingConf` to wrap the passed `SQLConf`. Then we can use `SQLConf.get` to get the same SQLConf of `SessionState` when initializing `QueryExecutionListener`



### Why are the changes needed?
Pass corresponding SessionState's SQLConf to QueryExecutionListener. Then user can use SQLConf.get to use the  SQLConf.


### Does this PR introduce _any_ user-facing change?
User can use QueryExecutionListener with corresponding `SessionState`'s `SQLConf`. For example
```
private class SQLConfQueryExecutionListener extends QueryExecutionListener {
  val sqlConf = SQLConf.get
  override def onSuccess(funcName: String, qe: QueryExecution, durationNs: Long): Unit = {
  }
  override def onFailure(funcName: String, qe: QueryExecution, exception: Exception): Unit = {
  }
}
```



### How was this patch tested?
Added UT",https://api.github.com/repos/apache/spark/issues/35059/timeline,,spark,apache,AngersZhuuuu,46485123,MDQ6VXNlcjQ2NDg1MTIz,https://avatars.githubusercontent.com/u/46485123?v=4,,https://api.github.com/users/AngersZhuuuu,https://github.com/AngersZhuuuu,https://api.github.com/users/AngersZhuuuu/followers,https://api.github.com/users/AngersZhuuuu/following{/other_user},https://api.github.com/users/AngersZhuuuu/gists{/gist_id},https://api.github.com/users/AngersZhuuuu/starred{/owner}{/repo},https://api.github.com/users/AngersZhuuuu/subscriptions,https://api.github.com/users/AngersZhuuuu/orgs,https://api.github.com/users/AngersZhuuuu/repos,https://api.github.com/users/AngersZhuuuu/events{/privacy},https://api.github.com/users/AngersZhuuuu/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/35059,https://github.com/apache/spark/pull/35059,https://github.com/apache/spark/pull/35059.diff,https://github.com/apache/spark/pull/35059.patch,,https://api.github.com/repos/apache/spark/issues/35059/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
14,https://api.github.com/repos/apache/spark/issues/35055,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/35055/labels{/name},https://api.github.com/repos/apache/spark/issues/35055/comments,https://api.github.com/repos/apache/spark/issues/35055/events,https://github.com/apache/spark/pull/35055,1090353937,PR_kwDOAQXtWs4wXAX4,35055,[SPARK-37769][SQL][FOLLOWUP] Filtering files if metadata columns are present in the data filter,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,2,2021-12-29T08:31:20Z,2021-12-30T06:47:37Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?
Follow-up PR of #34575. Filtering files if metadata columns are present in the data filter.

### Why are the changes needed?
Performance improvements.


### Does this PR introduce _any_ user-facing change?
No


### How was this patch tested?
Existing UTs and a new UT.
",https://api.github.com/repos/apache/spark/issues/35055/timeline,,spark,apache,Yaohua628,79476540,MDQ6VXNlcjc5NDc2NTQw,https://avatars.githubusercontent.com/u/79476540?v=4,,https://api.github.com/users/Yaohua628,https://github.com/Yaohua628,https://api.github.com/users/Yaohua628/followers,https://api.github.com/users/Yaohua628/following{/other_user},https://api.github.com/users/Yaohua628/gists{/gist_id},https://api.github.com/users/Yaohua628/starred{/owner}{/repo},https://api.github.com/users/Yaohua628/subscriptions,https://api.github.com/users/Yaohua628/orgs,https://api.github.com/users/Yaohua628/repos,https://api.github.com/users/Yaohua628/events{/privacy},https://api.github.com/users/Yaohua628/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/35055,https://github.com/apache/spark/pull/35055,https://github.com/apache/spark/pull/35055.diff,https://github.com/apache/spark/pull/35055.patch,,https://api.github.com/repos/apache/spark/issues/35055/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
15,https://api.github.com/repos/apache/spark/issues/35052,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/35052/labels{/name},https://api.github.com/repos/apache/spark/issues/35052/comments,https://api.github.com/repos/apache/spark/issues/35052/events,https://github.com/apache/spark/pull/35052,1090314585,PR_kwDOAQXtWs4wW4Ma,35052,"[SPARK-37644][SQL][FOLLOWUP] When partition column is same as group by key, pushing down aggregate completely.","[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,1,2021-12-29T07:06:38Z,2021-12-30T12:54:59Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?
When JDBC option specifying the ""partitionColumn"" and it's the same as group by key, the aggregate push-down should be completely.


### Why are the changes needed?
Improve the datasource v2 complete aggregate pushdown.


### Does this PR introduce _any_ user-facing change?
'No'. Just change the inner implement.


### How was this patch tested?
New tests.
",https://api.github.com/repos/apache/spark/issues/35052/timeline,,spark,apache,beliefer,8486025,MDQ6VXNlcjg0ODYwMjU=,https://avatars.githubusercontent.com/u/8486025?v=4,,https://api.github.com/users/beliefer,https://github.com/beliefer,https://api.github.com/users/beliefer/followers,https://api.github.com/users/beliefer/following{/other_user},https://api.github.com/users/beliefer/gists{/gist_id},https://api.github.com/users/beliefer/starred{/owner}{/repo},https://api.github.com/users/beliefer/subscriptions,https://api.github.com/users/beliefer/orgs,https://api.github.com/users/beliefer/repos,https://api.github.com/users/beliefer/events{/privacy},https://api.github.com/users/beliefer/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/35052,https://github.com/apache/spark/pull/35052,https://github.com/apache/spark/pull/35052.diff,https://github.com/apache/spark/pull/35052.patch,,https://api.github.com/repos/apache/spark/issues/35052/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
16,https://api.github.com/repos/apache/spark/issues/35049,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/35049/labels{/name},https://api.github.com/repos/apache/spark/issues/35049/comments,https://api.github.com/repos/apache/spark/issues/35049/events,https://github.com/apache/spark/pull/35049,1090233855,PR_kwDOAQXtWs4wWnuT,35049,[SPARK-37757][BUILD] Enable Spark test scheduled job on ARM runner,"[{'id': 1982110661, 'node_id': 'MDU6TGFiZWwxOTgyMTEwNjYx', 'url': 'https://api.github.com/repos/apache/spark/labels/INFRA', 'name': 'INFRA', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,2,2021-12-29T02:54:24Z,2022-01-01T00:20:37Z,,CONTRIBUTOR,,True,"### What changes were proposed in this pull request?
This patch adds the scheduled job on ARM runner.


### Why are the changes needed?
Migrate Spark Arm Job from Jenkins to GitHub Actions.


### Does this PR introduce _any_ user-facing change?
NO


### How was this patch tested?
- Trigger arm64 test in the repo: https://github.com/Yikun/spark/pull/51
```
Current runner version: '2.285.1'
Runner name: 'ubuntu-20.04-arm64'
Runner group name: 'Default'
Machine name: 'yikun-arm'
```
- Trigger original x86 test in this patch",https://api.github.com/repos/apache/spark/issues/35049/timeline,,spark,apache,Yikun,1736354,MDQ6VXNlcjE3MzYzNTQ=,https://avatars.githubusercontent.com/u/1736354?v=4,,https://api.github.com/users/Yikun,https://github.com/Yikun,https://api.github.com/users/Yikun/followers,https://api.github.com/users/Yikun/following{/other_user},https://api.github.com/users/Yikun/gists{/gist_id},https://api.github.com/users/Yikun/starred{/owner}{/repo},https://api.github.com/users/Yikun/subscriptions,https://api.github.com/users/Yikun/orgs,https://api.github.com/users/Yikun/repos,https://api.github.com/users/Yikun/events{/privacy},https://api.github.com/users/Yikun/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/35049,https://github.com/apache/spark/pull/35049,https://github.com/apache/spark/pull/35049.diff,https://github.com/apache/spark/pull/35049.patch,,https://api.github.com/repos/apache/spark/issues/35049/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
17,https://api.github.com/repos/apache/spark/issues/35047,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/35047/labels{/name},https://api.github.com/repos/apache/spark/issues/35047/comments,https://api.github.com/repos/apache/spark/issues/35047/events,https://github.com/apache/spark/pull/35047,1090223117,PR_kwDOAQXtWs4wWlop,35047,[SPARK-37175][SQL] Performance improvement to hash joins with many duplicate keys,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1406627200, 'node_id': 'MDU6TGFiZWwxNDA2NjI3MjAw', 'url': 'https://api.github.com/repos/apache/spark/labels/BUILD', 'name': 'BUILD', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,1,2021-12-29T02:12:45Z,2021-12-31T06:11:39Z,,CONTRIBUTOR,,False,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->
This PR aims at improving performance for Hash joins with many duplicate keys. 

A HashedRelation uses a map underneath to store rows against a corresponding key. A LongToUnsafeRowMap is used by LongHashedRelation and a BytesToBytesMap is used by UnsafeHashedRelation.
We propose to reorder the underlying map thereby placing all the rows for a given key adjacent in the memory to improve the spatial locality while iterating over them in the stream side of the join.

This is achieved in the following steps:
- creating another copy of the underlying map
- for all keys in the existing map
  - get the corresponding rows 
  - insert all the rows for the given key at once in the new map
- use the new map for look-ups

This optimization can be enabled by specifying `spark.sql.hashedRelationReorderFactor=<value>`.
Once the condition `number of rows >= number of unique keys * above value` is satisfied for the underlying map, the optimization will kick in.

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->
There is no order maintained when the rows are added to the underlying map, thus for a given key, the corresponding rows are typically non-adjacent in memory, resulting in a poor spatial locality. Placing the rows for adjacent in memory yields a performance boost thereby reducing execution time.


### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->
No.

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.
-->
- Modified existing unit tests to run against the suggested improvement.
- Added a couple of cases to test the scenarios when the improvement throws an exception due to insufficient memory.
- Added a micro-benchmark that clearly indicates performance improvements when there are duplicate keys.
- Ran the four example queries mentioned in the JIRA in spark-sql as a final check for performance improvement.

### Credits
This work is based on the initial idea proposed by @bersprockets.",https://api.github.com/repos/apache/spark/issues/35047/timeline,,spark,apache,sumeetgajjar,7351922,MDQ6VXNlcjczNTE5MjI=,https://avatars.githubusercontent.com/u/7351922?v=4,,https://api.github.com/users/sumeetgajjar,https://github.com/sumeetgajjar,https://api.github.com/users/sumeetgajjar/followers,https://api.github.com/users/sumeetgajjar/following{/other_user},https://api.github.com/users/sumeetgajjar/gists{/gist_id},https://api.github.com/users/sumeetgajjar/starred{/owner}{/repo},https://api.github.com/users/sumeetgajjar/subscriptions,https://api.github.com/users/sumeetgajjar/orgs,https://api.github.com/users/sumeetgajjar/repos,https://api.github.com/users/sumeetgajjar/events{/privacy},https://api.github.com/users/sumeetgajjar/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/35047,https://github.com/apache/spark/pull/35047,https://github.com/apache/spark/pull/35047.diff,https://github.com/apache/spark/pull/35047.patch,,https://api.github.com/repos/apache/spark/issues/35047/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
18,https://api.github.com/repos/apache/spark/issues/35045,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/35045/labels{/name},https://api.github.com/repos/apache/spark/issues/35045/comments,https://api.github.com/repos/apache/spark/issues/35045/events,https://github.com/apache/spark/pull/35045,1090162794,PR_kwDOAQXtWs4wWZfN,35045,[SPARK-37765][PYSPARK][WIP] DynamicDataFrame implementation,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1982260031, 'node_id': 'MDU6TGFiZWwxOTgyMjYwMDMx', 'url': 'https://api.github.com/repos/apache/spark/labels/PYTHON', 'name': 'PYTHON', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,4,2021-12-28T22:45:17Z,2021-12-29T11:33:44Z,,NONE,,False,"This is still missing a way to handle typing and also a set of tests
for the API, but it's a functional implementation to showcase the examples
written down in the PR and the JIRA ticket.

### What changes were proposed in this pull request?
It adds a new class, `DynamicDataFrame`, that allows users to implement inheritance on the `DataFrames` without losing chainability. The PR also has the autogenerator of the `DynamicDataFrame` class code in case any reviewer wants to remove one of the methods from the default-inherited ones.

### Why are the changes needed?
In typical development settings, multiple tables with very different concepts are mapped to the same `DataFrame` class. The inheritance from the pyspark `DataFrame` class is a bit cumbersome because of the chainable methods and it also makes it difficult to abstract regularly used queries. The proposal is to generate a `DynamicDataFrame` that allows easy inheritance retaining `DataFrame` methods without losing chainability neither for the newly generated queries nor for the usual dataframe ones.

In our experience, this allowed us to iterate much faster, generating business-centric classes in a couple of lines of code.


### Does this PR introduce _any_ user-facing change?
Yes: it adds a new class. It doesn't change any already existing code.


### How was this patch tested?
There is no test suite here since we are not sure about how to properly test the new API (we are inclined to generate one test case per method, ensuring that it generates objects of the proper type). We add here two code examples:

#### Inheriting from DataFrame
This shows the typical issues encountered when we try to inherit from pyspark `DataFrame`. It should work on the `master` branch as well

```python
import pyspark
from pyspark.sql import DataFrame
from pyspark.sql import functions as F

spark = pyspark.sql.SparkSession.builder.getOrCreate()


class Inventory(DataFrame):
    def __init__(self, df: DataFrame):
        super().__init__(df._jdf, df.sql_ctx)

    def update_prices(self, factor: float = 2.0):
        return self.withColumn(""price"", F.col(""price"") * factor)


base_dataframe = spark.createDataFrame(
    data=[[""product_1"", 2.0], [""product_2"", 4.0]],
    schema=[""name"", ""price""],
)
inventory = Inventory(base_dataframe)
inventory_updated = inventory.update_prices(2.0)
print(""inventory_updated.show():"")
inventory_updated.show()
print(""But after one use of the query we have a plain dataframe again"")
print(f""type(inventory_updated): {type(inventory_updated) }"")
# This would raise an AttributeError
# inventory_updated.update_prices(5.0)

print(""The same happens when we use DataFrame methods"")
expensive_inventory = inventory.filter(F.col(""price"") > 3.0)
print(""expensive_inventory.show():"")
expensive_inventory.show()
print(f""type(expensive_inventory): {type(expensive_inventory) }"")
```

and its output
```
inventory_updated.show():
+---------+-----+
|     name|price|
+---------+-----+
|product_1|  4.0|
|product_2|  8.0|
+---------+-----+

But after one use of the query we have a plain dataframe again
type(inventory_updated): <class 'pyspark.sql.dataframe.DataFrame'>
The same happens when we use DataFrame methods
expensive_inventory.show():
+---------+-----+
|     name|price|
+---------+-----+
|product_2|  4.0|
+---------+-----+

type(expensive_inventory): <class 'pyspark.sql.dataframe.DataFrame'>
```
#### Inheritance from DynamicDataFrame
This is what inheritance would look like if we used `DynamicDataFrame`, that runs on the current branch:

```python
import pyspark
from pyspark.sql import DynamicDataFrame
from pyspark.sql import functions as F

spark = pyspark.sql.SparkSession.builder.getOrCreate()


class Inventory(DynamicDataFrame):
    def update_prices(self, factor: float = 2.0):
        return self.withColumn(""price"", F.col(""price"") * factor)


base_dataframe = spark.createDataFrame(
    data=[[""product_1"", 2.0], [""product_2"", 4.0]],
    schema=[""name"", ""price""],
)
print(""Doing an inheritance mediated by DynamicDataFrame"")
inventory = Inventory(base_dataframe)
inventory_updated = inventory.update_prices(2.0).update_prices(5.0)
print(""inventory_updated.show():"")
inventory_updated.show()
print(""After multiple uses of the query we still have the desired type"")
print(f""type(inventory_updated): {type(inventory_updated)}"")
print(""We can still use the usual dataframe methods"")
expensive_inventory = inventory_updated.filter(F.col(""price"") > 25)
print(""expensive_inventory.show():"")
expensive_inventory.show()
print(""And retain the desired type"")
print(f""type(expensive_inventory): {type(expensive_inventory)}"")
```

and its output:
```
Doing an inheritance mediated by DynamicDataFrame
inventory_updated.show():
+---------+-----+
|     name|price|
+---------+-----+
|product_1| 20.0|
|product_2| 40.0|
+---------+-----+

After multiple uses of the query we still have the desired type
type(inventory_updated): <class '__main__.Inventory'>
We can still use the usual dataframe methods
expensive_inventory.show():
+---------+-----+
|     name|price|
+---------+-----+
|product_2| 40.0|
+---------+-----+

And retain the desired type
type(expensive_inventory): <class '__main__.Inventory'>
```
",https://api.github.com/repos/apache/spark/issues/35045/timeline,,spark,apache,pabloalcain,6975120,MDQ6VXNlcjY5NzUxMjA=,https://avatars.githubusercontent.com/u/6975120?v=4,,https://api.github.com/users/pabloalcain,https://github.com/pabloalcain,https://api.github.com/users/pabloalcain/followers,https://api.github.com/users/pabloalcain/following{/other_user},https://api.github.com/users/pabloalcain/gists{/gist_id},https://api.github.com/users/pabloalcain/starred{/owner}{/repo},https://api.github.com/users/pabloalcain/subscriptions,https://api.github.com/users/pabloalcain/orgs,https://api.github.com/users/pabloalcain/repos,https://api.github.com/users/pabloalcain/events{/privacy},https://api.github.com/users/pabloalcain/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/35045,https://github.com/apache/spark/pull/35045,https://github.com/apache/spark/pull/35045.diff,https://github.com/apache/spark/pull/35045.patch,,https://api.github.com/repos/apache/spark/issues/35045/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
19,https://api.github.com/repos/apache/spark/issues/35043,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/35043/labels{/name},https://api.github.com/repos/apache/spark/issues/35043/comments,https://api.github.com/repos/apache/spark/issues/35043/events,https://github.com/apache/spark/pull/35043,1090108135,PR_kwDOAQXtWs4wWOTm,35043,Remove characters added by IPython,"[{'id': 1406627200, 'node_id': 'MDU6TGFiZWwxNDA2NjI3MjAw', 'url': 'https://api.github.com/repos/apache/spark/labels/BUILD', 'name': 'BUILD', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1982260031, 'node_id': 'MDU6TGFiZWwxOTgyMjYwMDMx', 'url': 'https://api.github.com/repos/apache/spark/labels/PYTHON', 'name': 'PYTHON', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,2,2021-12-28T20:18:06Z,2021-12-29T02:24:14Z,,NONE,,False,"When running `PYSPARK_DRIVER_PYTHON=ipython pyspark` on xterm the find-spark-home script calls `ipython /path/to/find_spark_home.py` and the string printed by that script gets assigned to SPARK_HOME. When run with IPython that string will start with a sequence bounded by control characters before the path determined by find_spark_home.py. While this part of the string does not appear on echo it will cause pyspark to compose paths improperly when using SPARK_HOME.

To see the sequence run:

>>> import os
>>> p = os.popen('ipython somescript.py')
>>> p.read()

`'\x1b[22;0t\x1b]0;IPython: {current directory}\x07the expected output\n'`

The cut command removes the sequence before ""the expected output"". Lines without a bell character (\x07), such as you get when running `python3 find_spark_home.py`, remain unchanged.

### What changes were proposed in this pull request?
Fixing the assignment to SPARK_HOME in find-spark-home to remove the control characters added when using ipython.


### Why are the changes needed?
On xterm running `PYSPARK_DRIVER_PYTHON=ipython pyspark` causes pyspark to compose paths improperly, prepending the current working directory to SPARK_HOME as determined by find_spark_home.py, making it unable to find the files it seeks.


### Does this PR introduce _any_ user-facing change?
Yes. Before the change I would get ""No such file or directory"" errors as the current working directory would get prepended to SPARK_HOME. After the change the pyspark interactive prompt starts as expected with an ipython prompt.


### How was this patch tested?
I ran pyspark with PYSPARK_DRIVER_PYTHON set to ""python"", ""python3"" and ""ipython"". All three variations gave the appropriate prompt with the expected session and context variables set. I also tested the pipe to the cut command with lines with and without bell characters to ensure that the addition had no effect on the latter. I didn't modify the current testing scheme because I couldn't find an extant test for any of the relevant bash scripts.
",https://api.github.com/repos/apache/spark/issues/35043/timeline,,spark,apache,Shooter23,44271378,MDQ6VXNlcjQ0MjcxMzc4,https://avatars.githubusercontent.com/u/44271378?v=4,,https://api.github.com/users/Shooter23,https://github.com/Shooter23,https://api.github.com/users/Shooter23/followers,https://api.github.com/users/Shooter23/following{/other_user},https://api.github.com/users/Shooter23/gists{/gist_id},https://api.github.com/users/Shooter23/starred{/owner}{/repo},https://api.github.com/users/Shooter23/subscriptions,https://api.github.com/users/Shooter23/orgs,https://api.github.com/users/Shooter23/repos,https://api.github.com/users/Shooter23/events{/privacy},https://api.github.com/users/Shooter23/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/35043,https://github.com/apache/spark/pull/35043,https://github.com/apache/spark/pull/35043.diff,https://github.com/apache/spark/pull/35043.patch,,https://api.github.com/repos/apache/spark/issues/35043/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
20,https://api.github.com/repos/apache/spark/issues/35041,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/35041/labels{/name},https://api.github.com/repos/apache/spark/issues/35041/comments,https://api.github.com/repos/apache/spark/issues/35041/events,https://github.com/apache/spark/pull/35041,1089777512,PR_kwDOAQXtWs4wVKcA,35041,[SPARK-37691][SQL] Support ANSI Aggregation Function: `percentile_disc`,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981471081, 'node_id': 'MDU6TGFiZWwxOTgxNDcxMDgx', 'url': 'https://api.github.com/repos/apache/spark/labels/DOCS', 'name': 'DOCS', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,1,2021-12-28T10:29:20Z,2021-12-30T11:51:48Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?
`PERCENTILE_DISC` is an ANSI aggregate functions.

The mainstream database supports `percentile_disc` show below:
**Postgresql**
https://www.postgresql.org/docs/9.4/functions-aggregate.html
**Teradata**
https://docs.teradata.com/r/kmuOwjp1zEYg98JsB8fu_A/cPkFySIBORL~M938Zv07Cg
**Snowflake**
https://docs.snowflake.com/en/sql-reference/functions/percentile_disc.html
**Oracle**
https://docs.oracle.com/en/database/oracle/oracle-database/19/sqlrf/PERCENTILE_DISC.html#GUID-7C34FDDA-C241-474F-8C5C-50CC0182E005
**H2**
http://www.h2database.com/html/functions-aggregate.html#percentile_disc
**Sybase**
https://infocenter.sybase.com/help/index.jsp?topic=/com.sybase.infocenter.dc01776.1601/doc/html/san1278453110413.html
**Exasol**
https://docs.exasol.com/sql_references/functions/alphabeticallistfunctions/percentile_disc.htm
**RedShift**
https://docs.aws.amazon.com/redshift/latest/dg/r_APPROXIMATE_PERCENTILE_DISC.html
**Yellowbrick**
https://www.yellowbrick.com/docs/2.2/ybd_sqlref/percentile_disc.html
**Mariadb**
https://mariadb.com/kb/en/percentile_disc/
**Phoenix**
http://phoenix.incubator.apache.org/language/functions.html#percentile_disc
**Singlestore**
https://docs.singlestore.com/db/v7.6/en/reference/sql-reference/window-functions/percentile_disc.html

### Why are the changes needed?
`PERCENTILE_DISC` is very useful. Exposing the expression can make the migration from other systems to Spark SQL easier.

### Does this PR introduce _any_ user-facing change?
'Yes'. New feature.


### How was this patch tested?
New tests.
",https://api.github.com/repos/apache/spark/issues/35041/timeline,,spark,apache,beliefer,8486025,MDQ6VXNlcjg0ODYwMjU=,https://avatars.githubusercontent.com/u/8486025?v=4,,https://api.github.com/users/beliefer,https://github.com/beliefer,https://api.github.com/users/beliefer/followers,https://api.github.com/users/beliefer/following{/other_user},https://api.github.com/users/beliefer/gists{/gist_id},https://api.github.com/users/beliefer/starred{/owner}{/repo},https://api.github.com/users/beliefer/subscriptions,https://api.github.com/users/beliefer/orgs,https://api.github.com/users/beliefer/repos,https://api.github.com/users/beliefer/events{/privacy},https://api.github.com/users/beliefer/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/35041,https://github.com/apache/spark/pull/35041,https://github.com/apache/spark/pull/35041.diff,https://github.com/apache/spark/pull/35041.patch,,https://api.github.com/repos/apache/spark/issues/35041/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
21,https://api.github.com/repos/apache/spark/issues/35040,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/35040/labels{/name},https://api.github.com/repos/apache/spark/issues/35040/comments,https://api.github.com/repos/apache/spark/issues/35040/events,https://github.com/apache/spark/pull/35040,1089732737,PR_kwDOAQXtWs4wVBS9,35040,[WIP][DEBUG][SPARK-37367][SQL] Reenable exception test in DDLParserSuite.create view -- basic,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,0,2021-12-28T09:20:22Z,2021-12-30T06:11:59Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->


### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->


### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->


### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.
-->
",https://api.github.com/repos/apache/spark/issues/35040/timeline,,spark,apache,AngersZhuuuu,46485123,MDQ6VXNlcjQ2NDg1MTIz,https://avatars.githubusercontent.com/u/46485123?v=4,,https://api.github.com/users/AngersZhuuuu,https://github.com/AngersZhuuuu,https://api.github.com/users/AngersZhuuuu/followers,https://api.github.com/users/AngersZhuuuu/following{/other_user},https://api.github.com/users/AngersZhuuuu/gists{/gist_id},https://api.github.com/users/AngersZhuuuu/starred{/owner}{/repo},https://api.github.com/users/AngersZhuuuu/subscriptions,https://api.github.com/users/AngersZhuuuu/orgs,https://api.github.com/users/AngersZhuuuu/repos,https://api.github.com/users/AngersZhuuuu/events{/privacy},https://api.github.com/users/AngersZhuuuu/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/35040,https://github.com/apache/spark/pull/35040,https://github.com/apache/spark/pull/35040.diff,https://github.com/apache/spark/pull/35040.patch,,https://api.github.com/repos/apache/spark/issues/35040/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
22,https://api.github.com/repos/apache/spark/issues/35038,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/35038/labels{/name},https://api.github.com/repos/apache/spark/issues/35038/comments,https://api.github.com/repos/apache/spark/issues/35038/events,https://github.com/apache/spark/pull/35038,1089688861,PR_kwDOAQXtWs4wU4FT,35038,[SPARK-37728][SQL][3.2] Reading nested columns with ORC vectorized reader can cause ArrayIndexOutOfBoundsException,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,6,2021-12-28T08:02:50Z,2021-12-31T18:55:27Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?

This is a backport of https://github.com/apache/spark/pull/35002 .

When an OrcColumnarBatchReader is created, method initBatch will be called only once. In method initBatch:

`orcVectorWrappers[i] = OrcColumnVectorUtils.toOrcColumnVector(dt, wrap.batch().cols[colId]);`

When the second argument of toOrcColumnVector is a ListColumnVector/MapColumnVector, orcVectorWrappers[i] is initialized with the ListColumnVector or MapColumnVector's offsets and lengths. 

However, when method nextBatch of OrcColumnarBatchReader is called, method ensureSize of ColumnVector (and its subclasses, like MultiValuedColumnVector) could be called, then the ListColumnVector/MapColumnVector's offsets and lengths could refer to new array objects. This could result in the ArrayIndexOutOfBoundsException. 

This PR makes OrcArrayColumnVector.getArray and OrcMapColumnVector.getMap always get offsets and lengths from the underlying ColumnVector, which can resolve this issue.

### Why are the changes needed?

Bugfix

### Does this PR introduce _any_ user-facing change?

No

### How was this patch tested?

Pass the CIs with the newly added test case.",https://api.github.com/repos/apache/spark/issues/35038/timeline,,spark,apache,yym1995,26797163,MDQ6VXNlcjI2Nzk3MTYz,https://avatars.githubusercontent.com/u/26797163?v=4,,https://api.github.com/users/yym1995,https://github.com/yym1995,https://api.github.com/users/yym1995/followers,https://api.github.com/users/yym1995/following{/other_user},https://api.github.com/users/yym1995/gists{/gist_id},https://api.github.com/users/yym1995/starred{/owner}{/repo},https://api.github.com/users/yym1995/subscriptions,https://api.github.com/users/yym1995/orgs,https://api.github.com/users/yym1995/repos,https://api.github.com/users/yym1995/events{/privacy},https://api.github.com/users/yym1995/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/35038,https://github.com/apache/spark/pull/35038,https://github.com/apache/spark/pull/35038.diff,https://github.com/apache/spark/pull/35038.patch,,https://api.github.com/repos/apache/spark/issues/35038/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
23,https://api.github.com/repos/apache/spark/issues/35031,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/35031/labels{/name},https://api.github.com/repos/apache/spark/issues/35031/comments,https://api.github.com/repos/apache/spark/issues/35031/events,https://github.com/apache/spark/pull/35031,1089063571,PR_kwDOAQXtWs4wS3TZ,35031,[SPARK-37750][SQL] ANSI mode: optionally return null result if element not exists in array/map,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,4,2021-12-27T08:24:07Z,2021-12-28T03:29:47Z,,MEMBER,,False,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->
Add a new configuration `spark.sql.ansi.failOnElementNotExists` which controls whether throwing exceptions or returning null results when element not exists in:
- [] operator in array/map type
- element_at()
- elt()

The default value of the new configuration is true.

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->
* Provide an alternative way for Spark SQL users who replies on null results when element not exists in array/map, e.g. `select .. where array[index] is not null` or `select .. where map[key] is not null`
* Map type is not part of the ANSI SQL type. There can be arguments that map[key] should return null if key not exist.
* For array access, the index of try_element_at() starts from 1, while the index of `array[index]` starts from 0. Replacing `array[index]` as ry_element_at() can be troublesome.

### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->
Yes, providing a new option `spark.sql.ansi.failOnElementNotExists` which can optionally return null result if element not exists in array/map. However, the default behavior is not changed.

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.
-->
Unit tests",https://api.github.com/repos/apache/spark/issues/35031/timeline,,spark,apache,gengliangwang,1097932,MDQ6VXNlcjEwOTc5MzI=,https://avatars.githubusercontent.com/u/1097932?v=4,,https://api.github.com/users/gengliangwang,https://github.com/gengliangwang,https://api.github.com/users/gengliangwang/followers,https://api.github.com/users/gengliangwang/following{/other_user},https://api.github.com/users/gengliangwang/gists{/gist_id},https://api.github.com/users/gengliangwang/starred{/owner}{/repo},https://api.github.com/users/gengliangwang/subscriptions,https://api.github.com/users/gengliangwang/orgs,https://api.github.com/users/gengliangwang/repos,https://api.github.com/users/gengliangwang/events{/privacy},https://api.github.com/users/gengliangwang/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/35031,https://github.com/apache/spark/pull/35031,https://github.com/apache/spark/pull/35031.diff,https://github.com/apache/spark/pull/35031.patch,,https://api.github.com/repos/apache/spark/issues/35031/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
24,https://api.github.com/repos/apache/spark/issues/35020,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/35020/labels{/name},https://api.github.com/repos/apache/spark/issues/35020/comments,https://api.github.com/repos/apache/spark/issues/35020/events,https://github.com/apache/spark/pull/35020,1088590700,PR_kwDOAQXtWs4wReR8,35020,[SPARK-37429][PYTHON][MLLIB] Inline annotations for pyspark.mllib.linalg.__init__.py,"[{'id': 1405803268, 'node_id': 'MDU6TGFiZWwxNDA1ODAzMjY4', 'url': 'https://api.github.com/repos/apache/spark/labels/MLLIB', 'name': 'MLLIB', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1982260031, 'node_id': 'MDU6TGFiZWwxOTgyMjYwMDMx', 'url': 'https://api.github.com/repos/apache/spark/labels/PYTHON', 'name': 'PYTHON', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,1,2021-12-25T11:50:00Z,2021-12-28T01:15:36Z,,MEMBER,,False,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->

Inline annotations for `pyspark.mllib.linalg.__init__.py`


### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->

Currently, there is type hint stub files  `pyspark.mllib.linalg.__init__.pyi` to show the expected types for functions, but we can also take advantage of static type checking within the functions by inlining the type hints.


### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->

No.

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.
-->

Existing tests.
",https://api.github.com/repos/apache/spark/issues/35020/timeline,,spark,apache,zero323,1554276,MDQ6VXNlcjE1NTQyNzY=,https://avatars.githubusercontent.com/u/1554276?v=4,,https://api.github.com/users/zero323,https://github.com/zero323,https://api.github.com/users/zero323/followers,https://api.github.com/users/zero323/following{/other_user},https://api.github.com/users/zero323/gists{/gist_id},https://api.github.com/users/zero323/starred{/owner}{/repo},https://api.github.com/users/zero323/subscriptions,https://api.github.com/users/zero323/orgs,https://api.github.com/users/zero323/repos,https://api.github.com/users/zero323/events{/privacy},https://api.github.com/users/zero323/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/35020,https://github.com/apache/spark/pull/35020,https://github.com/apache/spark/pull/35020.diff,https://github.com/apache/spark/pull/35020.patch,,https://api.github.com/repos/apache/spark/issues/35020/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
25,https://api.github.com/repos/apache/spark/issues/35017,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/35017/labels{/name},https://api.github.com/repos/apache/spark/issues/35017/comments,https://api.github.com/repos/apache/spark/issues/35017/events,https://github.com/apache/spark/pull/35017,1088555915,PR_kwDOAQXtWs4wRYLX,35017,[SPARK-36853][BUILD] Code failing on checkstyle,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1406627200, 'node_id': 'MDU6TGFiZWwxNDA2NjI3MjAw', 'url': 'https://api.github.com/repos/apache/spark/labels/BUILD', 'name': 'BUILD', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1496047285, 'node_id': 'MDU6TGFiZWwxNDk2MDQ3Mjg1', 'url': 'https://api.github.com/repos/apache/spark/labels/EXAMPLES', 'name': 'EXAMPLES', 'color': 'ededed', 'default': False, 'description': ''}, {'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,5,2021-12-25T06:32:52Z,2021-12-29T16:29:17Z,,CONTRIBUTOR,,False,"What changes were proposed in this pull request?

Fixed the checkstyle error thrown in windows in maven installing phase.

Why are the changes needed?

Due to some problems in CI system, these obvious errors have not been detected. I think these checkstyle errors should be solved.

Does this PR introduce any user-facing change?

No.

How was this patch tested?

I have verified by executing following commands in the win10 system, except for (modifier) RedundantModifier in TimSort.java and (naming) MethodName in GroupStateTimeout.java and OutputMode.java and Trigger.java, the rest of the errors have been fixed.
`mvn -DskipTest clean install`",https://api.github.com/repos/apache/spark/issues/35017/timeline,,spark,apache,Shockang,28219857,MDQ6VXNlcjI4MjE5ODU3,https://avatars.githubusercontent.com/u/28219857?v=4,,https://api.github.com/users/Shockang,https://github.com/Shockang,https://api.github.com/users/Shockang/followers,https://api.github.com/users/Shockang/following{/other_user},https://api.github.com/users/Shockang/gists{/gist_id},https://api.github.com/users/Shockang/starred{/owner}{/repo},https://api.github.com/users/Shockang/subscriptions,https://api.github.com/users/Shockang/orgs,https://api.github.com/users/Shockang/repos,https://api.github.com/users/Shockang/events{/privacy},https://api.github.com/users/Shockang/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/35017,https://github.com/apache/spark/pull/35017,https://github.com/apache/spark/pull/35017.diff,https://github.com/apache/spark/pull/35017.patch,,https://api.github.com/repos/apache/spark/issues/35017/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
26,https://api.github.com/repos/apache/spark/issues/35013,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/35013/labels{/name},https://api.github.com/repos/apache/spark/issues/35013/comments,https://api.github.com/repos/apache/spark/issues/35013/events,https://github.com/apache/spark/pull/35013,1088251455,PR_kwDOAQXtWs4wQcrd,35013,[SPARK-37734][SQL][TESTS] Upgrade h2 from 1.4.195 to 2.0.202,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1406627200, 'node_id': 'MDU6TGFiZWwxNDA2NjI3MjAw', 'url': 'https://api.github.com/repos/apache/spark/labels/BUILD', 'name': 'BUILD', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,11,2021-12-24T08:56:16Z,2021-12-31T01:18:54Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?
This PR aims to upgrade `com.h2database` from 1.4.195 to 2.0.202

### Why are the changes needed?
Fix one vulnerability, ref: https://www.tenable.com/cve/CVE-2021-23463

### Does this PR introduce _any_ user-facing change?
'No'.

### How was this patch tested?
Jenkins test.
",https://api.github.com/repos/apache/spark/issues/35013/timeline,,spark,apache,beliefer,8486025,MDQ6VXNlcjg0ODYwMjU=,https://avatars.githubusercontent.com/u/8486025?v=4,,https://api.github.com/users/beliefer,https://github.com/beliefer,https://api.github.com/users/beliefer/followers,https://api.github.com/users/beliefer/following{/other_user},https://api.github.com/users/beliefer/gists{/gist_id},https://api.github.com/users/beliefer/starred{/owner}{/repo},https://api.github.com/users/beliefer/subscriptions,https://api.github.com/users/beliefer/orgs,https://api.github.com/users/beliefer/repos,https://api.github.com/users/beliefer/events{/privacy},https://api.github.com/users/beliefer/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/35013,https://github.com/apache/spark/pull/35013,https://github.com/apache/spark/pull/35013.diff,https://github.com/apache/spark/pull/35013.patch,,https://api.github.com/repos/apache/spark/issues/35013/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
27,https://api.github.com/repos/apache/spark/issues/35009,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/35009/labels{/name},https://api.github.com/repos/apache/spark/issues/35009/comments,https://api.github.com/repos/apache/spark/issues/35009/events,https://github.com/apache/spark/pull/35009,1088177287,PR_kwDOAQXtWs4wQNmf,35009,[SPARK-37527][SQL] Translate more standard aggregate functions for pushdown,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,4,2021-12-24T06:15:45Z,2021-12-28T06:44:27Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?
Currently, Spark aggregate pushdown will translate some standard aggregate functions, so that compile these functions to adapt specify database.
After this job, users could override `JdbcDialect.compileAggregate` to implement some standard aggregate functions supported by some database.
Because some aggregate functions will be converted by Optimizer show below, this PR no need to match them.

|Input|Parsed|Optimized|
|------|--------------------|----------|
|`Every`| `aggregate.BoolAnd` |`Min`|
|`Any`| `aggregate.BoolOr` |`Max`|
|`Some`| `aggregate.BoolOr` |`Max`|

### Why are the changes needed?
Make the implement of `*Dialect` could extends the aggregate functions by override `JdbcDialect.compileAggregate`.


### Does this PR introduce _any_ user-facing change?
Yes. Users could pushdown more aggregate functions.


### How was this patch tested?
Exists tests.
",https://api.github.com/repos/apache/spark/issues/35009/timeline,,spark,apache,beliefer,8486025,MDQ6VXNlcjg0ODYwMjU=,https://avatars.githubusercontent.com/u/8486025?v=4,,https://api.github.com/users/beliefer,https://github.com/beliefer,https://api.github.com/users/beliefer/followers,https://api.github.com/users/beliefer/following{/other_user},https://api.github.com/users/beliefer/gists{/gist_id},https://api.github.com/users/beliefer/starred{/owner}{/repo},https://api.github.com/users/beliefer/subscriptions,https://api.github.com/users/beliefer/orgs,https://api.github.com/users/beliefer/repos,https://api.github.com/users/beliefer/events{/privacy},https://api.github.com/users/beliefer/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/35009,https://github.com/apache/spark/pull/35009,https://github.com/apache/spark/pull/35009.diff,https://github.com/apache/spark/pull/35009.patch,,https://api.github.com/repos/apache/spark/issues/35009/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
28,https://api.github.com/repos/apache/spark/issues/35005,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/35005/labels{/name},https://api.github.com/repos/apache/spark/issues/35005/comments,https://api.github.com/repos/apache/spark/issues/35005/events,https://github.com/apache/spark/pull/35005,1087979295,PR_kwDOAQXtWs4wPldL,35005,[SPARK-8582][CORE] Checkpoint eagerly when asked to do so for real,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,13,2021-12-23T20:36:11Z,2021-12-31T21:21:22Z,,NONE,,False,"Run checkpoint job only once when asked to do so eagerly.

### What changes were proposed in this pull request?

The flow is like so:
```
- df.checkpoint(eager = true, reliable = true)
  - rdd = get rdd from this df's physical plan
  - rdd.checkpoint (just marks checkpointData)
  - rdd.count (if eager = true)
    - SparkContext.runJob for all the partitions
      - DAGScheduler.runJob
      - rdd.doCheckpoint
        - ReliableCheckpointRDD#writeRDDToCheckpointDirectory
          - SparkContext.runJob for all the partitions
          - DAGScheduler.runJob (<-- This is the repeat job)
```

The local checkpointing case is better because there it will just
recompute the missing partitions.

We tried a fix where we just replaced `rdd.count` above with
`rdd.doCheckpoint` and it seemed to work and pass the unit tests.

So the new flow is simply:
```
- df.checkpoint(eager = true, reliable = true)
  - rdd = get rdd from this df's physical plan
  - rdd.checkpoint (just marks checkpointData)
  - rdd.doCheckpoint (if eager = true)
    - ReliableCheckpointRDD#writeRDDToCheckpointDirectory
      - SparkContext.runJob for all the partitions
      - DAGScheduler.runJob (<-- Only one job is run)
```

### Why are the changes needed?
This simple fix drastically improves spark jobs that make heavy use of
Dataframe.checkpoint.


### Does this PR introduce _any_ user-facing change?

Yes, it would make eager checkpointing jobs supposedly faster by doing half as many spark jobs.

### How was this patch tested?

Customer spark apps using checkpoint with this fix see half as many
spark jobs launched, seeing upto 50% less runtime in some cases.
Also, added one more unit test to check that only job is created.

This patch may have some interactions with the Spark-Streaming, since it
touches the codepaths enabled via the config
spark.checkpoint.checkpointAllMarkedAncestors, so would be happy to add
more testing there if pointed in the right direction.",https://api.github.com/repos/apache/spark/issues/35005/timeline,,spark,apache,agrawaldevesh,20564223,MDQ6VXNlcjIwNTY0MjIz,https://avatars.githubusercontent.com/u/20564223?v=4,,https://api.github.com/users/agrawaldevesh,https://github.com/agrawaldevesh,https://api.github.com/users/agrawaldevesh/followers,https://api.github.com/users/agrawaldevesh/following{/other_user},https://api.github.com/users/agrawaldevesh/gists{/gist_id},https://api.github.com/users/agrawaldevesh/starred{/owner}{/repo},https://api.github.com/users/agrawaldevesh/subscriptions,https://api.github.com/users/agrawaldevesh/orgs,https://api.github.com/users/agrawaldevesh/repos,https://api.github.com/users/agrawaldevesh/events{/privacy},https://api.github.com/users/agrawaldevesh/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/35005,https://github.com/apache/spark/pull/35005,https://github.com/apache/spark/pull/35005.diff,https://github.com/apache/spark/pull/35005.patch,,https://api.github.com/repos/apache/spark/issues/35005/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
29,https://api.github.com/repos/apache/spark/issues/35004,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/35004/labels{/name},https://api.github.com/repos/apache/spark/issues/35004/comments,https://api.github.com/repos/apache/spark/issues/35004/events,https://github.com/apache/spark/pull/35004,1087790239,PR_kwDOAQXtWs4wO9fk,35004,[SPARK-37731][SQL] Refactor and cleanup function lookup in Analyzer,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981471081, 'node_id': 'MDU6TGFiZWwxOTgxNDcxMDgx', 'url': 'https://api.github.com/repos/apache/spark/labels/DOCS', 'name': 'DOCS', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,3,2021-12-23T15:04:00Z,2021-12-30T13:56:57Z,,CONTRIBUTOR,,False,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->
Today, the function lookup code path is pretty hard to understand as it spreads over many places:
1. lookup v1 function
2. lookup v2 function
3. lookup higher-order function
4. lookup table function
5. lookup functions in different levels: builtin, temp and persistent.

This PR is a major refactor of the function lookup code path and cleans it up quite a bit. In general, it follows the idea of table lookup:
1. Analyzer looks up built-in or temp functions first.
2. Analyzer qualifies the function name with current catalog and namespace, or with view catalog/namespace if we are resolving a view.
3. Analyzer calls v1 sessin catalog if the catalog is `spark_catalog`, otherwise call the v2 catalog.

With this refactor, the analyzer is kind of the router and the v1 session catalog can just have some small functions with very specific goals.

The function DDL commands also follow similar table/view commands and can fail automatically if the command requires persistent functions but the resolved function is built-in/temp. After this change, it should be simpler to add v2 function DDL commands.

Note that, table function lookup is still in its own rule as it has a dedicated function registry and doesn't share the namespace of the scalar functions.

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->
code cleanup to improve readability.

### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->
Yes. Since Spark 3.3, DESCRIBE FUNCTION fails if the function does not exist. In Spark 3.2 or earlier, DESCRIBE FUNCTION can still run and print ""Function: func_name not found"".

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.
-->
existing tests",https://api.github.com/repos/apache/spark/issues/35004/timeline,,spark,apache,cloud-fan,3182036,MDQ6VXNlcjMxODIwMzY=,https://avatars.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/35004,https://github.com/apache/spark/pull/35004,https://github.com/apache/spark/pull/35004.diff,https://github.com/apache/spark/pull/35004.patch,,https://api.github.com/repos/apache/spark/issues/35004/reactions,1,1,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
30,https://api.github.com/repos/apache/spark/issues/34995,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34995/labels{/name},https://api.github.com/repos/apache/spark/issues/34995/comments,https://api.github.com/repos/apache/spark/issues/34995/events,https://github.com/apache/spark/pull/34995,1087396103,PR_kwDOAQXtWs4wNqOw,34995,[SPARK-37722][SQL] Escape dot character in partition names,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,18,2021-12-23T05:45:19Z,2021-12-29T05:36:49Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?

This PR escapes `.` character in partition names as certain file systems (for example, ABFS) may not support it.

I simply added the `.` character to the list of special characters in `ExternalCatalogUtils`. The class already covers backward compatibility so it would read the existing unescaped values with dots correctly.

### Why are the changes needed?
Fixes an issue when ABFS throws `Caused by: java.lang.IllegalArgumentException: ABFS does not allow files or directories to end with a dot.` exception if a partition name contains `.` symbol.

### Does this PR introduce _any_ user-facing change?
Yes.

Forward compatibility is fine, Spark will be able to read both escaped and unescaped `.` characters.

Although there are no changes in reading string values, there are the following backward incompatible changes:
- Double values will not be inferred correctly, this would require backports to older Spark versions.
- There is a user-facing change when displaying any double values or string values containing a dot. For example, the command `SHOW PARTITIONS` would return slightly different result as well as directory names:

Before:
```
SHOW PARTITIONS escapeDots;

+----------+
| partition|
+----------+
|value=%2E.|
|   value=.|
|  value=a.|
|value=b.c.|
+----------+
```


After:
```
SHOW PARTITIONS escapeDots;

+--------------+
|     partition|
+--------------+
|value=%252E%2E|
|     value=%2E|
|    value=a%2E|
|value=b%2Ec%2E|
+--------------+
```

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.
-->

Added a unit test in `InsertSuite` to confirm that dots are now escaped + existing unit tests.",https://api.github.com/repos/apache/spark/issues/34995/timeline,,spark,apache,sadikovi,7788766,MDQ6VXNlcjc3ODg3NjY=,https://avatars.githubusercontent.com/u/7788766?v=4,,https://api.github.com/users/sadikovi,https://github.com/sadikovi,https://api.github.com/users/sadikovi/followers,https://api.github.com/users/sadikovi/following{/other_user},https://api.github.com/users/sadikovi/gists{/gist_id},https://api.github.com/users/sadikovi/starred{/owner}{/repo},https://api.github.com/users/sadikovi/subscriptions,https://api.github.com/users/sadikovi/orgs,https://api.github.com/users/sadikovi/repos,https://api.github.com/users/sadikovi/events{/privacy},https://api.github.com/users/sadikovi/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34995,https://github.com/apache/spark/pull/34995,https://github.com/apache/spark/pull/34995.diff,https://github.com/apache/spark/pull/34995.patch,,https://api.github.com/repos/apache/spark/issues/34995/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
31,https://api.github.com/repos/apache/spark/issues/34990,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34990/labels{/name},https://api.github.com/repos/apache/spark/issues/34990/comments,https://api.github.com/repos/apache/spark/issues/34990/events,https://github.com/apache/spark/pull/34990,1087307237,PR_kwDOAQXtWs4wNX_6,34990,[SPARK-37717][SQL] Improve logging in BroadcastExchangeExec,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,2,2021-12-23T01:55:36Z,2021-12-24T20:17:49Z,,CONTRIBUTOR,,False,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
The logging and errors from inside

`
override lazy val relationFuture: Future[broadcast.Broadcast[Any]] = {
  SQLExecution.withThreadLocalCaptured[broadcast.Broadcast[Any]](
    session, BroadcastExchangeExec.executionContext) {
        try {
.....     
} 
`
are hard to relate to the right join in a plan with lost of Broadcast joins.
Spark UI displays

`BroadcastExchangeExec.runId`

but it's not in the logs.
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->


### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->


### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->
Logs will now look like
`2021-11-13 21:14:59,028 INFO [broadcast-exchange-6-e20800a6-5204-40c5-82d8-40662affc244] [tenant:] [app: ] [appID:application_1627787411812_5950609] [executor:driver] [cid: ] [prid: ] TaskMemoryManager: org.apache.spark.memory.TaskMemoryManager.acquireExecutionMemory(TaskMemoryManager.java:238) - null (TID 0) acquired 16.0 KiB for org.apache.spark.unsafe.map.BytesToBytesMap@33bfacf0`

instead of

`2021-11-13 21:14:59,028 INFO [broadcast-exchange-6] [tenant:] [app: ] [appID:application_1627787411812_5950609] [executor:driver] [cid: ] [prid: ] TaskMemoryManager: org.apache.spark.memory.TaskMemoryManager.acquireExecutionMemory(TaskMemoryManager.java:238) - null (TID 0) acquired 16.0 KiB for org.apache.spark.unsafe.map.BytesToBytesMap@33bfacf0`

### How was this patch tested?
Existing tests
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.
-->
",https://api.github.com/repos/apache/spark/issues/34990/timeline,,spark,apache,ekoifman,4297661,MDQ6VXNlcjQyOTc2NjE=,https://avatars.githubusercontent.com/u/4297661?v=4,,https://api.github.com/users/ekoifman,https://github.com/ekoifman,https://api.github.com/users/ekoifman/followers,https://api.github.com/users/ekoifman/following{/other_user},https://api.github.com/users/ekoifman/gists{/gist_id},https://api.github.com/users/ekoifman/starred{/owner}{/repo},https://api.github.com/users/ekoifman/subscriptions,https://api.github.com/users/ekoifman/orgs,https://api.github.com/users/ekoifman/repos,https://api.github.com/users/ekoifman/events{/privacy},https://api.github.com/users/ekoifman/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34990,https://github.com/apache/spark/pull/34990,https://github.com/apache/spark/pull/34990.diff,https://github.com/apache/spark/pull/34990.patch,,https://api.github.com/repos/apache/spark/issues/34990/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
32,https://api.github.com/repos/apache/spark/issues/34984,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34984/labels{/name},https://api.github.com/repos/apache/spark/issues/34984/comments,https://api.github.com/repos/apache/spark/issues/34984/events,https://github.com/apache/spark/pull/34984,1086713325,PR_kwDOAQXtWs4wLbFj,34984,[SPARK-37463][SQL] Read/Write Timestamp ntz from/to Orc uses int64,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,29,2021-12-22T11:09:00Z,2021-12-28T11:07:38Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?
#33588 (comment) show Spark cannot read/write timestamp ntz and ltz correctly. Based on the discussion https://github.com/apache/spark/pull/34741#issuecomment-983660633, we just to fix read/write timestamp ntz to Orc uses int64.

### Why are the changes needed?
Fix the bug about read/write timestamp ntz from/to Orc with different times zone.


### Does this PR introduce _any_ user-facing change?
Yes. Orc timestamp ntz is a new feature.


### How was this patch tested?
New tests.
",https://api.github.com/repos/apache/spark/issues/34984/timeline,,spark,apache,beliefer,8486025,MDQ6VXNlcjg0ODYwMjU=,https://avatars.githubusercontent.com/u/8486025?v=4,,https://api.github.com/users/beliefer,https://github.com/beliefer,https://api.github.com/users/beliefer/followers,https://api.github.com/users/beliefer/following{/other_user},https://api.github.com/users/beliefer/gists{/gist_id},https://api.github.com/users/beliefer/starred{/owner}{/repo},https://api.github.com/users/beliefer/subscriptions,https://api.github.com/users/beliefer/orgs,https://api.github.com/users/beliefer/repos,https://api.github.com/users/beliefer/events{/privacy},https://api.github.com/users/beliefer/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34984,https://github.com/apache/spark/pull/34984,https://github.com/apache/spark/pull/34984.diff,https://github.com/apache/spark/pull/34984.patch,,https://api.github.com/repos/apache/spark/issues/34984/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
33,https://api.github.com/repos/apache/spark/issues/34982,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34982/labels{/name},https://api.github.com/repos/apache/spark/issues/34982/comments,https://api.github.com/repos/apache/spark/issues/34982/events,https://github.com/apache/spark/pull/34982,1086550167,PR_kwDOAQXtWs4wK4oI,34982,[SPARK-37712][YARN] Spark request yarn cluster metrics slow cause delay,"[{'id': 1427970157, 'node_id': 'MDU6TGFiZWwxNDI3OTcwMTU3', 'url': 'https://api.github.com/repos/apache/spark/labels/YARN', 'name': 'YARN', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,5,2021-12-22T07:53:37Z,2021-12-28T04:38:44Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?
Spark will request yarn cluster metrics and print a log about nodemanager number, it's not so important and this rpc is always slow
![image](https://user-images.githubusercontent.com/46485123/147055954-30698764-b313-419f-8759-772ad9f301ff.png)

We can make it as debug level


### Why are the changes needed?
Avoid unnecessary delay when submit application.


### Does this PR introduce _any_ user-facing change?
No


### How was this patch tested?
Not need
",https://api.github.com/repos/apache/spark/issues/34982/timeline,,spark,apache,AngersZhuuuu,46485123,MDQ6VXNlcjQ2NDg1MTIz,https://avatars.githubusercontent.com/u/46485123?v=4,,https://api.github.com/users/AngersZhuuuu,https://github.com/AngersZhuuuu,https://api.github.com/users/AngersZhuuuu/followers,https://api.github.com/users/AngersZhuuuu/following{/other_user},https://api.github.com/users/AngersZhuuuu/gists{/gist_id},https://api.github.com/users/AngersZhuuuu/starred{/owner}{/repo},https://api.github.com/users/AngersZhuuuu/subscriptions,https://api.github.com/users/AngersZhuuuu/orgs,https://api.github.com/users/AngersZhuuuu/repos,https://api.github.com/users/AngersZhuuuu/events{/privacy},https://api.github.com/users/AngersZhuuuu/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34982,https://github.com/apache/spark/pull/34982,https://github.com/apache/spark/pull/34982.diff,https://github.com/apache/spark/pull/34982.patch,,https://api.github.com/repos/apache/spark/issues/34982/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
34,https://api.github.com/repos/apache/spark/issues/34980,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34980/labels{/name},https://api.github.com/repos/apache/spark/issues/34980/comments,https://api.github.com/repos/apache/spark/issues/34980/events,https://github.com/apache/spark/pull/34980,1086284530,PR_kwDOAQXtWs4wKBZd,34980,[SPARK-37710][CORE] Add detailed error message for java.io.IOException occurring on Kryo flow,"[{'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,5,2021-12-21T22:38:40Z,2021-12-31T02:58:33Z,,MEMBER,,False,"### What changes were proposed in this pull request?
`java.io.IOException: Input/output error` usually points environmental issues such as disk read/write failures due to disk corruption, network access failures etc. This PR aims to be added clear error message to catch this kind of environmental cases occurring on `BlockManager` and logs with `BlockManager hostname`, `blockId` and `blockPath`.

### Why are the changes needed?
This kind of problems usually environmental problems and clear error message can help its analysis and save RCA time.

Following stack-trace is from `disk corruption` case:
```
com.esotericsoftware.kryo.KryoException: java.io.IOException: Input/output error
Serialization trace:
buffers (org.apache.spark.sql.execution.columnar.DefaultCachedBatch)
    at com.esotericsoftware.kryo.io.Input.fill(Input.java:166)
    at com.esotericsoftware.kryo.io.Input.require(Input.java:196)
    at com.esotericsoftware.kryo.io.Input.readBytes(Input.java:346)
    at com.esotericsoftware.kryo.io.Input.readBytes(Input.java:326)
    at com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ByteArraySerializer.read(DefaultArraySerializers.java:55)
    at com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ByteArraySerializer.read(DefaultArraySerializers.java:38)
    at com.esotericsoftware.kryo.Kryo.readObjectOrNull(Kryo.java:789)
    at com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ObjectArraySerializer.read(DefaultArraySerializers.java:381)
    at com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ObjectArraySerializer.read(DefaultArraySerializers.java:302)
    at com.esotericsoftware.kryo.Kryo.readObjectOrNull(Kryo.java:789)
    at com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:132)
    at com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:543)
    at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:816)
    at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:296)
    at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:168)
    at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
    at org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)
    at org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)
    at org.apache.spark.storage.BlockManager.maybeCacheDiskValuesInMemory(BlockManager.scala:1569)
    at org.apache.spark.storage.BlockManager.getLocalValues(BlockManager.scala:877)
    at org.apache.spark.storage.BlockManager.get(BlockManager.scala:1163)
...
Caused by: java.io.IOException: Input/output error
    at java.io.FileInputStream.readBytes(Native Method)
    at java.io.FileInputStream.read(FileInputStream.java:255)
    at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)
    at java.io.BufferedInputStream.read1(BufferedInputStream.java:286)
    at java.io.BufferedInputStream.read(BufferedInputStream.java:345)
    at net.jpountz.lz4.LZ4BlockInputStream.tryReadFully(LZ4BlockInputStream.java:269)
    at net.jpountz.lz4.LZ4BlockInputStream.readFully(LZ4BlockInputStream.java:280)
    at net.jpountz.lz4.LZ4BlockInputStream.refill(LZ4BlockInputStream.java:243)
    at net.jpountz.lz4.LZ4BlockInputStream.read(LZ4BlockInputStream.java:157)
    at com.esotericsoftware.kryo.io.Input.fill(Input.java:164)
    ... 87 more 
```

Proposed Error Message:
```
java.io.IOException: Input/output error. Please check if environment status is healthy for disk corruption, 
network failure (etc). BlockManagerId(driver, localhost, 58677, None) - blockName: test_my-block-id 
- blockDiskPath: /private/var/folders/kj/mccyycwn6mjdwnglw9g3k6pm0000gq/T/
blockmgr-24325b3a-0045-483a-98b8-673a2e07bed1/11/test_my-block-id
```

### Does this PR introduce _any_ user-facing change?
No

### How was this patch tested?
Added 2 new Unit Tests by reproducing the issue with `TestDiskCorruptedInputStream` and logging new error message when getting data blocks from disk.
",https://api.github.com/repos/apache/spark/issues/34980/timeline,,spark,apache,erenavsarogullari,1437738,MDQ6VXNlcjE0Mzc3Mzg=,https://avatars.githubusercontent.com/u/1437738?v=4,,https://api.github.com/users/erenavsarogullari,https://github.com/erenavsarogullari,https://api.github.com/users/erenavsarogullari/followers,https://api.github.com/users/erenavsarogullari/following{/other_user},https://api.github.com/users/erenavsarogullari/gists{/gist_id},https://api.github.com/users/erenavsarogullari/starred{/owner}{/repo},https://api.github.com/users/erenavsarogullari/subscriptions,https://api.github.com/users/erenavsarogullari/orgs,https://api.github.com/users/erenavsarogullari/repos,https://api.github.com/users/erenavsarogullari/events{/privacy},https://api.github.com/users/erenavsarogullari/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34980,https://github.com/apache/spark/pull/34980,https://github.com/apache/spark/pull/34980.diff,https://github.com/apache/spark/pull/34980.patch,,https://api.github.com/repos/apache/spark/issues/34980/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
35,https://api.github.com/repos/apache/spark/issues/34970,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34970/labels{/name},https://api.github.com/repos/apache/spark/issues/34970/comments,https://api.github.com/repos/apache/spark/issues/34970/events,https://github.com/apache/spark/pull/34970,1085588191,PR_kwDOAQXtWs4wHsP7,34970,[DO NOT MERGE] investigate test failures if we test ANSI mode in github actions,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,6,2021-12-21T08:55:18Z,2021-12-22T08:49:05Z,,MEMBER,,False,I am thinking about adding a new Github action job for testing ANSI mode. This PR is to collect the related test failures.,https://api.github.com/repos/apache/spark/issues/34970/timeline,,spark,apache,gengliangwang,1097932,MDQ6VXNlcjEwOTc5MzI=,https://avatars.githubusercontent.com/u/1097932?v=4,,https://api.github.com/users/gengliangwang,https://github.com/gengliangwang,https://api.github.com/users/gengliangwang/followers,https://api.github.com/users/gengliangwang/following{/other_user},https://api.github.com/users/gengliangwang/gists{/gist_id},https://api.github.com/users/gengliangwang/starred{/owner}{/repo},https://api.github.com/users/gengliangwang/subscriptions,https://api.github.com/users/gengliangwang/orgs,https://api.github.com/users/gengliangwang/repos,https://api.github.com/users/gengliangwang/events{/privacy},https://api.github.com/users/gengliangwang/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34970,https://github.com/apache/spark/pull/34970,https://github.com/apache/spark/pull/34970.diff,https://github.com/apache/spark/pull/34970.patch,,https://api.github.com/repos/apache/spark/issues/34970/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
36,https://api.github.com/repos/apache/spark/issues/34964,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34964/labels{/name},https://api.github.com/repos/apache/spark/issues/34964/comments,https://api.github.com/repos/apache/spark/issues/34964/events,https://github.com/apache/spark/pull/34964,1085357606,PR_kwDOAQXtWs4wG8OV,34964,[WIP][SPARK-37681][SQL] Support ANSI Aggregate Function: regr_sxy,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,3,2021-12-21T01:57:06Z,2021-12-22T13:15:36Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?
This PR used to support ANSI aggregate Function: `regr_sxy`

The mainstream database supports `regr_sxy` show below:
**Teradata**
https://docs.teradata.com/r/kmuOwjp1zEYg98JsB8fu_A/MXr3jFyWutZ9J4fhlXv_fQ
**Snowflake**
https://docs.snowflake.com/en/sql-reference/functions/regr_sxy.html
**Oracle**
https://docs.oracle.com/en/database/oracle/oracle-database/19/sqlrf/REGR_-Linear-Regression-Functions.html#GUID-A675B68F-2A88-4843-BE2C-FCDE9C65F9A9
**H2**
http://www.h2database.com/html/functions-aggregate.html#regr_sxy
**Postgresql**
https://www.postgresql.org/docs/8.4/functions-aggregate.html
**Sybase**
https://infocenter.sybase.com/help/index.jsp?topic=/com.sybase.help.sqlanywhere.12.0.0/dbreference/regr-sxy-function.html
**Exasol**
https://docs.exasol.com/sql_references/functions/alphabeticallistfunctions/regr_function.htm


### Why are the changes needed?
`regr_sxy` is very useful.


### Does this PR introduce _any_ user-facing change?
'Yes'. New feature.


### How was this patch tested?
New tests.
",https://api.github.com/repos/apache/spark/issues/34964/timeline,,spark,apache,beliefer,8486025,MDQ6VXNlcjg0ODYwMjU=,https://avatars.githubusercontent.com/u/8486025?v=4,,https://api.github.com/users/beliefer,https://github.com/beliefer,https://api.github.com/users/beliefer/followers,https://api.github.com/users/beliefer/following{/other_user},https://api.github.com/users/beliefer/gists{/gist_id},https://api.github.com/users/beliefer/starred{/owner}{/repo},https://api.github.com/users/beliefer/subscriptions,https://api.github.com/users/beliefer/orgs,https://api.github.com/users/beliefer/repos,https://api.github.com/users/beliefer/events{/privacy},https://api.github.com/users/beliefer/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34964,https://github.com/apache/spark/pull/34964,https://github.com/apache/spark/pull/34964.diff,https://github.com/apache/spark/pull/34964.patch,,https://api.github.com/repos/apache/spark/issues/34964/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
37,https://api.github.com/repos/apache/spark/issues/34956,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34956/labels{/name},https://api.github.com/repos/apache/spark/issues/34956/comments,https://api.github.com/repos/apache/spark/issues/34956/events,https://github.com/apache/spark/pull/34956,1084465193,PR_kwDOAQXtWs4wECV4,34956,[SPARK-37688][CORE] ExecutorMonitor should ignore SparkListenerBlockUpdated event if executor was not active,"[{'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,3,2021-12-20T07:48:34Z,2021-12-28T07:51:12Z,,CONTRIBUTOR,,False,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->
`ExecutorMonitor` should ignore `SparkListenerBlockUpdated` event if executor was not active

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->
If not ignored, `ExecutorMonitor` will create a new executor tracker with UNKNOWN_RESOURCE_PROFILE_ID for the dead executor. And `ExecutorAllocationManager` will not remove executor with UNKNOWN_RESOURCE_PROFILE_ID, which cause a executor slot is occupied by the dead executor, so a new one cannot be created.

### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->

No

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.
-->

Add a new test.",https://api.github.com/repos/apache/spark/issues/34956/timeline,,spark,apache,sleep1661,8802096,MDQ6VXNlcjg4MDIwOTY=,https://avatars.githubusercontent.com/u/8802096?v=4,,https://api.github.com/users/sleep1661,https://github.com/sleep1661,https://api.github.com/users/sleep1661/followers,https://api.github.com/users/sleep1661/following{/other_user},https://api.github.com/users/sleep1661/gists{/gist_id},https://api.github.com/users/sleep1661/starred{/owner}{/repo},https://api.github.com/users/sleep1661/subscriptions,https://api.github.com/users/sleep1661/orgs,https://api.github.com/users/sleep1661/repos,https://api.github.com/users/sleep1661/events{/privacy},https://api.github.com/users/sleep1661/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34956,https://github.com/apache/spark/pull/34956,https://github.com/apache/spark/pull/34956.diff,https://github.com/apache/spark/pull/34956.patch,,https://api.github.com/repos/apache/spark/issues/34956/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
38,https://api.github.com/repos/apache/spark/issues/34953,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34953/labels{/name},https://api.github.com/repos/apache/spark/issues/34953/comments,https://api.github.com/repos/apache/spark/issues/34953/events,https://github.com/apache/spark/pull/34953,1084329653,PR_kwDOAQXtWs4wDlqE,34953,[SPARK-37682][SQL]Apply 'merged column' and 'bit vector' in RewriteDistinctAggregates,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,3,2021-12-20T04:00:02Z,2021-12-23T06:27:33Z,,NONE,,False,"### What changes were proposed in this pull request?
Adjust the grouping rules of `distinctAggGroups`, specifically in `RewriteDistinctAggregates.groupDistinctAggExpr`, so that some 'distinct' can be grouped together, and conditions(eg. CaseWhen, If) involved in them will be stored in the 'if_vector' to avoid unnecessary expanding. The 'if_vector' and 'filter_vector' introduced here can reduce the number of columns in the expand. Besides, children in distinct aggregate function with same datatype will share same project column.
Here is a example comparing the difference between the original expand rewriting and the new with 'merged column' and 'bit vector' (in sql):
```sql
SELECT
  COUNT(DISTINCT cat1) FILTER (WHERE id > 1) as cat1_filter_cnt_dist,
  COUNT(DISTINCT cat2) FILTER (WHERE id > 2) as cat2_filter_cnt_dist,
  COUNT(DISTINCT IF(value > 5, cat1, null)) as cat1_if_cnt_dist,
  COUNT(DISTINCT id) as id_cnt_dist,
  SUM(DISTINCT value) as id_sum_dist
FROM data
GROUP BY key
```
Current rule will rewrite the above sql plan to the following (pseudo) logical plan:
```
Aggregate(
   key = ['key]
   functions = [
       count('cat1) FILTER (WHERE (('gid = 1) AND 'max(id > 1))),
       count('(IF((value > 5), cat1, null))) FILTER (WHERE ('gid = 5)),
       count('cat2) FILTER (WHERE (('gid = 3) AND 'max(id > 2))),
       count('id) FILTER (WHERE ('gid = 2)),
       sum('value) FILTER (WHERE ('gid = 4))
   ]
   output = ['key, 'cat1_filter_cnt_dist, 'cat2_filter_cnt_dist, 'cat1_if_cnt_dist,
             'id_cnt_dist, 'id_sum_dist])
  Aggregate(
     key = ['key, 'cat1, 'value, 'cat2, '(IF((value > 5), cat1, null)), 'id, 'gid]
     functions = [max('id > 1), max('id > 2)]
     output = ['key, 'cat1, 'value, 'cat2, '(IF((value > 5), cat1, null)), 'id, 'gid,
               'max(id > 1), 'max(id > 2)])
    Expand(
       projections = [
         ('key, 'cat1, null, null, null, null, 1, ('id > 1), null),
         ('key, null, null, null, null, 'id, 2, null, null),
         ('key, null, null, 'cat2, null, null, 3, null, ('id > 2)),
         ('key, null, 'value, null, null, null, 4, null, null),
         ('key, null, null, null, if (('value > 5)) 'cat1 else null, null, 5, null, null)
       ]
       output = ['key, 'cat1, 'value, 'cat2, '(IF((value > 5), cat1, null)), 'id,
                 'gid, '(id > 1), '(id > 2)])
      LocalTableScan [...]
```
After applying 'merged column' and 'bit vector' tricks, the logical plan will become:
```
Aggregate(
   key = ['key]
   functions = [
       count('merged_string_1) FILTER (WHERE (('gid = 1) AND NOT (('filter_vector_1 & 1) = 0))),
       count(if (NOT (('if_vector_1 & 1) = 0)) 'merged_string_1 else null) FILTER (WHERE ('gid = 1)),
       count('merged_string_1) FILTER (WHERE (('gid = 2) AND NOT (('filter_vector_1 & 1) = 0))),
       count('merged_integer_1) FILTER (WHERE ('gid = 3)),
       sum('merged_integer_1) FILTER (WHERE ('gid = 4))
   ]
   output = ['key, 'cat1_filter_cnt_dist, 'cat2_filter_cnt_dist, 'cat1_if_cnt_dist,
             'id_cnt_dist, 'id_sum_dist])
  Aggregate(
     key = ['key, 'merged_string_1, 'merged_integer_1, 'gid]
     functions = [bit_or('if_vector_1),bit_or('filter_vector_1)]
     output = ['key, 'merged_string_1, 'merged_integer_1, 'gid, 'bit_or(if_vector_1), 'bit_or(filter_vector_1)])
    Expand(
       projections = [
         ('key, 'cat1, null, 1, if ('value > 5) 1 else 0, if ('id > 1) 1 else 0),
         ('key, 'cat2, null, 2, null, if ('id > 2) 1 else 0),
         ('key, null, 'id, 3, null, null),
         ('key, null, 'value, 4, null, null)
       ]
       output = ['key, 'merged_string_1, 'merged_integer_1, 'gid, 'if_vector_1, 'filter_vector_1])
      LocalTableScan [...]
```


### Why are the changes needed?
It can save mass memory and improve performance in some cases like:
```sql
SELECT 
  count(distinct case when cond1 then col1 end),
  count(distinct case when cond2 then col1 end),
  ...
FROM data
```

### Does this PR introduce _any_ user-facing change?
No


### How was this patch tested?
Existing test and a new UT in DataFrameAggregateSuite to test 'Vector Size larger than 64'.
I have written some SQL locally to test the correctness of the distinct calculation, but it seems difficult to cover most of the cases. Perhaps spark's existing test set will be more comprehensive, so I didn't leave it in the code.
",https://api.github.com/repos/apache/spark/issues/34953/timeline,,spark,apache,Flyangz,39462475,MDQ6VXNlcjM5NDYyNDc1,https://avatars.githubusercontent.com/u/39462475?v=4,,https://api.github.com/users/Flyangz,https://github.com/Flyangz,https://api.github.com/users/Flyangz/followers,https://api.github.com/users/Flyangz/following{/other_user},https://api.github.com/users/Flyangz/gists{/gist_id},https://api.github.com/users/Flyangz/starred{/owner}{/repo},https://api.github.com/users/Flyangz/subscriptions,https://api.github.com/users/Flyangz/orgs,https://api.github.com/users/Flyangz/repos,https://api.github.com/users/Flyangz/events{/privacy},https://api.github.com/users/Flyangz/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34953,https://github.com/apache/spark/pull/34953,https://github.com/apache/spark/pull/34953.diff,https://github.com/apache/spark/pull/34953.patch,,https://api.github.com/repos/apache/spark/issues/34953/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
39,https://api.github.com/repos/apache/spark/issues/34951,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34951/labels{/name},https://api.github.com/repos/apache/spark/issues/34951/comments,https://api.github.com/repos/apache/spark/issues/34951/events,https://github.com/apache/spark/pull/34951,1084066729,PR_kwDOAQXtWs4wCyal,34951,[SPARK-37686][PYTHON][SQL] Use _invoke_function helpers for all pyspark.sql.functions,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1982260031, 'node_id': 'MDU6TGFiZWwxOTgyMjYwMDMx', 'url': 'https://api.github.com/repos/apache/spark/labels/PYTHON', 'name': 'PYTHON', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,8,2021-12-19T10:39:06Z,2021-12-30T16:21:30Z,,MEMBER,,False,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->

This PR proposes conversion of functions not covered by SPARK-32084 to `_invoke_functions` style.

Two new `_invoke` functions where added:

- `_invoke_function_over_columns`
- `_invoke_function_over_seq_of_columns`

to address common examples.


### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->

To reduce boilerplate (especially related to type checking) and improve manageability.

Additionally, it opens opportunity for reducing driver-side invocation latency.

### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->

No.

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.
-->

Existing tests.",https://api.github.com/repos/apache/spark/issues/34951/timeline,,spark,apache,zero323,1554276,MDQ6VXNlcjE1NTQyNzY=,https://avatars.githubusercontent.com/u/1554276?v=4,,https://api.github.com/users/zero323,https://github.com/zero323,https://api.github.com/users/zero323/followers,https://api.github.com/users/zero323/following{/other_user},https://api.github.com/users/zero323/gists{/gist_id},https://api.github.com/users/zero323/starred{/owner}{/repo},https://api.github.com/users/zero323/subscriptions,https://api.github.com/users/zero323/orgs,https://api.github.com/users/zero323/repos,https://api.github.com/users/zero323/events{/privacy},https://api.github.com/users/zero323/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34951,https://github.com/apache/spark/pull/34951,https://github.com/apache/spark/pull/34951.diff,https://github.com/apache/spark/pull/34951.patch,,https://api.github.com/repos/apache/spark/issues/34951/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
40,https://api.github.com/repos/apache/spark/issues/34946,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34946/labels{/name},https://api.github.com/repos/apache/spark/issues/34946/comments,https://api.github.com/repos/apache/spark/issues/34946/events,https://github.com/apache/spark/pull/34946,1083830441,PR_kwDOAQXtWs4wCEUX,34946,[DO-NOT-MERGE][SPARK-37683][PYTHON] Type flaky access,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1406587328, 'node_id': 'MDU6TGFiZWwxNDA2NTg3MzI4', 'url': 'https://api.github.com/repos/apache/spark/labels/STRUCTURED%20STREAMING', 'name': 'STRUCTURED STREAMING', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1982260031, 'node_id': 'MDU6TGFiZWwxOTgyMjYwMDMx', 'url': 'https://api.github.com/repos/apache/spark/labels/PYTHON', 'name': 'PYTHON', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,7,2021-12-18T13:13:24Z,2021-12-19T12:37:08Z,,MEMBER,,False,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->

This PR adds type annotations for certain internal variables, that seem to (nondeterministically) confuse mypy in data tests.


### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->

To avoid flaky tests.

### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->

No.

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.
-->

Existing tests.",https://api.github.com/repos/apache/spark/issues/34946/timeline,,spark,apache,zero323,1554276,MDQ6VXNlcjE1NTQyNzY=,https://avatars.githubusercontent.com/u/1554276?v=4,,https://api.github.com/users/zero323,https://github.com/zero323,https://api.github.com/users/zero323/followers,https://api.github.com/users/zero323/following{/other_user},https://api.github.com/users/zero323/gists{/gist_id},https://api.github.com/users/zero323/starred{/owner}{/repo},https://api.github.com/users/zero323/subscriptions,https://api.github.com/users/zero323/orgs,https://api.github.com/users/zero323/repos,https://api.github.com/users/zero323/events{/privacy},https://api.github.com/users/zero323/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34946,https://github.com/apache/spark/pull/34946,https://github.com/apache/spark/pull/34946.diff,https://github.com/apache/spark/pull/34946.patch,,https://api.github.com/repos/apache/spark/issues/34946/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
41,https://api.github.com/repos/apache/spark/issues/34943,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34943/labels{/name},https://api.github.com/repos/apache/spark/issues/34943/comments,https://api.github.com/repos/apache/spark/issues/34943/events,https://github.com/apache/spark/pull/34943,1083774405,PR_kwDOAQXtWs4wB6Eb,34943,[WIP][SPARK-37672][SQL] Support ANSI Aggregate Function: regr_sxx,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,9,2021-12-18T07:46:42Z,2021-12-22T13:15:53Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?
This PR used to support ANSI aggregate Function: `regr_sxx`

The mainstream database supports `regr_sxx` show below:
**Teradata**
https://docs.teradata.com/r/kmuOwjp1zEYg98JsB8fu_A/PBEW1OPIaxqkIf3CJfIr6A
**Snowflake**
https://docs.snowflake.com/en/sql-reference/functions/regr_sxx.html
**Oracle**
https://docs.oracle.com/en/database/oracle/oracle-database/19/sqlrf/REGR_-Linear-Regression-Functions.html#GUID-A675B68F-2A88-4843-BE2C-FCDE9C65F9A9
**H2**
http://www.h2database.com/html/functions-aggregate.html#regr_sxx
**Postgresql**
https://www.postgresql.org/docs/8.4/functions-aggregate.html
**Sybase**
https://infocenter.sybase.com/help/index.jsp?topic=/com.sybase.help.sqlanywhere.12.0.0/dbreference/regr-sxx-function.html
**Exasol**
https://docs.exasol.com/sql_references/functions/alphabeticallistfunctions/regr_function.htm


### Why are the changes needed?
`regr_sxx` is very useful.


### Does this PR introduce _any_ user-facing change?
'Yes'. New feature.


### How was this patch tested?
New tests.
",https://api.github.com/repos/apache/spark/issues/34943/timeline,,spark,apache,beliefer,8486025,MDQ6VXNlcjg0ODYwMjU=,https://avatars.githubusercontent.com/u/8486025?v=4,,https://api.github.com/users/beliefer,https://github.com/beliefer,https://api.github.com/users/beliefer/followers,https://api.github.com/users/beliefer/following{/other_user},https://api.github.com/users/beliefer/gists{/gist_id},https://api.github.com/users/beliefer/starred{/owner}{/repo},https://api.github.com/users/beliefer/subscriptions,https://api.github.com/users/beliefer/orgs,https://api.github.com/users/beliefer/repos,https://api.github.com/users/beliefer/events{/privacy},https://api.github.com/users/beliefer/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34943,https://github.com/apache/spark/pull/34943,https://github.com/apache/spark/pull/34943.diff,https://github.com/apache/spark/pull/34943.patch,,https://api.github.com/repos/apache/spark/issues/34943/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
42,https://api.github.com/repos/apache/spark/issues/34940,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34940/labels{/name},https://api.github.com/repos/apache/spark/issues/34940/comments,https://api.github.com/repos/apache/spark/issues/34940/events,https://github.com/apache/spark/pull/34940,1083746549,PR_kwDOAQXtWs4wB0yr,34940,[PYTHON] Use raise ... from instead of simply raise where applicable,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1405801475, 'node_id': 'MDU6TGFiZWwxNDA1ODAxNDc1', 'url': 'https://api.github.com/repos/apache/spark/labels/ML', 'name': 'ML', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1405803268, 'node_id': 'MDU6TGFiZWwxNDA1ODAzMjY4', 'url': 'https://api.github.com/repos/apache/spark/labels/MLLIB', 'name': 'MLLIB', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1406627200, 'node_id': 'MDU6TGFiZWwxNDA2NjI3MjAw', 'url': 'https://api.github.com/repos/apache/spark/labels/BUILD', 'name': 'BUILD', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1982260031, 'node_id': 'MDU6TGFiZWwxOTgyMjYwMDMx', 'url': 'https://api.github.com/repos/apache/spark/labels/PYTHON', 'name': 'PYTHON', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,1,2021-12-18T04:49:31Z,2021-12-18T04:49:52Z,,NONE,,False,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->
- use raise from to maintain cause traceability instead of raising a new exception during the handling of a prior exception


### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->
- using `raise ... from` maintains exception traceability and provides better tracebacks in tools such as Sentry
- see e.g. https://stackoverflow.com/questions/24752395/python-raise-from-usage


### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->
- yes, traceback chainings will be improved and more understandable


### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.
-->
- this change is mainly cosmetic, no tests were added",https://api.github.com/repos/apache/spark/issues/34940/timeline,,spark,apache,martimlobao,6430786,MDQ6VXNlcjY0MzA3ODY=,https://avatars.githubusercontent.com/u/6430786?v=4,,https://api.github.com/users/martimlobao,https://github.com/martimlobao,https://api.github.com/users/martimlobao/followers,https://api.github.com/users/martimlobao/following{/other_user},https://api.github.com/users/martimlobao/gists{/gist_id},https://api.github.com/users/martimlobao/starred{/owner}{/repo},https://api.github.com/users/martimlobao/subscriptions,https://api.github.com/users/martimlobao/orgs,https://api.github.com/users/martimlobao/repos,https://api.github.com/users/martimlobao/events{/privacy},https://api.github.com/users/martimlobao/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34940,https://github.com/apache/spark/pull/34940,https://github.com/apache/spark/pull/34940.diff,https://github.com/apache/spark/pull/34940.patch,,https://api.github.com/repos/apache/spark/issues/34940/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
43,https://api.github.com/repos/apache/spark/issues/34939,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34939/labels{/name},https://api.github.com/repos/apache/spark/issues/34939/comments,https://api.github.com/repos/apache/spark/issues/34939/events,https://github.com/apache/spark/pull/34939,1083742137,PR_kwDOAQXtWs4wBz9z,34939,[SPARK-37258][K8S][BUILD] Upgrade kubernetes-client to 5.11.1,"[{'id': 1406605057, 'node_id': 'MDU6TGFiZWwxNDA2NjA1MDU3', 'url': 'https://api.github.com/repos/apache/spark/labels/KUBERNETES', 'name': 'KUBERNETES', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1406627200, 'node_id': 'MDU6TGFiZWwxNDA2NjI3MjAw', 'url': 'https://api.github.com/repos/apache/spark/labels/BUILD', 'name': 'BUILD', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,28,2021-12-18T04:17:35Z,2022-01-01T00:28:39Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?
This patch aims to upgrade `kubernetes-client` from 5.10.1 to 5.11.1, the Volcano extension was supported since v5.11, it would be useful for users who want to use Volcano as customized scheduler in Spark on K8S.

There are also a fix to make sure it is compatiable to create http client with custom dispatcher.

### Why are the changes needed?

This will bring the several bug fixes and improvements (such as Volcano/Istio support), see more in:
-  https://github.com/fabric8io/kubernetes-client/releases/tag/v5.11.0
-  https://github.com/fabric8io/kubernetes-client/releases/tag/v5.11.1


### Does this PR introduce _any_ user-facing change?
No.

### How was this patch tested?
Pass the CIs
",https://api.github.com/repos/apache/spark/issues/34939/timeline,,spark,apache,Yikun,1736354,MDQ6VXNlcjE3MzYzNTQ=,https://avatars.githubusercontent.com/u/1736354?v=4,,https://api.github.com/users/Yikun,https://github.com/Yikun,https://api.github.com/users/Yikun/followers,https://api.github.com/users/Yikun/following{/other_user},https://api.github.com/users/Yikun/gists{/gist_id},https://api.github.com/users/Yikun/starred{/owner}{/repo},https://api.github.com/users/Yikun/subscriptions,https://api.github.com/users/Yikun/orgs,https://api.github.com/users/Yikun/repos,https://api.github.com/users/Yikun/events{/privacy},https://api.github.com/users/Yikun/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34939,https://github.com/apache/spark/pull/34939,https://github.com/apache/spark/pull/34939.diff,https://github.com/apache/spark/pull/34939.patch,,https://api.github.com/repos/apache/spark/issues/34939/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
44,https://api.github.com/repos/apache/spark/issues/34934,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34934/labels{/name},https://api.github.com/repos/apache/spark/issues/34934/comments,https://api.github.com/repos/apache/spark/issues/34934/events,https://github.com/apache/spark/pull/34934,1083106432,PR_kwDOAQXtWs4v_uSB,34934,[SPARK-37675][CORE][SHUFFLE] Return PushMergedRemoteMetaFailedFetchResult if no available push-merged block,"[{'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,11,2021-12-17T10:23:20Z,2021-12-30T10:00:09Z,,MEMBER,,False,"### What changes were proposed in this pull request?

When push-based shuffle enabled, reduce task will ask ESS for MergedMetas, then `PushMergedRemoteMetaFailedFetchResult` should be returned instead of `PushMergedRemoteMetaFetchResult` if there is no available push-merged block on ESS.

### Why are the changes needed?

Because that push-based shuffle works as best-effort, there are opportunities that no chunks of the block are available on ESS, in current implementation, it will cause reduce task failed with `ArithmeticException: / by zero`.

```
Caused by: java.lang.ArithmeticException: / by zero
	at org.apache.spark.storage.PushBasedFetchHelper.createChunkBlockInfosFromMetaResponse(PushBasedFetchHelper.scala:117)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:980)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:84)
	at org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)
```

After the change, a `PushMergedRemoteMetaFetchResult` will let reduce task fall back to fetching the original blocks.

### Does this PR introduce _any_ user-facing change?

Yes, it'a bug fix. The change makes push-based shuffle stable. Before this change, my 1T TPCDS test failed several times w/ `ArithmeticException: / by zero`, after the change, passed w/o any exception.

### How was this patch tested?

Existing tests, and run 1T TPCDS manually.",https://api.github.com/repos/apache/spark/issues/34934/timeline,,spark,apache,pan3793,26535726,MDQ6VXNlcjI2NTM1NzI2,https://avatars.githubusercontent.com/u/26535726?v=4,,https://api.github.com/users/pan3793,https://github.com/pan3793,https://api.github.com/users/pan3793/followers,https://api.github.com/users/pan3793/following{/other_user},https://api.github.com/users/pan3793/gists{/gist_id},https://api.github.com/users/pan3793/starred{/owner}{/repo},https://api.github.com/users/pan3793/subscriptions,https://api.github.com/users/pan3793/orgs,https://api.github.com/users/pan3793/repos,https://api.github.com/users/pan3793/events{/privacy},https://api.github.com/users/pan3793/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34934,https://github.com/apache/spark/pull/34934,https://github.com/apache/spark/pull/34934.diff,https://github.com/apache/spark/pull/34934.patch,,https://api.github.com/repos/apache/spark/issues/34934/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
45,https://api.github.com/repos/apache/spark/issues/34933,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34933/labels{/name},https://api.github.com/repos/apache/spark/issues/34933/comments,https://api.github.com/repos/apache/spark/issues/34933/events,https://github.com/apache/spark/pull/34933,1083025946,PR_kwDOAQXtWs4v_dOV,34933,[SPARK-37674][SQL] Reduce the output partition of output stage to avoid producing small files.,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,1,2021-12-17T08:54:18Z,2021-12-17T09:27:58Z,,CONTRIBUTOR,,False,"
### What changes were proposed in this pull request?
Reduce the output partition of output stage to avoid producing small files.

### Why are the changes needed?

The partition size of the finalStage with `DataWritingCommand` or `V2TableWriteExec`  may use the `ADVISORY_PARTITION_SIZE_IN_BYTES` which is smaller one, and  may produce some small files, it is bad for production.

Sometime, we may adjust `ADVISORY_PARTITION_SIZE_IN_BYTES` to a big one to avoid above , but it is NOT a good idea, it may take effect other Jobs or stages to  coalesce small shuffle partitions or split skewed shuffle partition.

So we should introduce a new partition size instead of  `ADVISORY_PARTITION_SIZE_IN_BYTES`  for  the finalStage with `DataWritingCommand` or `V2TableWriteExec`  to avoid small files.


### Does this PR introduce _any_ user-facing change?

NO

### How was this patch tested?
Added unittests.
",https://api.github.com/repos/apache/spark/issues/34933/timeline,,spark,apache,weixiuli,39684231,MDQ6VXNlcjM5Njg0MjMx,https://avatars.githubusercontent.com/u/39684231?v=4,,https://api.github.com/users/weixiuli,https://github.com/weixiuli,https://api.github.com/users/weixiuli/followers,https://api.github.com/users/weixiuli/following{/other_user},https://api.github.com/users/weixiuli/gists{/gist_id},https://api.github.com/users/weixiuli/starred{/owner}{/repo},https://api.github.com/users/weixiuli/subscriptions,https://api.github.com/users/weixiuli/orgs,https://api.github.com/users/weixiuli/repos,https://api.github.com/users/weixiuli/events{/privacy},https://api.github.com/users/weixiuli/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34933,https://github.com/apache/spark/pull/34933,https://github.com/apache/spark/pull/34933.diff,https://github.com/apache/spark/pull/34933.patch,,https://api.github.com/repos/apache/spark/issues/34933/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
46,https://api.github.com/repos/apache/spark/issues/34929,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34929/labels{/name},https://api.github.com/repos/apache/spark/issues/34929/comments,https://api.github.com/repos/apache/spark/issues/34929/events,https://github.com/apache/spark/pull/34929,1082897559,PR_kwDOAQXtWs4v_Cgv,34929,[SPARK-37670][SQL] Support predicate pushdown and column pruning for de-duped CTEs,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,3,2021-12-17T05:18:06Z,2021-12-20T14:39:23Z,,CONTRIBUTOR,,False,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->

This PR adds predicate push-down and column pruning to CTEs that are not inlined as well as fixes a few potential correctness issues:
  1) Replace (previously not inlined) CTE refs with Repartition operations at the end of logical plan optimization so that WithCTE is not carried over to physical plan. As a result, we can simplify the logic of physical planning, as well as avoid a correctness issue where the logical link of a physical plan node can point to `WithCTE` and lead to unexpected behaviors in AQE, e.g., class cast exceptions in DPP.
  2) Pull (not inlined) CTE defs from subqueries up to the main query level, in order to avoid creating copies of the same CTE def during predicate push-downs and other transformations.
  3) Make CTE IDs more deterministic by starting from 0 for each query.

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->

Improve de-duped CTEs' performance with predicate pushdown and column pruning; fixes de-duped CTEs' correctness issues.

### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->

No.

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.
-->

Added UTs.",https://api.github.com/repos/apache/spark/issues/34929/timeline,,spark,apache,maryannxue,4171904,MDQ6VXNlcjQxNzE5MDQ=,https://avatars.githubusercontent.com/u/4171904?v=4,,https://api.github.com/users/maryannxue,https://github.com/maryannxue,https://api.github.com/users/maryannxue/followers,https://api.github.com/users/maryannxue/following{/other_user},https://api.github.com/users/maryannxue/gists{/gist_id},https://api.github.com/users/maryannxue/starred{/owner}{/repo},https://api.github.com/users/maryannxue/subscriptions,https://api.github.com/users/maryannxue/orgs,https://api.github.com/users/maryannxue/repos,https://api.github.com/users/maryannxue/events{/privacy},https://api.github.com/users/maryannxue/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34929,https://github.com/apache/spark/pull/34929,https://github.com/apache/spark/pull/34929.diff,https://github.com/apache/spark/pull/34929.patch,,https://api.github.com/repos/apache/spark/issues/34929/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
47,https://api.github.com/repos/apache/spark/issues/34924,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34924/labels{/name},https://api.github.com/repos/apache/spark/issues/34924/comments,https://api.github.com/repos/apache/spark/issues/34924/events,https://github.com/apache/spark/pull/34924,1082450530,PR_kwDOAQXtWs4v9k0w,34924,[SPARK-37145][K8S] Add support for extending user feature steps with configuration,"[{'id': 1406605057, 'node_id': 'MDU6TGFiZWwxNDA2NjA1MDU3', 'url': 'https://api.github.com/repos/apache/spark/labels/KUBERNETES', 'name': 'KUBERNETES', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,6,2021-12-16T17:12:46Z,2021-12-21T02:18:19Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?

This patch adds the support for extending user feature steps with configuration.

Before this patch user can only add feature step like:
- `class TestStep extends KubernetesFeatureConfigStep`

After this patch user can add feature step with configuration like:
- `class TestStep extends KubernetesFeatureConfigStep`
- `class TestStepWithK8SConf(conf: KubernetesConf) extends KubernetesFeatureConfigStep`
- `class TestStepWithDriverConf(conf: KubernetesDriverConf) extends KubernetesFeatureConfigStep`
- `class TestStepWithExecConf(conf: KubernetesExecutorConf) extends KubernetesFeatureConfigStep`

### Why are the changes needed?
In https://github.com/apache/spark/pull/30206 , a developer API for custom feature steps has been added, but it didn't support initialize user feature step with kubernetes conf (like `KubernetesConf`/`KubernetesDriverConf`/`KubernetesExecutorConf`).

In most of scenarios, users want to make corresponding changes in their feature steps according to the configuration. Such as, the customized scheduler scenario, user wants to configure pod according to passed job configuration.

### Does this PR introduce _any_ user-facing change?
Improve the developer API for for custom feature steps.


### How was this patch tested?
Added UT",https://api.github.com/repos/apache/spark/issues/34924/timeline,,spark,apache,Yikun,1736354,MDQ6VXNlcjE3MzYzNTQ=,https://avatars.githubusercontent.com/u/1736354?v=4,,https://api.github.com/users/Yikun,https://github.com/Yikun,https://api.github.com/users/Yikun/followers,https://api.github.com/users/Yikun/following{/other_user},https://api.github.com/users/Yikun/gists{/gist_id},https://api.github.com/users/Yikun/starred{/owner}{/repo},https://api.github.com/users/Yikun/subscriptions,https://api.github.com/users/Yikun/orgs,https://api.github.com/users/Yikun/repos,https://api.github.com/users/Yikun/events{/privacy},https://api.github.com/users/Yikun/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34924,https://github.com/apache/spark/pull/34924,https://github.com/apache/spark/pull/34924.diff,https://github.com/apache/spark/pull/34924.patch,,https://api.github.com/repos/apache/spark/issues/34924/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
48,https://api.github.com/repos/apache/spark/issues/34923,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34923/labels{/name},https://api.github.com/repos/apache/spark/issues/34923/comments,https://api.github.com/repos/apache/spark/issues/34923/events,https://github.com/apache/spark/pull/34923,1082262223,PR_kwDOAQXtWs4v88Ld,34923,[SPARK-35739][SQL] Add Java-compatible Dataset.join overloads,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,1,2021-12-16T14:28:53Z,2021-12-16T14:37:27Z,,NONE,,False,"### What changes were proposed in this pull request?
Adds 3 new syntactic sugar overloads to Dataset's join method as proposed in [SPARK-35739](https://issues.apache.org/jira/browse/SPARK-35739).

### Why are the changes needed?
Improved development experience for developers using Spark SQL, specifically when coding in Java.  

Prior to changes the Seq overloads required developers to use less-known Java-to-Scala converter methods that made code less readable.  The overloads internalize those converter calls for two of the new methods and the third method adds a single-item overload that is useful for both Java and Scala.

### Does this PR introduce _any_ user-facing change?
Yes, the three new overloads technically constitute an API change to the Dataset class.  These overloads are net-new and have been commented appropriately in line with the existing methods.

### How was this patch tested?
Test cases were not added because it is unclear to me where/how syntactic sugar overloads fit into the testing suites (if at all).  Happy to add them if I can be pointed in the correct direction.

* Changes were tested in Scala via spark-shell.
* Changes were tested in Java by modifying an example:
  ```
  diff --git a/examples/src/main/java/org/apache/spark/examples/sql/JavaSparkSQLExample.java b/examples/src/main/java/org/apache/spark/examples/sql/JavaSparkSQLExample.java
  index 86a9045d8a..342810c1e6 100644
  --- a/examples/src/main/java/org/apache/spark/examples/sql/JavaSparkSQLExample.java
  +++ b/examples/src/main/java/org/apache/spark/examples/sql/JavaSparkSQLExample.java
  @@ -124,6 +124,10 @@ public class JavaSparkSQLExample {
       // |-- age: long (nullable = true)
       // |-- name: string (nullable = true)

  +    df.join(df, new String[] {""age""}).show();
  +    df.join(df, ""age"", ""left"").show();
  +    df.join(df, new String[] {""age""}, ""left"").show();
  +
       // Select only the ""name"" column
       df.select(""name"").show();
       // +-------+
  ```

#### Notes
Re-opening of #33323 with comments addressed.",https://api.github.com/repos/apache/spark/issues/34923/timeline,,spark,apache,brandondahler,1155895,MDQ6VXNlcjExNTU4OTU=,https://avatars.githubusercontent.com/u/1155895?v=4,,https://api.github.com/users/brandondahler,https://github.com/brandondahler,https://api.github.com/users/brandondahler/followers,https://api.github.com/users/brandondahler/following{/other_user},https://api.github.com/users/brandondahler/gists{/gist_id},https://api.github.com/users/brandondahler/starred{/owner}{/repo},https://api.github.com/users/brandondahler/subscriptions,https://api.github.com/users/brandondahler/orgs,https://api.github.com/users/brandondahler/repos,https://api.github.com/users/brandondahler/events{/privacy},https://api.github.com/users/brandondahler/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34923,https://github.com/apache/spark/pull/34923,https://github.com/apache/spark/pull/34923.diff,https://github.com/apache/spark/pull/34923.patch,,https://api.github.com/repos/apache/spark/issues/34923/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
49,https://api.github.com/repos/apache/spark/issues/34920,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34920/labels{/name},https://api.github.com/repos/apache/spark/issues/34920/comments,https://api.github.com/repos/apache/spark/issues/34920/events,https://github.com/apache/spark/pull/34920,1081933181,PR_kwDOAQXtWs4v72uH,34920,[SPARK-37661][SQL] SparkSQLCLIDriver shall avoid using hive defaults to resolve warehouse dir,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,4,2021-12-16T09:00:06Z,2021-12-18T16:35:58Z,,CONTRIBUTOR,,True,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->

```
21/12/16 15:27:26.713 main INFO SharedState: spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir.
21/12/16 15:27:26.761 main INFO SharedState: Warehouse path is 'file:/user/hive/warehouse'.

...
...

21/12/16 15:27:36.559 main INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
21/12/16 15:27:36.561 main INFO SharedState: Warehouse path is 'file:/Users/kentyao/Downloads/spark/spark-3.2.0-bin-hadoop3.2/spark-warehouse'.
```

With log4j file appender, you will be able to see the logs like the above one when using spark-sql shell.

The first time the warehouse is resolved with hive defaults from codebase but the latter does not.

Later, we do extra effort in HiveClient to set it right again.

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->

1. fix a minor bug in the log,
2. simply to fix potential bugs


### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->

no

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.
-->
the existing tests shall be enough",https://api.github.com/repos/apache/spark/issues/34920/timeline,,spark,apache,yaooqinn,8326978,MDQ6VXNlcjgzMjY5Nzg=,https://avatars.githubusercontent.com/u/8326978?v=4,,https://api.github.com/users/yaooqinn,https://github.com/yaooqinn,https://api.github.com/users/yaooqinn/followers,https://api.github.com/users/yaooqinn/following{/other_user},https://api.github.com/users/yaooqinn/gists{/gist_id},https://api.github.com/users/yaooqinn/starred{/owner}{/repo},https://api.github.com/users/yaooqinn/subscriptions,https://api.github.com/users/yaooqinn/orgs,https://api.github.com/users/yaooqinn/repos,https://api.github.com/users/yaooqinn/events{/privacy},https://api.github.com/users/yaooqinn/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34920,https://github.com/apache/spark/pull/34920,https://github.com/apache/spark/pull/34920.diff,https://github.com/apache/spark/pull/34920.patch,,https://api.github.com/repos/apache/spark/issues/34920/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
50,https://api.github.com/repos/apache/spark/issues/34916,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34916/labels{/name},https://api.github.com/repos/apache/spark/issues/34916/comments,https://api.github.com/repos/apache/spark/issues/34916/events,https://github.com/apache/spark/pull/34916,1081710675,PR_kwDOAQXtWs4v7IUM,34916,[SPARK-35355][SQL] Avoid shuffle in some scenes with CollectLimitExec,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,5,2021-12-16T02:46:57Z,2021-12-16T10:25:10Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?
Currently, Some scenes retain the combination of GlobalLimitExec - LocalLimitExec, which can actually be optimized to CollectLimitExec, like: `INSERT INTO t2 SELECT c1 FROM t1 LIMIT 1`

The PR aims to 2 points:

- Improve `INSERT INTO t2 SELECT c1 FROM t1 LIMIT 1` to `CollectLimitExec`
- Add a flag to identifies `CollectLimitExec` as subExec without shuffling and scan all data. 

Example:
> CREATE TABLE t1(id STRING) USING parquet
CREATE TABLE t2(id STRING) USING parquet
INSERT INTO t1 SELECT * FROM t2 LIMIT 5

Before:
![Before PR](https://user-images.githubusercontent.com/51110188/146298688-4c79ab16-0df6-4677-af52-b880c7480cf0.png)

After:
<img width=""669"" alt=""After PR"" src=""https://user-images.githubusercontent.com/51110188/146298706-78ef867d-8342-4340-8ad5-7a648c597e50.png"">


### Why are the changes needed?
Optimize logic


### Does this PR introduce _any_ user-facing change?
No, optimize only


### How was this patch tested?
Unit Test and manually test.
",https://api.github.com/repos/apache/spark/issues/34916/timeline,,spark,apache,yikf,51110188,MDQ6VXNlcjUxMTEwMTg4,https://avatars.githubusercontent.com/u/51110188?v=4,,https://api.github.com/users/yikf,https://github.com/yikf,https://api.github.com/users/yikf/followers,https://api.github.com/users/yikf/following{/other_user},https://api.github.com/users/yikf/gists{/gist_id},https://api.github.com/users/yikf/starred{/owner}{/repo},https://api.github.com/users/yikf/subscriptions,https://api.github.com/users/yikf/orgs,https://api.github.com/users/yikf/repos,https://api.github.com/users/yikf/events{/privacy},https://api.github.com/users/yikf/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34916,https://github.com/apache/spark/pull/34916,https://github.com/apache/spark/pull/34916.diff,https://github.com/apache/spark/pull/34916.patch,,https://api.github.com/repos/apache/spark/issues/34916/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
51,https://api.github.com/repos/apache/spark/issues/34914,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34914/labels{/name},https://api.github.com/repos/apache/spark/issues/34914/comments,https://api.github.com/repos/apache/spark/issues/34914/events,https://github.com/apache/spark/pull/34914,1081621023,PR_kwDOAQXtWs4v61wR,34914,[SPARK-37627][SQL][FOLLOWUP] Add tests for sorted BucketTransform,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,3,2021-12-15T23:52:52Z,2021-12-16T05:34:39Z,,CONTRIBUTOR,,True,"

### What changes were proposed in this pull request?
Add tests to make sure 
`bucket(numBuckets: Int, references: Array[NamedReference])` 
and 
`bucket(
      numBuckets: Int,
      references: Array[NamedReference],
      sortedCols: Array[NamedReference])` work


### Why are the changes needed?

test coverage

### Does this PR introduce _any_ user-facing change?
No


### How was this patch tested?
New tests
",https://api.github.com/repos/apache/spark/issues/34914/timeline,,spark,apache,huaxingao,13592258,MDQ6VXNlcjEzNTkyMjU4,https://avatars.githubusercontent.com/u/13592258?v=4,,https://api.github.com/users/huaxingao,https://github.com/huaxingao,https://api.github.com/users/huaxingao/followers,https://api.github.com/users/huaxingao/following{/other_user},https://api.github.com/users/huaxingao/gists{/gist_id},https://api.github.com/users/huaxingao/starred{/owner}{/repo},https://api.github.com/users/huaxingao/subscriptions,https://api.github.com/users/huaxingao/orgs,https://api.github.com/users/huaxingao/repos,https://api.github.com/users/huaxingao/events{/privacy},https://api.github.com/users/huaxingao/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34914,https://github.com/apache/spark/pull/34914,https://github.com/apache/spark/pull/34914.diff,https://github.com/apache/spark/pull/34914.patch,,https://api.github.com/repos/apache/spark/issues/34914/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
52,https://api.github.com/repos/apache/spark/issues/34908,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34908/labels{/name},https://api.github.com/repos/apache/spark/issues/34908/comments,https://api.github.com/repos/apache/spark/issues/34908/events,https://github.com/apache/spark/pull/34908,1080998439,PR_kwDOAQXtWs4v42a5,34908,[SPARK-37652][SQL]Support optimize skewed join through union,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,5,2021-12-15T12:45:31Z,2021-12-30T01:30:41Z,,NONE,,False,"### What changes were proposed in this pull request?

Each child of the union handles data skew separately.


### Why are the changes needed?
`OptimizeSkewedJoin` rule will take effect only when the plan has two ShuffleQueryStageExec.

With `Union`, it might break the assumption. For example, the following plans

<b>scenes 1</b>
```
Union
    SMJ
        ShuffleQueryStage
        ShuffleQueryStage
    SMJ
        ShuffleQueryStage
        ShuffleQueryStage
```

<b>scenes 2</b>
```
Union
    SMJ
        ShuffleQueryStage
        ShuffleQueryStage
    HashAggregate
```
when one or more of the SMJ data in the above plan is skewed, it cannot be processed at present.

It's better to support partial optimize with Union.

### Does this PR introduce any user-facing change?

Probably yes, the result partition might changed.

### How was this patch tested?

Add test",https://api.github.com/repos/apache/spark/issues/34908/timeline,,spark,apache,mcdull-zhang,63445864,MDQ6VXNlcjYzNDQ1ODY0,https://avatars.githubusercontent.com/u/63445864?v=4,,https://api.github.com/users/mcdull-zhang,https://github.com/mcdull-zhang,https://api.github.com/users/mcdull-zhang/followers,https://api.github.com/users/mcdull-zhang/following{/other_user},https://api.github.com/users/mcdull-zhang/gists{/gist_id},https://api.github.com/users/mcdull-zhang/starred{/owner}{/repo},https://api.github.com/users/mcdull-zhang/subscriptions,https://api.github.com/users/mcdull-zhang/orgs,https://api.github.com/users/mcdull-zhang/repos,https://api.github.com/users/mcdull-zhang/events{/privacy},https://api.github.com/users/mcdull-zhang/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34908,https://github.com/apache/spark/pull/34908,https://github.com/apache/spark/pull/34908.diff,https://github.com/apache/spark/pull/34908.patch,,https://api.github.com/repos/apache/spark/issues/34908/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
53,https://api.github.com/repos/apache/spark/issues/34903,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34903/labels{/name},https://api.github.com/repos/apache/spark/issues/34903/comments,https://api.github.com/repos/apache/spark/issues/34903/events,https://github.com/apache/spark/pull/34903,1080437729,PR_kwDOAQXtWs4v2_iw,34903,[SPARK-37650][PYTHON] Tell spark-env.sh the python interpreter,"[{'id': 1981471081, 'node_id': 'MDU6TGFiZWwxOTgxNDcxMDgx', 'url': 'https://api.github.com/repos/apache/spark/labels/DOCS', 'name': 'DOCS', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1982260031, 'node_id': 'MDU6TGFiZWwxOTgyMjYwMDMx', 'url': 'https://api.github.com/repos/apache/spark/labels/PYTHON', 'name': 'PYTHON', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,7,2021-12-15T00:43:33Z,2021-12-15T16:06:54Z,,NONE,,False,"When loading config defaults via spark-env.sh, it can be useful to know
the current pyspark python interpreter to allow the configuration to set
values properly. Pass this value in the environment as
_PYSPARK_DRIVER_SYS_EXECUTABLE to the environment script.

<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->
It's currently possible to set sensible site-wide spark configuration defaults by using `$SPARK_CONF_DIR/spark-env.sh`. In the case where a user is using pyspark, however, there are a number of things that aren't discoverable by that script, due to the way that it's called. There is a chain of calls (java_gateway.py -> shell script -> java -> shell script) that ends up obliterating any bit of the python context.

This change proposes to add en environment variable `_PYSPARK_DRIVER_SYS_EXECUTABLE` which points to the filename of the top-level python executable within pyspark's `java_gateway.py` bootstrapping process. With that, spark-env.sh will be able to infer enough information about the python environment to set the appropriate configuration variables.

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->
Right now, there a number of config options useful to pyspark that can't be reliably set by `spark-env.sh` because it is unaware of the python context that spawning the executor. To give the most trivial example, it is currently possible to set `spark.kubernetes.container.image` or `spark.driver.host` based on information readily available from the environment (e.g. the k8s downward API). However, `spark.pyspark.python` and family cannot be set because when `spark-env.sh` executes it's lost all of the python context. We can instruct users to add the appropriate config variables, but this form of cargo-culting is error-prone and not scalable. It would be much better to expose important python variables so that pyspark can not be a second-class citizen.


### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->
Yes. With this change, if python spawns the JVM, `spark-env.sh` will receive an environment variable `_PYSPARK_DRIVER_SYS_EXECUTABLE` pointing to the python executor.


### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.
-->
To be perfectly honest, I don't know where this fits into the testing infrastructure. I monkey-patched a binary 3.2.0 install to add the lines to java_gateway.py and that works, but in terms of adding this to the CI ... I'm at a loss. I'm more than willing to add the additional info, if needed.
",https://api.github.com/repos/apache/spark/issues/34903/timeline,,spark,apache,PerilousApricot,93354,MDQ6VXNlcjkzMzU0,https://avatars.githubusercontent.com/u/93354?v=4,,https://api.github.com/users/PerilousApricot,https://github.com/PerilousApricot,https://api.github.com/users/PerilousApricot/followers,https://api.github.com/users/PerilousApricot/following{/other_user},https://api.github.com/users/PerilousApricot/gists{/gist_id},https://api.github.com/users/PerilousApricot/starred{/owner}{/repo},https://api.github.com/users/PerilousApricot/subscriptions,https://api.github.com/users/PerilousApricot/orgs,https://api.github.com/users/PerilousApricot/repos,https://api.github.com/users/PerilousApricot/events{/privacy},https://api.github.com/users/PerilousApricot/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34903,https://github.com/apache/spark/pull/34903,https://github.com/apache/spark/pull/34903.diff,https://github.com/apache/spark/pull/34903.patch,,https://api.github.com/repos/apache/spark/issues/34903/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
54,https://api.github.com/repos/apache/spark/issues/34900,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34900/labels{/name},https://api.github.com/repos/apache/spark/issues/34900/comments,https://api.github.com/repos/apache/spark/issues/34900/events,https://github.com/apache/spark/pull/34900,1079934979,PR_kwDOAQXtWs4v1SPn,34900,"[SPARK-37643][SQL] when charVarcharAsString is true, char datatype partition table query incorrect","[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,2,2021-12-14T16:01:09Z,2021-12-27T13:28:47Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?
after add ApplyCharTypePadding rule, when partition data type is char, if partition value length is less then defined, partition expr filter will be right-padding, then will query incorrect result


### Why are the changes needed?
fix query incorrect issue when filter is partition type and partition type is char.

### Does this PR introduce _any_ user-facing change?
before this fix, if we using char partition type, then we should be careful to set charVarcharAsString to true.

### How was this patch tested?
add new UT.
",https://api.github.com/repos/apache/spark/issues/34900/timeline,,spark,apache,fhygh,25889738,MDQ6VXNlcjI1ODg5NzM4,https://avatars.githubusercontent.com/u/25889738?v=4,,https://api.github.com/users/fhygh,https://github.com/fhygh,https://api.github.com/users/fhygh/followers,https://api.github.com/users/fhygh/following{/other_user},https://api.github.com/users/fhygh/gists{/gist_id},https://api.github.com/users/fhygh/starred{/owner}{/repo},https://api.github.com/users/fhygh/subscriptions,https://api.github.com/users/fhygh/orgs,https://api.github.com/users/fhygh/repos,https://api.github.com/users/fhygh/events{/privacy},https://api.github.com/users/fhygh/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34900,https://github.com/apache/spark/pull/34900,https://github.com/apache/spark/pull/34900.diff,https://github.com/apache/spark/pull/34900.patch,,https://api.github.com/repos/apache/spark/issues/34900/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
55,https://api.github.com/repos/apache/spark/issues/34896,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34896/labels{/name},https://api.github.com/repos/apache/spark/issues/34896/comments,https://api.github.com/repos/apache/spark/issues/34896/events,https://github.com/apache/spark/pull/34896,1079736293,PR_kwDOAQXtWs4v0n1n,34896,[SPARK-37568][SQL] Support 2-arguments by the convert_timezone() function,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,6,2021-12-14T13:06:57Z,2021-12-27T10:13:11Z,,CONTRIBUTOR,,False,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->

Support 2-arguments  by the convert_timezone() function.
If users omit sourceTz, spark takes it from sourceTimestamp or SESSION_LOCAL_TIMEZONE.

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->
Users can call this API with fewer arguments. The same function in Snowflake supports 2 arguments.

### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->
Yes, users can call this function with 2 arguments.

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.
-->
unit tests",https://api.github.com/repos/apache/spark/issues/34896/timeline,,spark,apache,yoda-mon,14937752,MDQ6VXNlcjE0OTM3NzUy,https://avatars.githubusercontent.com/u/14937752?v=4,,https://api.github.com/users/yoda-mon,https://github.com/yoda-mon,https://api.github.com/users/yoda-mon/followers,https://api.github.com/users/yoda-mon/following{/other_user},https://api.github.com/users/yoda-mon/gists{/gist_id},https://api.github.com/users/yoda-mon/starred{/owner}{/repo},https://api.github.com/users/yoda-mon/subscriptions,https://api.github.com/users/yoda-mon/orgs,https://api.github.com/users/yoda-mon/repos,https://api.github.com/users/yoda-mon/events{/privacy},https://api.github.com/users/yoda-mon/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34896,https://github.com/apache/spark/pull/34896,https://github.com/apache/spark/pull/34896.diff,https://github.com/apache/spark/pull/34896.patch,,https://api.github.com/repos/apache/spark/issues/34896/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
56,https://api.github.com/repos/apache/spark/issues/34894,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34894/labels{/name},https://api.github.com/repos/apache/spark/issues/34894/comments,https://api.github.com/repos/apache/spark/issues/34894/events,https://github.com/apache/spark/pull/34894,1079474439,PR_kwDOAQXtWs4vzvtm,34894,[WIP][SPARK-37641][SQL] Support ANSI Aggregate Function: regr_r2,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,13,2021-12-14T08:47:33Z,2021-12-22T13:16:26Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?
This PR used to support ANSI aggregate Function: `regr_r2`

The mainstream database supports `regr_r2` show below:
**Teradata**
https://docs.teradata.com/r/756LNiPSFdY~4JcCCcR5Cw/exhFe2f_YyGqKFakYYUn2A
**Snowflake**
https://docs.snowflake.com/en/sql-reference/functions/regr_r2.html
**Oracle**
https://docs.oracle.com/en/database/oracle/oracle-database/19/sqlrf/REGR_-Linear-Regression-Functions.html#GUID-A675B68F-2A88-4843-BE2C-FCDE9C65F9A9
**H2**
http://www.h2database.com/html/functions-aggregate.html#regr_r2
**Postgresql**
https://www.postgresql.org/docs/8.4/functions-aggregate.html
**Sybase**
https://infocenter.sybase.com/help/index.jsp?topic=/com.sybase.help.sqlanywhere.12.0.0/dbreference/regr-r2-function.html
**Exasol**
https://docs.exasol.com/sql_references/functions/alphabeticallistfunctions/regr_function.htm

### Why are the changes needed?
`regr_r2` is very useful.


### Does this PR introduce _any_ user-facing change?
'Yes'. New feature.


### How was this patch tested?
New tests.
",https://api.github.com/repos/apache/spark/issues/34894/timeline,,spark,apache,beliefer,8486025,MDQ6VXNlcjg0ODYwMjU=,https://avatars.githubusercontent.com/u/8486025?v=4,,https://api.github.com/users/beliefer,https://github.com/beliefer,https://api.github.com/users/beliefer/followers,https://api.github.com/users/beliefer/following{/other_user},https://api.github.com/users/beliefer/gists{/gist_id},https://api.github.com/users/beliefer/starred{/owner}{/repo},https://api.github.com/users/beliefer/subscriptions,https://api.github.com/users/beliefer/orgs,https://api.github.com/users/beliefer/repos,https://api.github.com/users/beliefer/events{/privacy},https://api.github.com/users/beliefer/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34894,https://github.com/apache/spark/pull/34894,https://github.com/apache/spark/pull/34894.diff,https://github.com/apache/spark/pull/34894.patch,,https://api.github.com/repos/apache/spark/issues/34894/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
57,https://api.github.com/repos/apache/spark/issues/34887,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34887/labels{/name},https://api.github.com/repos/apache/spark/issues/34887/comments,https://api.github.com/repos/apache/spark/issues/34887/events,https://github.com/apache/spark/pull/34887,1079188672,PR_kwDOAQXtWs4vy0ex,34887,[WIP] Add IPython command cancel event handler for CrossValidator,"[{'id': 1405801475, 'node_id': 'MDU6TGFiZWwxNDA1ODAxNDc1', 'url': 'https://api.github.com/repos/apache/spark/labels/ML', 'name': 'ML', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1982260031, 'node_id': 'MDU6TGFiZWwxOTgyMjYwMDMx', 'url': 'https://api.github.com/repos/apache/spark/labels/PYTHON', 'name': 'PYTHON', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,6,2021-12-14T00:55:13Z,2021-12-21T17:46:05Z,,CONTRIBUTOR,,False,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
Add IPython command cancel event handler for CrossValidator

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->


### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->


### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.
-->
",https://api.github.com/repos/apache/spark/issues/34887/timeline,,spark,apache,WeichenXu123,19235986,MDQ6VXNlcjE5MjM1OTg2,https://avatars.githubusercontent.com/u/19235986?v=4,,https://api.github.com/users/WeichenXu123,https://github.com/WeichenXu123,https://api.github.com/users/WeichenXu123/followers,https://api.github.com/users/WeichenXu123/following{/other_user},https://api.github.com/users/WeichenXu123/gists{/gist_id},https://api.github.com/users/WeichenXu123/starred{/owner}{/repo},https://api.github.com/users/WeichenXu123/subscriptions,https://api.github.com/users/WeichenXu123/orgs,https://api.github.com/users/WeichenXu123/repos,https://api.github.com/users/WeichenXu123/events{/privacy},https://api.github.com/users/WeichenXu123/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34887,https://github.com/apache/spark/pull/34887,https://github.com/apache/spark/pull/34887.diff,https://github.com/apache/spark/pull/34887.patch,,https://api.github.com/repos/apache/spark/issues/34887/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
58,https://api.github.com/repos/apache/spark/issues/34882,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34882/labels{/name},https://api.github.com/repos/apache/spark/issues/34882/comments,https://api.github.com/repos/apache/spark/issues/34882/events,https://github.com/apache/spark/pull/34882,1078431255,PR_kwDOAQXtWs4vwRBB,34882,[WIP][SPARK-37623][SQL] Support ANSI Aggregate Function: regr_slope & regr_intercept,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,12,2021-12-13T11:47:08Z,2021-12-22T13:16:42Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?
`REGR_SLOPE` and `REGR_INTERCEPT` are ANSI aggregate functions

The mainstream database supports `regr_count` show below:
**Teradata**
https://docs.teradata.com/r/kmuOwjp1zEYg98JsB8fu_A/I0~kqsq3f3uNmjUaZr8hDg
**Snowflake**
https://docs.snowflake.com/en/sql-reference/functions/regr_slope.html
**Oracle**
https://docs.oracle.com/en/database/oracle/oracle-database/19/sqlrf/REGR_-Linear-Regression-Functions.html#GUID-A675B68F-2A88-4843-BE2C-FCDE9C65F9A9
**H2**
http://www.h2database.com/html/functions-aggregate.html#regr_slope
**Postgresql**
https://www.postgresql.org/docs/8.4/functions-aggregate.html
**Sybase**
https://infocenter.sybase.com/help/topic/com.sybase.help.sqlanywhere.12.0.0/dbreference/regr-slope-function.html
**Presto**
https://prestodb.io/docs/current/functions/aggregate.html
Exasol
https://docs.exasol.com/sql_references/functions/alphabeticallistfunctions/regr_function.htm

### Why are the changes needed?
`REGR_SLOPE` and `REGR_INTERCEPT` are very useful.


### Does this PR introduce _any_ user-facing change?
'Yes'. New feature.


### How was this patch tested?
New tests.
",https://api.github.com/repos/apache/spark/issues/34882/timeline,,spark,apache,beliefer,8486025,MDQ6VXNlcjg0ODYwMjU=,https://avatars.githubusercontent.com/u/8486025?v=4,,https://api.github.com/users/beliefer,https://github.com/beliefer,https://api.github.com/users/beliefer/followers,https://api.github.com/users/beliefer/following{/other_user},https://api.github.com/users/beliefer/gists{/gist_id},https://api.github.com/users/beliefer/starred{/owner}{/repo},https://api.github.com/users/beliefer/subscriptions,https://api.github.com/users/beliefer/orgs,https://api.github.com/users/beliefer/repos,https://api.github.com/users/beliefer/events{/privacy},https://api.github.com/users/beliefer/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34882,https://github.com/apache/spark/pull/34882,https://github.com/apache/spark/pull/34882.diff,https://github.com/apache/spark/pull/34882.patch,,https://api.github.com/repos/apache/spark/issues/34882/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
59,https://api.github.com/repos/apache/spark/issues/34872,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34872/labels{/name},https://api.github.com/repos/apache/spark/issues/34872/comments,https://api.github.com/repos/apache/spark/issues/34872/events,https://github.com/apache/spark/pull/34872,1077834416,PR_kwDOAQXtWs4vuVyT,34872,"[SPARK-37617][SQL][HIVE] In CTAS, Replace Parquet name columns that have not alias","[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,4,2021-12-12T15:27:52Z,2021-12-26T13:09:11Z,,CONTRIBUTOR,,False,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
In CTAS, Replace name columns that have not alias.Mostly, columns without alias
  always is operator such as sum, divide that will lead to schema check error. Also,  provide a config option, ""spark.sql.schema.replace.alias.asColumn"", default is false to choose if we should replace the alias  or not. 


### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->
if we do not replace the alias ,the error 'Column name ""$name"" contains invalid character(s).
         |Please use alias to rename it.' will appear

### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->
yes, 
spark sql(""CREATE TABLE `alias` (
  `a` INT,
  `b` STRING)
USING parquet"")
spark.sql("" create table  alias_b as select a, "" +
            ""max(unix_timestamp(b)) from alias group by a"")

Before this pr:
logical plan like this :
```
OptimizedCreateHiveTableAsSelectCommand [Database: default, TableName: alias_b, InsertIntoHadoopFsRelationCommand]
+- Aggregate [a#10], [a#10, max(unix_timestamp(b#11, yyyy-MM-dd HH:mm:ss, Some(America/Los_Angeles), false)) AS max(unix_timestamp(b, yyyy-MM-dd HH:mm:ss))#19L]
   +- SubqueryAlias spark_catalog.default.alias
      +- Relation default.alias[a#10,b#11] parquet

```
and error will throw:
```
Column name ""max(unix_timestamp(b, yyyy-MM-dd HH:mm:ss))"" contains invalid character(s). Please use alias to rename it.
```
After this pr and set spark.sql.schema.replace.alias.asColumn =true:
logical plan is :
```
== Analyzed Logical Plan ==
OptimizedCreateHiveTableAsSelectCommand [Database: default, TableName: alias_b, InsertIntoHadoopFsRelationCommand]
+- Aggregate [a#10], [a#10, max(unix_timestamp(b#11, yyyy-MM-dd HH:mm:ss, Some(America/Los_Angeles), false)) AS max_unix_timestamp_b__yyyy-MM-dd_HH:mm:ss__#14L]
   +- SubqueryAlias spark_catalog.default.alias
      +- Relation default.alias[a#10,b#11] parquet
```
and sql(""desc table alias_b"").show(false):
```
+-------------------------------------------+---------+-------+
|col_name                                   |data_type|comment|
+-------------------------------------------+---------+-------+
|a                                          |int      |null   |
|max_unix_timestamp_b__yyyy-MM-dd_HH:mm:ss__|bigint   |null   |
+-------------------------------------------+---------+-------+
```
### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.
-->
unit test
",https://api.github.com/repos/apache/spark/issues/34872/timeline,,spark,apache,monkeyboy123,9074114,MDQ6VXNlcjkwNzQxMTQ=,https://avatars.githubusercontent.com/u/9074114?v=4,,https://api.github.com/users/monkeyboy123,https://github.com/monkeyboy123,https://api.github.com/users/monkeyboy123/followers,https://api.github.com/users/monkeyboy123/following{/other_user},https://api.github.com/users/monkeyboy123/gists{/gist_id},https://api.github.com/users/monkeyboy123/starred{/owner}{/repo},https://api.github.com/users/monkeyboy123/subscriptions,https://api.github.com/users/monkeyboy123/orgs,https://api.github.com/users/monkeyboy123/repos,https://api.github.com/users/monkeyboy123/events{/privacy},https://api.github.com/users/monkeyboy123/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34872,https://github.com/apache/spark/pull/34872,https://github.com/apache/spark/pull/34872.diff,https://github.com/apache/spark/pull/34872.patch,,https://api.github.com/repos/apache/spark/issues/34872/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
60,https://api.github.com/repos/apache/spark/issues/34871,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34871/labels{/name},https://api.github.com/repos/apache/spark/issues/34871/comments,https://api.github.com/repos/apache/spark/issues/34871/events,https://github.com/apache/spark/pull/34871,1077786843,PR_kwDOAQXtWs4vuMoZ,34871,[SPARK-37616][SQL] Support pushing down a dynamic partition pruning from one join to other joins.,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,2,2021-12-12T11:53:57Z,2021-12-17T09:46:52Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?
 Introduce a rule to handle pushing down a dynamic partition pruning from a child join to its parent join, when the following conditions are met:
 (1) the pruning side of the parent join is a partition table
 (2) the table to prune is filterable by the JOIN key
 (3) the parent join operation is one of the following types: INNER, LEFT SEMI,
  LEFT OUTER (partitioned on right), or RIGHT OUTER (partitioned on left)
 
A query example: 
```sql
SELECT f.store_id, k.units_sold
FROM fact_stats f
JOIN dim_stats d
    ON f.store_id = d.store_id
        AND d.country = 'NL'
LEFT JOIN fact_sk k
    ON f.store_id = k.store_id
```

Before the PR:

![image](https://user-images.githubusercontent.com/39684231/145710866-70b44119-12ba-4afc-8a02-385189712950.png)

After the PR:
![image](https://user-images.githubusercontent.com/39684231/145710859-93ed90ae-1b5b-4516-a780-dfb1c14e5e0d.png)

### Why are the changes needed?
Push down a dynamic partition pruning from one join to other joins to prune the table of other joins  to improve performance.

### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->

No.

### How was this patch tested?
Added unittests.
",https://api.github.com/repos/apache/spark/issues/34871/timeline,,spark,apache,weixiuli,39684231,MDQ6VXNlcjM5Njg0MjMx,https://avatars.githubusercontent.com/u/39684231?v=4,,https://api.github.com/users/weixiuli,https://github.com/weixiuli,https://api.github.com/users/weixiuli/followers,https://api.github.com/users/weixiuli/following{/other_user},https://api.github.com/users/weixiuli/gists{/gist_id},https://api.github.com/users/weixiuli/starred{/owner}{/repo},https://api.github.com/users/weixiuli/subscriptions,https://api.github.com/users/weixiuli/orgs,https://api.github.com/users/weixiuli/repos,https://api.github.com/users/weixiuli/events{/privacy},https://api.github.com/users/weixiuli/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34871,https://github.com/apache/spark/pull/34871,https://github.com/apache/spark/pull/34871.diff,https://github.com/apache/spark/pull/34871.patch,,https://api.github.com/repos/apache/spark/issues/34871/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
61,https://api.github.com/repos/apache/spark/issues/34868,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34868/labels{/name},https://api.github.com/repos/apache/spark/issues/34868/comments,https://api.github.com/repos/apache/spark/issues/34868/events,https://github.com/apache/spark/pull/34868,1077529296,PR_kwDOAQXtWs4vthd4,34868,[SPARK-37614][SQL] Support ANSI Aggregate Function: regr_avgx & regr_avgy,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,25,2021-12-11T12:51:51Z,2021-12-28T06:24:32Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?
`REGR_AVGX` and `REGR_AVGY` are ANSI aggregate functions

The mainstream database supports `regr_count` show below:
**Teradata**
https://docs.teradata.com/r/kmuOwjp1zEYg98JsB8fu_A/KkJgUSq2O6JRU3bCK~0cug
**Snowflake**
https://docs.snowflake.com/en/sql-reference/functions/regr_avgx.html
**Oracle**
https://docs.oracle.com/en/database/oracle/oracle-database/19/sqlrf/REGR_-Linear-Regression-Functions.html#GUID-A675B68F-2A88-4843-BE2C-FCDE9C65F9A9
**H2**
http://www.h2database.com/html/functions-aggregate.html#regr_avgx
**Postgresql**
https://www.postgresql.org/docs/8.4/functions-aggregate.html
**Sybase**
https://infocenter.sybase.com/help/index.jsp?topic=/com.sybase.help.sqlanywhere.12.0.0/dbreference/regr-avgx-function.html
**Exasol**
https://docs.exasol.com/sql_references/functions/alphabeticallistfunctions/regr_function.htm

### Why are the changes needed?
`REGR_AVGX` and `REGR_AVGY` are very useful.


### Does this PR introduce _any_ user-facing change?
'Yes'. New feature.


### How was this patch tested?
New tests.
",https://api.github.com/repos/apache/spark/issues/34868/timeline,,spark,apache,beliefer,8486025,MDQ6VXNlcjg0ODYwMjU=,https://avatars.githubusercontent.com/u/8486025?v=4,,https://api.github.com/users/beliefer,https://github.com/beliefer,https://api.github.com/users/beliefer/followers,https://api.github.com/users/beliefer/following{/other_user},https://api.github.com/users/beliefer/gists{/gist_id},https://api.github.com/users/beliefer/starred{/owner}{/repo},https://api.github.com/users/beliefer/subscriptions,https://api.github.com/users/beliefer/orgs,https://api.github.com/users/beliefer/repos,https://api.github.com/users/beliefer/events{/privacy},https://api.github.com/users/beliefer/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34868,https://github.com/apache/spark/pull/34868,https://github.com/apache/spark/pull/34868.diff,https://github.com/apache/spark/pull/34868.patch,,https://api.github.com/repos/apache/spark/issues/34868/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
62,https://api.github.com/repos/apache/spark/issues/34864,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34864/labels{/name},https://api.github.com/repos/apache/spark/issues/34864/comments,https://api.github.com/repos/apache/spark/issues/34864/events,https://github.com/apache/spark/pull/34864,1077261110,PR_kwDOAQXtWs4vsylT,34864,[SHUFFLE] [WIP] Prototype: store shuffle file on external storage like S3,"[{'id': 1406627200, 'node_id': 'MDU6TGFiZWwxNDA2NjI3MjAw', 'url': 'https://api.github.com/repos/apache/spark/labels/BUILD', 'name': 'BUILD', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981471081, 'node_id': 'MDU6TGFiZWwxOTgxNDcxMDgx', 'url': 'https://api.github.com/repos/apache/spark/labels/DOCS', 'name': 'DOCS', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,17,2021-12-10T21:52:31Z,2021-12-20T19:39:03Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?
This PR ([design doc](https://docs.google.com/document/d/10rhvjXUlbQfWg-zh02_aqRqDT_ZnwYmICPAR--aRv64)) provides support to store shuffle files on external shuffle storage like S3. It helps Dynamic
Allocation on Kubernetes. Spark driver could release idle executors without worrying about losing
shuffle data because the shuffle data is store on external shuffle storage which are different
from executors.

This could be viewed as a followup work for https://issues.apache.org/jira/browse/SPARK-25299.

There is previously Worker Decommission feature ([SPARK-33545](https://issues.apache.org/jira/browse/SPARK-33545)), which is a great feature to copy shuffle data to fallback storage like S3. People appreciate that work to address the critical issue to handle shuffle data on Spark executor termination. The work in the PR does not intent to replace that feature. The intent is to get further discussion about how to save shuffle data on S3 during normal execution time.

### Why are the changes needed?

To better support Dynamic Allocation on Kubernetes, we need to decouple shuffle data from Spark
executor. This PR implements another Shuffle Manager and support writing shuffle data on S3.

### Does this PR introduce _any_ user-facing change?

Yes, this PR adds two Spark config like following to plug in another StarShuffleManager and store
shuffle data on provided S3 location.
```
spark.shuffle.manager=org.apache.spark.shuffle.StarShuffleManager
spark.shuffle.star.rootDir=s3://my_bucket_name/my_shuffle_folder
```

### How was this patch tested?

Added a unit test for StartShuffleManager. A lot of classes are copied from Spark, thus not add tests
for those classes. We will work with the community to get feedback first, then work on removing code
copy/duplication.",https://api.github.com/repos/apache/spark/issues/34864/timeline,,spark,apache,hiboyang,14280154,MDQ6VXNlcjE0MjgwMTU0,https://avatars.githubusercontent.com/u/14280154?v=4,,https://api.github.com/users/hiboyang,https://github.com/hiboyang,https://api.github.com/users/hiboyang/followers,https://api.github.com/users/hiboyang/following{/other_user},https://api.github.com/users/hiboyang/gists{/gist_id},https://api.github.com/users/hiboyang/starred{/owner}{/repo},https://api.github.com/users/hiboyang/subscriptions,https://api.github.com/users/hiboyang/orgs,https://api.github.com/users/hiboyang/repos,https://api.github.com/users/hiboyang/events{/privacy},https://api.github.com/users/hiboyang/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34864,https://github.com/apache/spark/pull/34864,https://github.com/apache/spark/pull/34864.diff,https://github.com/apache/spark/pull/34864.patch,,https://api.github.com/repos/apache/spark/issues/34864/reactions,8,8,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
63,https://api.github.com/repos/apache/spark/issues/34859,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34859/labels{/name},https://api.github.com/repos/apache/spark/issues/34859/comments,https://api.github.com/repos/apache/spark/issues/34859/events,https://github.com/apache/spark/pull/34859,1076428781,PR_kwDOAQXtWs4vqE4o,34859,[SPARK-37605][SQL] Support the configuration of the initial number of scan partitions when executing a take on a query,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,2,2021-12-10T04:57:39Z,2021-12-13T02:11:08Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?
Support the configuration of the initial number of scan partitions when executing a take on a query.
`spark.sql.limit.initialPartitionNum`


### Why are the changes needed?
Now the initial number of scanned partitions is 1 by default when executing a take on a query.
This number does not support configuration.
Sometimes the first task runs slower. If we have this configuration, we can increase the initial parallelism.



### Does this PR introduce _any_ user-facing change?
No


### How was this patch tested?
manual test
",https://api.github.com/repos/apache/spark/issues/34859/timeline,,spark,apache,cxzl25,3898450,MDQ6VXNlcjM4OTg0NTA=,https://avatars.githubusercontent.com/u/3898450?v=4,,https://api.github.com/users/cxzl25,https://github.com/cxzl25,https://api.github.com/users/cxzl25/followers,https://api.github.com/users/cxzl25/following{/other_user},https://api.github.com/users/cxzl25/gists{/gist_id},https://api.github.com/users/cxzl25/starred{/owner}{/repo},https://api.github.com/users/cxzl25/subscriptions,https://api.github.com/users/cxzl25/orgs,https://api.github.com/users/cxzl25/repos,https://api.github.com/users/cxzl25/events{/privacy},https://api.github.com/users/cxzl25/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34859,https://github.com/apache/spark/pull/34859,https://github.com/apache/spark/pull/34859.diff,https://github.com/apache/spark/pull/34859.patch,,https://api.github.com/repos/apache/spark/issues/34859/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
64,https://api.github.com/repos/apache/spark/issues/34856,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34856/labels{/name},https://api.github.com/repos/apache/spark/issues/34856/comments,https://api.github.com/repos/apache/spark/issues/34856/events,https://github.com/apache/spark/pull/34856,1076263237,PR_kwDOAQXtWs4vpjfJ,34856,[SPARK-37602][CORE] Add config property to set default Spark listeners,"[{'id': 1981471081, 'node_id': 'MDU6TGFiZWwxOTgxNDcxMDgx', 'url': 'https://api.github.com/repos/apache/spark/labels/DOCS', 'name': 'DOCS', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,5,2021-12-10T01:10:27Z,2021-12-15T19:39:58Z,,CONTRIBUTOR,,False,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
Adds a new config property `spark.defaultListeners` which is intended to be set by administrators to signify the ""default"" set of Spark listeners to be used in the job. This list will be combined with `spark.extraListeners` at runtime. This is similar in spirit to `spark.driver.defaultJavaOptions` and `spark.plugins.defaultList`.
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->


### Why are the changes needed?
`spark.extraListeners` allows users to custom Spark Listeners. Spark platform administrators would want to set their own set of ""default"" listeners for all jobs on the platform, however using `spark.extraListeners` makes it easy for end users to unknowingly override the default listeners.
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->


### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->
Yes, adds new a config property.


### How was this patch tested?
Modified existing UT to test new config property
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.
-->
",https://api.github.com/repos/apache/spark/issues/34856/timeline,,spark,apache,shardulm94,6961317,MDQ6VXNlcjY5NjEzMTc=,https://avatars.githubusercontent.com/u/6961317?v=4,,https://api.github.com/users/shardulm94,https://github.com/shardulm94,https://api.github.com/users/shardulm94/followers,https://api.github.com/users/shardulm94/following{/other_user},https://api.github.com/users/shardulm94/gists{/gist_id},https://api.github.com/users/shardulm94/starred{/owner}{/repo},https://api.github.com/users/shardulm94/subscriptions,https://api.github.com/users/shardulm94/orgs,https://api.github.com/users/shardulm94/repos,https://api.github.com/users/shardulm94/events{/privacy},https://api.github.com/users/shardulm94/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34856,https://github.com/apache/spark/pull/34856,https://github.com/apache/spark/pull/34856.diff,https://github.com/apache/spark/pull/34856.patch,,https://api.github.com/repos/apache/spark/issues/34856/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
65,https://api.github.com/repos/apache/spark/issues/34855,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34855/labels{/name},https://api.github.com/repos/apache/spark/issues/34855/comments,https://api.github.com/repos/apache/spark/issues/34855/events,https://github.com/apache/spark/pull/34855,1075929570,PR_kwDOAQXtWs4vodWE,34855,[WIP][SPARK-37600][BUILD] Upgrade to Hadoop 3.3.2,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1406627200, 'node_id': 'MDU6TGFiZWwxNDA2NjI3MjAw', 'url': 'https://api.github.com/repos/apache/spark/labels/BUILD', 'name': 'BUILD', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,6,2021-12-09T19:35:39Z,2021-12-11T18:00:01Z,,CONTRIBUTOR,,True,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->

Upgrade to Hadoop 3.3.2.

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->


### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->


### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.
-->
",https://api.github.com/repos/apache/spark/issues/34855/timeline,,spark,apache,sunchao,506679,MDQ6VXNlcjUwNjY3OQ==,https://avatars.githubusercontent.com/u/506679?v=4,,https://api.github.com/users/sunchao,https://github.com/sunchao,https://api.github.com/users/sunchao/followers,https://api.github.com/users/sunchao/following{/other_user},https://api.github.com/users/sunchao/gists{/gist_id},https://api.github.com/users/sunchao/starred{/owner}{/repo},https://api.github.com/users/sunchao/subscriptions,https://api.github.com/users/sunchao/orgs,https://api.github.com/users/sunchao/repos,https://api.github.com/users/sunchao/events{/privacy},https://api.github.com/users/sunchao/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34855,https://github.com/apache/spark/pull/34855,https://github.com/apache/spark/pull/34855.diff,https://github.com/apache/spark/pull/34855.patch,,https://api.github.com/repos/apache/spark/issues/34855/reactions,2,2,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
66,https://api.github.com/repos/apache/spark/issues/34849,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34849/labels{/name},https://api.github.com/repos/apache/spark/issues/34849/comments,https://api.github.com/repos/apache/spark/issues/34849/events,https://github.com/apache/spark/pull/34849,1075446422,PR_kwDOAQXtWs4vm2wE,34849,[SPARK-37596][SQL] Add the support for struct type column in the DropDuplicate,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,2,2021-12-09T11:08:53Z,2021-12-10T06:10:16Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?
Add the support for struct type column in the DropDuplicate in spark.Currently on using the struct col in the DropDuplicate we will get the below exception
```
case class StructDropDup(c1: Int, c2: Int)
val df = Seq((""d1"", StructDropDup(1, 2)),
      (""d1"", StructDropDup(1, 2))).toDF(""a"", ""b"")
df.dropDuplicates(""b.c1"")

org.apache.spark.sql.AnalysisException: Cannot resolve column name ""b.c1"" among (a, b)
  at org.apache.spark.sql.Dataset.$anonfun$dropDuplicates$1(Dataset.scala:2576)
  at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)
  at scala.collection.Iterator.foreach(Iterator.scala:941)
  at scala.collection.Iterator.foreach$(Iterator.scala:941)
  at scala.collection.AbstractIterator.foreach(Iterator.scala:1429) 
```

As workAround in order to find the the duplicate using the struct column

df1.withColumn(""b.c1"", col(""b.c1"")).dropDuplicates(""b.c1"").drop(""b.c1"").collect
```
df.withColumn(""b.c1"", col(""b.c1"")).dropDuplicates(""b.c1"").drop(""b.c1"").collect
res25: Array[org.apache.spark.sql.Row] = Array([d1,[1,2]])
```
Used the similar approach from the workaround to provide the support for the struct column whenever struct col is present in the dropDuplicate

After fix

```
case class StructDropDup(c1: Int, c2: Int)
val df = Seq((""d1"", StructDropDup(1, 2)),
      (""d1"", StructDropDup(1, 2))).toDF(""a"", ""b"")
df.dropDuplicates(""b.c1"").show

+---+------+
|  a|     b|
+---+------+
| d1|{1, 2}|
+---+------+

scala>  df.dropDuplicates(""b.c1"").queryExecution
== Parsed Logical Plan ==
Project [a#7, b#8]
+- Deduplicate [b.c1#63]
   +- Project [a#7, b#8, b#8.c1 AS c1#62 AS b.c1#63]
      +- Project [_1#2 AS a#7, _2#3 AS b#8]
         +- LocalRelation [_1#2, _2#3]

== Analyzed Logical Plan ==
a: string, b: struct<c1:int,c2:int>
Project [a#7, b#8]
+- Deduplicate [b.c1#63]
   +- Project [a#7, b#8, b#8.c1 AS b.c1#63]
      +- Project [_1#2 AS a#7, _2#3 AS b#8]
         +- LocalRelation [_1#2, _2#3]

== Optimized Logical Plan ==
Aggregate [b.c1#63], [first(a#7, false) AS a#68, first(b#8, false) AS b#70]
+- LocalRelation [a#7, b#8, b.c1#63]

== Physical Plan ==
AdaptiveSparkPlan isFinalPlan=false
+- SortAggregate(key=[b.c1#63], functions=[first(a#7, false), first(b#8, false)], output=[a#68, b#70])
   +- Sort [b.c1#63 ASC NULLS FIRST], false, 0
      +- Exchange hashpartitioning(b.c1#63, 200), ENSURE_REQUIREMENTS, [id=#74]
         +- SortAggregate(key=[b.c1#63], functions=[partial_first(a#7, false), partial_first(b#8, false)], output=[b.c1#63, first#75, valueSet#76, first#77, valueSet#78])
            +- Sort [b.c1#63 ASC NULLS FIRST], false, 0
               +- LocalTableScan [a#7, b#8, b.c1#63]

```

### Why are the changes needed?
There is need to provide the the way to use the dropDuplicate with struct Column. In existing code we are getting exception on using the struct Column in the dropDuplicates.

### Does this PR introduce _any_ user-facing change?
No


### How was this patch tested?
Added the Unit test and also tested using the spark-shell
",https://api.github.com/repos/apache/spark/issues/34849/timeline,,spark,apache,SaurabhChawla100,34540906,MDQ6VXNlcjM0NTQwOTA2,https://avatars.githubusercontent.com/u/34540906?v=4,,https://api.github.com/users/SaurabhChawla100,https://github.com/SaurabhChawla100,https://api.github.com/users/SaurabhChawla100/followers,https://api.github.com/users/SaurabhChawla100/following{/other_user},https://api.github.com/users/SaurabhChawla100/gists{/gist_id},https://api.github.com/users/SaurabhChawla100/starred{/owner}{/repo},https://api.github.com/users/SaurabhChawla100/subscriptions,https://api.github.com/users/SaurabhChawla100/orgs,https://api.github.com/users/SaurabhChawla100/repos,https://api.github.com/users/SaurabhChawla100/events{/privacy},https://api.github.com/users/SaurabhChawla100/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34849,https://github.com/apache/spark/pull/34849,https://github.com/apache/spark/pull/34849.diff,https://github.com/apache/spark/pull/34849.patch,,https://api.github.com/repos/apache/spark/issues/34849/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
67,https://api.github.com/repos/apache/spark/issues/34848,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34848/labels{/name},https://api.github.com/repos/apache/spark/issues/34848/comments,https://api.github.com/repos/apache/spark/issues/34848/events,https://github.com/apache/spark/pull/34848,1075428219,PR_kwDOAQXtWs4vmy2K,34848,"[SPARK-37582][SPARK-37583][SQL] CONTAINS, STARTSWITH, ENDSWITH should support binary type","[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,19,2021-12-09T10:49:56Z,2021-12-22T09:38:34Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?
In current realization, `contains`, `startsWith` and `endsWith` only support StringType as input, in this pr, we make these three string function support binary too.


### Why are the changes needed?
Make  function `contains`, `startsWith`, `endsWith` support BinaryType

### Does this PR introduce _any_ user-facing change?
user can use binary as input.


expression | result 
-- | -- 
contains(encode('Spark SQL', 'utf-8'), 'Spark') |  true
startsWith(encode('Spark SQL', 'utf-8'), 'Spark') |  true
contains(encode('Spark SQL', 'utf-8'), 'SparkSQL') |  false
endsWith(encode('Spark SQL', 'utf-8'), 'Spark') |  false
endsWith(encode('Spark SQL', 'utf-8'), 'SQL') |  true





### How was this patch tested?
added UT
",https://api.github.com/repos/apache/spark/issues/34848/timeline,,spark,apache,AngersZhuuuu,46485123,MDQ6VXNlcjQ2NDg1MTIz,https://avatars.githubusercontent.com/u/46485123?v=4,,https://api.github.com/users/AngersZhuuuu,https://github.com/AngersZhuuuu,https://api.github.com/users/AngersZhuuuu/followers,https://api.github.com/users/AngersZhuuuu/following{/other_user},https://api.github.com/users/AngersZhuuuu/gists{/gist_id},https://api.github.com/users/AngersZhuuuu/starred{/owner}{/repo},https://api.github.com/users/AngersZhuuuu/subscriptions,https://api.github.com/users/AngersZhuuuu/orgs,https://api.github.com/users/AngersZhuuuu/repos,https://api.github.com/users/AngersZhuuuu/events{/privacy},https://api.github.com/users/AngersZhuuuu/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34848,https://github.com/apache/spark/pull/34848,https://github.com/apache/spark/pull/34848.diff,https://github.com/apache/spark/pull/34848.patch,,https://api.github.com/repos/apache/spark/issues/34848/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
68,https://api.github.com/repos/apache/spark/issues/34846,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34846/labels{/name},https://api.github.com/repos/apache/spark/issues/34846/comments,https://api.github.com/repos/apache/spark/issues/34846/events,https://github.com/apache/spark/pull/34846,1075351096,PR_kwDOAQXtWs4vmisE,34846,[SPARK-37593][CORE] Optimize HeapMemoryAllocator to avoid memory waste when using G1GC,"[{'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,29,2021-12-09T09:31:07Z,2021-12-29T05:59:06Z,,CONTRIBUTOR,,False,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
Spark's tungsten memory model usually tries to allocate memory by one `page` each time and allocated by `long[pageSizeBytes/8]` in `HeapMemoryAllocator.allocate`. 

Remember that java long array needs extra object header (usually 16 bytes in 64bit system), so the really bytes allocated is `pageSize+16`.

Assume that the `G1HeapRegionSize` is 4M and `pageSizeBytes` is 4M as well. Since every time we need to allocate 4M+16byte memory, so two regions are used with one region only occupies 16byte. Then there are about **50%** memory waste.
It can happenes under different combinations of G1HeapRegionSize (varies from 1M to 32M) and pageSizeBytes (varies from 1M to 64M).

 We can demo it using following piece of code.

```
public static void bufferSizeTest(boolean optimize) {
    long totalAllocatedSize = 0L;
    int blockSize = 1024 * 1024 * 4; // 4m
    if (optimize) {
      blockSize -= 16;
    }
    List<long[]> buffers = new ArrayList<>();
    while (true) {
      long[] arr = new long[blockSize/8];
      buffers.add(arr);
      totalAllocatedSize += blockSize;
      System.out.println(""Total allocated size: "" + totalAllocatedSize);
    }
  }
```

Run it using following jvm params
```
java -Xmx100m -XX:+UseG1GC -XX:G1HeapRegionSize=4m -XX:-UseGCOverheadLimit -verbose:gc -XX:+UnlockDiagnosticVMOptions -XX:+G1SummarizeConcMark -Xss4m -XX:+ExitOnOutOfMemoryError -XX:ParallelGCThreads=4 -XX:ConcGCThreads=4
```

with optimized = false
```
Total allocated size: 46137344
[GC pause (G1 Humongous Allocation) (young) 44M->44M(100M), 0.0007091 secs]
[GC pause (G1 Evacuation Pause) (young) (initial-mark)-- 48M->48M(100M), 0.0021528 secs]
[GC concurrent-root-region-scan-start]
[GC concurrent-root-region-scan-end, 0.0000021 secs]
[GC concurrent-mark-start]
[GC pause (G1 Evacuation Pause) (young) 48M->48M(100M), 0.0011289 secs]
[Full GC (Allocation Failure)  48M->48M(100M), 0.0017284 secs]
[Full GC (Allocation Failure)  48M->48M(100M), 0.0013437 secs]
Terminating due to java.lang.OutOfMemoryError: Java heap space
```

with optimzied = true
```
Total allocated size: 96468624
[GC pause (G1 Humongous Allocation) (young)-- 92M->92M(100M), 0.0024416 secs]
[Full GC (Allocation Failure)  92M->92M(100M), 0.0019883 secs]
[GC pause (G1 Evacuation Pause) (young) (initial-mark) 96M->96M(100M), 0.0004282 secs]
[GC concurrent-root-region-scan-start]
[GC concurrent-root-region-scan-end, 0.0000040 secs]
[GC concurrent-mark-start]
[GC pause (G1 Evacuation Pause) (young) 96M->96M(100M), 0.0003269 secs]
[Full GC (Allocation Failure)  96M->96M(100M), 0.0012409 secs]
[Full GC (Allocation Failure)  96M->96M(100M), 0.0012607 secs]
Terminating due to java.lang.OutOfMemoryError: Java heap space
```

This PR try to optimize the pageSize to avoid memory waste.

This case exists not only in `MemoryManagement`, but also in other places such as `TorrentBroadcast.blockSize`.  I would like to submit a followup PR if this modification is reasonable.



### Why are the changes needed?
To avoid memory waste in G1 GC


### Does this PR introduce _any_ user-facing change?
No


### How was this patch tested?
Existing UT",https://api.github.com/repos/apache/spark/issues/34846/timeline,,spark,apache,WangGuangxin,1312321,MDQ6VXNlcjEzMTIzMjE=,https://avatars.githubusercontent.com/u/1312321?v=4,,https://api.github.com/users/WangGuangxin,https://github.com/WangGuangxin,https://api.github.com/users/WangGuangxin/followers,https://api.github.com/users/WangGuangxin/following{/other_user},https://api.github.com/users/WangGuangxin/gists{/gist_id},https://api.github.com/users/WangGuangxin/starred{/owner}{/repo},https://api.github.com/users/WangGuangxin/subscriptions,https://api.github.com/users/WangGuangxin/orgs,https://api.github.com/users/WangGuangxin/repos,https://api.github.com/users/WangGuangxin/events{/privacy},https://api.github.com/users/WangGuangxin/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34846,https://github.com/apache/spark/pull/34846,https://github.com/apache/spark/pull/34846.diff,https://github.com/apache/spark/pull/34846.patch,,https://api.github.com/repos/apache/spark/issues/34846/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
69,https://api.github.com/repos/apache/spark/issues/34834,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34834/labels{/name},https://api.github.com/repos/apache/spark/issues/34834/comments,https://api.github.com/repos/apache/spark/issues/34834/events,https://github.com/apache/spark/pull/34834,1074220819,PR_kwDOAQXtWs4vi3FZ,34834,[SPARK-37580][CORE] Reset numFailures when one of task attempts succeeds,"[{'id': 1981471081, 'node_id': 'MDU6TGFiZWwxOTgxNDcxMDgx', 'url': 'https://api.github.com/repos/apache/spark/labels/DOCS', 'name': 'DOCS', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,11,2021-12-08T09:59:02Z,2021-12-29T07:11:42Z,,NONE,,False,"### What changes were proposed in this pull request?
When a task failed count reach the max threshold, abort check if another attempt succeed.


### Why are the changes needed?
In extreme situation, if  one task has failed 3 times(max failed threshold is 4 in default), and there is a retry task and speculative task both in running state, then one of these 2 task attempts succeed and to cancel another. But executor which task need to be cancelled lost(oom in our situcation), this task marked as failed, and TaskSetManager handle this failed task attempt, it has failed 4 times so abort this stage and cause job failed.


### Does this PR introduce _any_ user-facing change?
No


### How was this patch tested?
Unit test.
",https://api.github.com/repos/apache/spark/issues/34834/timeline,,spark,apache,wangshengjie123,45991044,MDQ6VXNlcjQ1OTkxMDQ0,https://avatars.githubusercontent.com/u/45991044?v=4,,https://api.github.com/users/wangshengjie123,https://github.com/wangshengjie123,https://api.github.com/users/wangshengjie123/followers,https://api.github.com/users/wangshengjie123/following{/other_user},https://api.github.com/users/wangshengjie123/gists{/gist_id},https://api.github.com/users/wangshengjie123/starred{/owner}{/repo},https://api.github.com/users/wangshengjie123/subscriptions,https://api.github.com/users/wangshengjie123/orgs,https://api.github.com/users/wangshengjie123/repos,https://api.github.com/users/wangshengjie123/events{/privacy},https://api.github.com/users/wangshengjie123/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34834,https://github.com/apache/spark/pull/34834,https://github.com/apache/spark/pull/34834.diff,https://github.com/apache/spark/pull/34834.patch,,https://api.github.com/repos/apache/spark/issues/34834/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
70,https://api.github.com/repos/apache/spark/issues/34831,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34831/labels{/name},https://api.github.com/repos/apache/spark/issues/34831/comments,https://api.github.com/repos/apache/spark/issues/34831/events,https://github.com/apache/spark/pull/34831,1074005286,PR_kwDOAQXtWs4viKCt,34831,[SPARK-37574][CORE][SHUFFLE] Simplify fetchBlocks w/o retry,"[{'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,1,2021-12-08T04:42:19Z,2021-12-20T13:37:46Z,,MEMBER,,False,"### What changes were proposed in this pull request?

Simplify code of fetchBlocks w/o retry

### Why are the changes needed?

Simplify code.

The original code added in SPARK-4188, the `RetryingBlockTransferor` looks quite stable now.

#33340 renames `RetryingBlockFetcher` to `RetryingBlockTransferor`, the comment still calls it as `Fetcher`, it's a little misleading.

```
if (maxRetries > 0) {
  // Note this Fetcher will correctly handle maxRetries == 0; we avoid it just in case there's
  // a bug in this code. We should remove the if statement once we're sure of the stability.
  new RetryingBlockTransferor(transportConf, blockFetchStarter, blockIds, listener).start()
} else {
  blockFetchStarter.createAndStart(blockIds, listener)
}
```

### Does this PR introduce _any_ user-facing change?
No


### How was this patch tested?

Update RetryingBlockTransferorSuite
",https://api.github.com/repos/apache/spark/issues/34831/timeline,,spark,apache,pan3793,26535726,MDQ6VXNlcjI2NTM1NzI2,https://avatars.githubusercontent.com/u/26535726?v=4,,https://api.github.com/users/pan3793,https://github.com/pan3793,https://api.github.com/users/pan3793/followers,https://api.github.com/users/pan3793/following{/other_user},https://api.github.com/users/pan3793/gists{/gist_id},https://api.github.com/users/pan3793/starred{/owner}{/repo},https://api.github.com/users/pan3793/subscriptions,https://api.github.com/users/pan3793/orgs,https://api.github.com/users/pan3793/repos,https://api.github.com/users/pan3793/events{/privacy},https://api.github.com/users/pan3793/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34831,https://github.com/apache/spark/pull/34831,https://github.com/apache/spark/pull/34831.diff,https://github.com/apache/spark/pull/34831.patch,,https://api.github.com/repos/apache/spark/issues/34831/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
71,https://api.github.com/repos/apache/spark/issues/34829,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34829/labels{/name},https://api.github.com/repos/apache/spark/issues/34829/comments,https://api.github.com/repos/apache/spark/issues/34829/events,https://github.com/apache/spark/pull/34829,1073662768,PR_kwDOAQXtWs4vhCjV,34829,[WIP][SPARK-23607][CORE] Use HDFS extended attributes to store application summary information in SHS,"[{'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,2,2021-12-07T18:52:05Z,2021-12-08T04:57:07Z,,CONTRIBUTOR,,False," ### What changes were proposed in this pull request?

 This PR seeks to improve the performance of serving the application list in History Server by storing the required information of the application as part of HDFS extended attributes instead of parsing the log file each time.

 ### Why are the changes needed?

 Improves the performance of the History Server listing page

 ### Does this PR introduce _any_ user-facing change?

 No.

 ### How was this patch tested?
 Will add unit tests",https://api.github.com/repos/apache/spark/issues/34829/timeline,,spark,apache,thejdeep,1708757,MDQ6VXNlcjE3MDg3NTc=,https://avatars.githubusercontent.com/u/1708757?v=4,,https://api.github.com/users/thejdeep,https://github.com/thejdeep,https://api.github.com/users/thejdeep/followers,https://api.github.com/users/thejdeep/following{/other_user},https://api.github.com/users/thejdeep/gists{/gist_id},https://api.github.com/users/thejdeep/starred{/owner}{/repo},https://api.github.com/users/thejdeep/subscriptions,https://api.github.com/users/thejdeep/orgs,https://api.github.com/users/thejdeep/repos,https://api.github.com/users/thejdeep/events{/privacy},https://api.github.com/users/thejdeep/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34829,https://github.com/apache/spark/pull/34829,https://github.com/apache/spark/pull/34829.diff,https://github.com/apache/spark/pull/34829.patch,,https://api.github.com/repos/apache/spark/issues/34829/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
72,https://api.github.com/repos/apache/spark/issues/34820,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34820/labels{/name},https://api.github.com/repos/apache/spark/issues/34820/comments,https://api.github.com/repos/apache/spark/issues/34820/events,https://github.com/apache/spark/pull/34820,1072071004,PR_kwDOAQXtWs4vbyRO,34820,[SPARK-37559][SQL] ShuffledRowRDD get preferred locations order by reduce size,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,8,2021-12-06T12:03:34Z,2021-12-07T10:01:40Z,,CONTRIBUTOR,,False,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->
Pass `MapOutputStatistics` to `ShuffledRowRDD` in AQE code path so we can do sort according to the origin reduce partition size.

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->
The coalesced partition can contain several reduce partitions. The preferred locations of the RDD partition should be the biggest reduce partition before coalesced. 

So it can get a better data locality and reduce the network traffic.

### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->
no

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.
-->
Added test",https://api.github.com/repos/apache/spark/issues/34820/timeline,,spark,apache,ulysses-you,12025282,MDQ6VXNlcjEyMDI1Mjgy,https://avatars.githubusercontent.com/u/12025282?v=4,,https://api.github.com/users/ulysses-you,https://github.com/ulysses-you,https://api.github.com/users/ulysses-you/followers,https://api.github.com/users/ulysses-you/following{/other_user},https://api.github.com/users/ulysses-you/gists{/gist_id},https://api.github.com/users/ulysses-you/starred{/owner}{/repo},https://api.github.com/users/ulysses-you/subscriptions,https://api.github.com/users/ulysses-you/orgs,https://api.github.com/users/ulysses-you/repos,https://api.github.com/users/ulysses-you/events{/privacy},https://api.github.com/users/ulysses-you/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34820,https://github.com/apache/spark/pull/34820,https://github.com/apache/spark/pull/34820.diff,https://github.com/apache/spark/pull/34820.patch,,https://api.github.com/repos/apache/spark/issues/34820/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
73,https://api.github.com/repos/apache/spark/issues/34812,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34812/labels{/name},https://api.github.com/repos/apache/spark/issues/34812/comments,https://api.github.com/repos/apache/spark/issues/34812/events,https://github.com/apache/spark/pull/34812,1071588253,PR_kwDOAQXtWs4vaMnC,34812,[SPARK-37553][PYTHON] Fix underscore (`_`) bug in pyspark.pandas.frames.DataFrame.pivot_table,"[{'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1982260031, 'node_id': 'MDU6TGFiZWwxOTgyMjYwMDMx', 'url': 'https://api.github.com/repos/apache/spark/labels/PYTHON', 'name': 'PYTHON', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,9,2021-12-05T23:29:55Z,2021-12-21T16:41:07Z,,NONE,,False,"### What changes were proposed in this pull request?
- Adds example code changes to allow for underscores in (1) the elements for the `columns` arg and (2) for the column names used for the `values` arg when `len(values) > 1`. 


### Why are the changes needed?
Fixes a bug with the method `pyspark.pandas.frames.DataFrame.pivot_table` that causes a `KeyError` when an underscore is present (more details in [SPARK-37553](https://issues.apache.org/jira/browse/SPARK-37553)).
```python
>>> import numpy as np
>>> import pandas as pd
>>> from pyspark import pandas as ps
>>> pdf = pd.DataFrame(
        {
            ""a"": [4, 2, 3, 4, 8, 6],
            ""b_b"": [1, 2, 2, 4, 2, 4],
            ""e"": [10, 20, 20, 40, 20, 40],
            ""c"": [1, 2, 9, 4, 7, 4],
            ""d"": [-1, -2, -3, -4, -5, -6],
        },
        index=np.random.rand(6),
    )
>>> psdf = ps.from_pandas(pdf)
>>> psdf.pivot_table(index=[""c""], columns=""a"", values=[""b_b"", ""e""])

---------------------------------------------------------------------------
KeyError                                  Traceback (most recent call last)
<ipython-input-8-32d5bb0e1166> in <module>
----> 1 psdf.pivot_table(index=[""c""], columns=""a"", values=[""b_b"", ""e""])

~/.pyenv/versions/3.7.9/envs/venv37/lib/python3.7/site-packages/pyspark/pandas/frame.py in pivot_table(self, values, index, columns, aggfunc, fill_value)
   6053                     column_labels = [
   6054                         tuple(list(column_name_to_index[name.split(""_"")[1]]) + [name.split(""_"")[0]])
-> 6055                         for name in data_columns
   6056                     ]
   6057                     column_label_names = (

~/.pyenv/versions/3.7.9/envs/venv37/lib/python3.7/site-packages/pyspark/pandas/frame.py in <listcomp>(.0)
   6053                     column_labels = [
   6054                         tuple(list(column_name_to_index[name.split(""_"")[1]]) + [name.split(""_"")[0]])
-> 6055                         for name in data_columns
   6056                     ]
   6057                     column_label_names = (

KeyError: 'b'
```


### Does this PR introduce _any_ user-facing change?
No

### How was this patch tested?
- [x] Add unit tests for code changes
- [] Build package via Github Actions 
",https://api.github.com/repos/apache/spark/issues/34812/timeline,,spark,apache,oeuf,3674511,MDQ6VXNlcjM2NzQ1MTE=,https://avatars.githubusercontent.com/u/3674511?v=4,,https://api.github.com/users/oeuf,https://github.com/oeuf,https://api.github.com/users/oeuf/followers,https://api.github.com/users/oeuf/following{/other_user},https://api.github.com/users/oeuf/gists{/gist_id},https://api.github.com/users/oeuf/starred{/owner}{/repo},https://api.github.com/users/oeuf/subscriptions,https://api.github.com/users/oeuf/orgs,https://api.github.com/users/oeuf/repos,https://api.github.com/users/oeuf/events{/privacy},https://api.github.com/users/oeuf/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34812,https://github.com/apache/spark/pull/34812,https://github.com/apache/spark/pull/34812.diff,https://github.com/apache/spark/pull/34812.patch,,https://api.github.com/repos/apache/spark/issues/34812/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
74,https://api.github.com/repos/apache/spark/issues/34810,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34810/labels{/name},https://api.github.com/repos/apache/spark/issues/34810/comments,https://api.github.com/repos/apache/spark/issues/34810/events,https://github.com/apache/spark/pull/34810,1071454844,PR_kwDOAQXtWs4vZzII,34810,[SPARK-37549][SQL] Support set parallel through data source properties,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1496047285, 'node_id': 'MDU6TGFiZWwxNDk2MDQ3Mjg1', 'url': 'https://api.github.com/repos/apache/spark/labels/EXAMPLES', 'name': 'EXAMPLES', 'color': 'ededed', 'default': False, 'description': ''}, {'id': 1981471081, 'node_id': 'MDU6TGFiZWwxOTgxNDcxMDgx', 'url': 'https://api.github.com/repos/apache/spark/labels/DOCS', 'name': 'DOCS', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1982260031, 'node_id': 'MDU6TGFiZWwxOTgyMjYwMDMx', 'url': 'https://api.github.com/repos/apache/spark/labels/PYTHON', 'name': 'PYTHON', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1984361638, 'node_id': 'MDU6TGFiZWwxOTg0MzYxNjM4', 'url': 'https://api.github.com/repos/apache/spark/labels/R', 'name': 'R', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,9,2021-12-05T13:05:58Z,2021-12-09T14:51:05Z,,MEMBER,,False,"### What changes were proposed in this pull request?

This pr add support set parallel through data source properties when reading data. For example:
```scala
spark.read.option(""Parallel"", 1000).parquet(""path/to/parquet"")
```
```sql
CREATE TABLE very_large_partitioned_bucketed_table (
  id STRING,
  foo STRING,
  bar STRING,
  other STRING,
  dt STRING,
  type STRING)
USING parquet
OPTIONS (
  compression 'gzip',
  PARALLEL '12000'
)
PARTITIONED BY (dt, type)
CLUSTERED BY (id)
INTO 6000 BUCKETS
```

Oracle has similar feature:
https://docs.oracle.com/cd/B19306_01/server.102/b14200/clauses006.htm
https://docs.oracle.com/cd/E11882_01/server.112/e25523/parallel002.htm#BEIDFDEH

### Why are the changes needed?

1. To decrease the degree of parallelism if it is very large partitioned and bucketed table as it is not always use bucket scan since [SPARK-32859](https://issues.apache.org/jira/browse/SPARK-32859).
2. To increase the degree of parallelism on the stream side if it is `BroadcastNestedLoopJoinExec`.
3. To support setting parallel through hint in the future(Oracle has similar feature: https://docs.oracle.com/cd/E11882_01/server.112/e41573/hintsref.htm#CHDJIGDG).

### Does this PR introduce _any_ user-facing change?

No.


### How was this patch tested?

Unit test.
",https://api.github.com/repos/apache/spark/issues/34810/timeline,,spark,apache,wangyum,5399861,MDQ6VXNlcjUzOTk4NjE=,https://avatars.githubusercontent.com/u/5399861?v=4,,https://api.github.com/users/wangyum,https://github.com/wangyum,https://api.github.com/users/wangyum/followers,https://api.github.com/users/wangyum/following{/other_user},https://api.github.com/users/wangyum/gists{/gist_id},https://api.github.com/users/wangyum/starred{/owner}{/repo},https://api.github.com/users/wangyum/subscriptions,https://api.github.com/users/wangyum/orgs,https://api.github.com/users/wangyum/repos,https://api.github.com/users/wangyum/events{/privacy},https://api.github.com/users/wangyum/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34810,https://github.com/apache/spark/pull/34810,https://github.com/apache/spark/pull/34810.diff,https://github.com/apache/spark/pull/34810.patch,,https://api.github.com/repos/apache/spark/issues/34810/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
75,https://api.github.com/repos/apache/spark/issues/34805,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34805/labels{/name},https://api.github.com/repos/apache/spark/issues/34805/comments,https://api.github.com/repos/apache/spark/issues/34805/events,https://github.com/apache/spark/pull/34805,1071142232,PR_kwDOAQXtWs4vY3-A,34805,[SPARK-37542][SQL] Optimize the dynamic partition pruning rule to avoid inserting unnecessary predicate to improve performance,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,10,2021-12-04T06:49:45Z,2021-12-07T03:45:45Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?
Optimize the dynamic partitioning pruning rule to avoid inserting unnecessary predicates to improve performance.

### Why are the changes needed?
Currently, the dynamic partition pruning rule will insert a predicate on the filterable table using the filter from the other side of the join , and the predicate will be re-optimized by the AQE or non-AQE.

But, the predicate may be unnecessary if the join can NOT reuse broadcastExchange or it is not benefit, and it will be removed by the rules of the AQE or non-AQE, so we should  optimize the dynamic partition pruning rule to avoid inserting unnecessary predicate to improve performance.

### Does this PR introduce _any_ user-facing change?
NO

### How was this patch tested?

A query example:

spark.sql.autoBroadcastJoinThreshold=10000
```sql
spark.range(1000).select(
  $""id"",
  ($""id"" + 1).cast(""string"").as(""one""),
  ($""id"" + 2).cast(""string"").as(""two""),
  ($""id"" + 3).cast(""string"").as(""three""),
  (($""id"" * 20) % 100).as(""mod""),
  ($""id"" % 10).cast(""string"").as(""str""))
  .write.partitionBy(""one"", ""two"", ""three"")
  .format(""parquet"").mode(""overwrite"").saveAsTable(""fact1"")

spark.range(1000).select(
  $""id"",
  ($""id"" + 1).cast(""string"").as(""one""),
  ($""id"" + 2).cast(""string"").as(""two""),
  ($""id"" + 3).cast(""string"").as(""three""),
  (($""id"" * 20) % 100).as(""mod""),
  ($""id"" % 10).cast(""string"").as(""str""))
  .write.partitionBy(""one"", ""two"", ""three"")
  .format(""parquet"").mode(""overwrite"").saveAsTable(""fact2"")

spark.range(10).select(
  $""id"",
  ($""id"" + 1).cast(""string"").as(""one""),
  ($""id"" + 2).cast(""string"").as(""two""),
  ($""id"" + 3).cast(""string"").as(""three""),
  ($""id"" * 10).as(""prod""))
  .write.partitionBy(""one"", ""two"", ""three"", ""prod"")
  .format(""parquet"").mode(""overwrite"").saveAsTable(""dim"")

val df = sql(
  """"""
  |select f1.id, f1.one,f1.two,f1.three
  |FROM fact1 f1
  | JOIN dim d
  | ON f1.one = d.one and d.prod > 80
  |left join fact2 f2
  |ON f1.one = f2.one
"""""".stripMargin)
df.collect()
```

before this pr:

```log
== Physical Plan ==
AdaptiveSparkPlan (36)
+- == Final Plan ==
   * Project (22)
   +- * SortMergeJoin LeftOuter (21)
      :- * Sort (13)
      :  +- AQEShuffleRead (12)
      :     +- ShuffleQueryStage (11)
      :        +- Exchange (10)
      :           +- * Project (9)
      :              +- * BroadcastHashJoin Inner BuildRight (8)
      :                 :- * ColumnarToRow (2)
      :                 :  +- Scan parquet default.fact1 (1)
      :                 +- BroadcastQueryStage (7)
      :                    +- BroadcastExchange (6)
      :                       +- * Project (5)
      :                          +- * ColumnarToRow (4)
      :                             +- Scan parquet default.dim (3)
      +- * Sort (20)
         +- AQEShuffleRead (19)
            +- ShuffleQueryStage (18)
               +- Exchange (17)
                  +- * Project (16)
                     +- * ColumnarToRow (15)
                        +- Scan parquet default.fact2 (14)
+- == Initial Plan ==
   Project (35)
   +- SortMergeJoin LeftOuter (34)
      :- Sort (29)
      :  +- Exchange (28)
      :     +- Project (27)
      :        +- BroadcastHashJoin Inner BuildRight (26)
      :           :- Scan parquet default.fact1 (23)
      :           +- BroadcastExchange (25)
      :              +- Project (24)
      :                 +- Scan parquet default.dim (3)
      +- Sort (33)
         +- Exchange (32)
            +- Project (31)
               +- Scan parquet default.fact2 (30)


(1) Scan parquet default.fact1
Output [4]: [id#5365L, one#5368, two#5369, three#5370]
Batched: true
Location: InMemoryFileIndex [file:/Users/weixiuli/git/master/sql/catalyst/spark-warehouse/org.apache.spark.sql.DynamicPartitionPruningV1SuiteAEOn/fact1/one=533/two=534/three=535, ... 999 entries]
PartitionFilters: [isnotnull(one#5368), dynamicpruningexpression(one#5368 IN dynamicpruning#5386)]
ReadSchema: struct<id:bigint>

(2) ColumnarToRow [codegen id : 3]
Input [4]: [id#5365L, one#5368, two#5369, three#5370]

(3) Scan parquet default.dim
Output [4]: [one#5372, two#5373, three#5374, prod#5375L]
Batched: true
Location: InMemoryFileIndex [file:/Users/weixiuli/git/master/sql/catalyst/spark-warehouse/org.apache.spark.sql.DynamicPartitionPruningV1SuiteAEOn/dim/one=10/two=11/three=12/prod=90]
PartitionFilters: [isnotnull(prod#5375L), (prod#5375L > 80), isnotnull(one#5372)]
ReadSchema: struct<>

(4) ColumnarToRow [codegen id : 1]
Input [4]: [one#5372, two#5373, three#5374, prod#5375L]

(5) Project [codegen id : 1]
Output [1]: [one#5372]
Input [4]: [one#5372, two#5373, three#5374, prod#5375L]

(6) BroadcastExchange
Input [1]: [one#5372]
Arguments: HashedRelationBroadcastMode(List(input[0, string, true]),false), [id=#414]

(7) BroadcastQueryStage
Output [1]: [one#5372]
Arguments: 0

(8) BroadcastHashJoin [codegen id : 3]
Left keys [1]: [one#5368]
Right keys [1]: [one#5372]
Join condition: None

(9) Project [codegen id : 3]
Output [4]: [id#5365L, one#5368, two#5369, three#5370]
Input [5]: [id#5365L, one#5368, two#5369, three#5370, one#5372]

(10) Exchange
Input [4]: [id#5365L, one#5368, two#5369, three#5370]
Arguments: hashpartitioning(one#5368, 5), ENSURE_REQUIREMENTS, [id=#528]

(11) ShuffleQueryStage
Output [4]: [id#5365L, one#5368, two#5369, three#5370]
Arguments: 2

(12) AQEShuffleRead
Input [4]: [id#5365L, one#5368, two#5369, three#5370]
Arguments: coalesced

(13) Sort [codegen id : 4]
Input [4]: [id#5365L, one#5368, two#5369, three#5370]
Arguments: [one#5368 ASC NULLS FIRST], false, 0

(14) Scan parquet default.fact2
Output [3]: [one#5379, two#5380, three#5381]
Batched: true
Location: InMemoryFileIndex [file:/Users/weixiuli/git/master/sql/catalyst/spark-warehouse/org.apache.spark.sql.DynamicPartitionPruningV1SuiteAEOn/fact2/one=533/two=534/three=535, ... 999 entries]
PartitionFilters: [isnotnull(one#5379), dynamicpruningexpression(true)]
ReadSchema: struct<>

(15) ColumnarToRow [codegen id : 2]
Input [3]: [one#5379, two#5380, three#5381]

(16) Project [codegen id : 2]
Output [1]: [one#5379]
Input [3]: [one#5379, two#5380, three#5381]

(17) Exchange
Input [1]: [one#5379]
Arguments: hashpartitioning(one#5379, 5), ENSURE_REQUIREMENTS, [id=#461]

(18) ShuffleQueryStage
Output [1]: [one#5379]
Arguments: 1

(19) AQEShuffleRead
Input [1]: [one#5379]
Arguments: coalesced

(20) Sort [codegen id : 5]
Input [1]: [one#5379]
Arguments: [one#5379 ASC NULLS FIRST], false, 0

(21) SortMergeJoin [codegen id : 6]
Left keys [1]: [one#5368]
Right keys [1]: [one#5379]
Join condition: None

(22) Project [codegen id : 6]
Output [4]: [id#5365L, one#5368, two#5369, three#5370]
Input [5]: [id#5365L, one#5368, two#5369, three#5370, one#5379]

(23) Scan parquet default.fact1
Output [4]: [id#5365L, one#5368, two#5369, three#5370]
Batched: true
Location: InMemoryFileIndex [file:/Users/weixiuli/git/master/sql/catalyst/spark-warehouse/org.apache.spark.sql.DynamicPartitionPruningV1SuiteAEOn/fact1/one=533/two=534/three=535, ... 999 entries]
PartitionFilters: [isnotnull(one#5368), dynamicpruningexpression(one#5368 IN dynamicpruning#5386)]
ReadSchema: struct<id:bigint>

(24) Project
Output [1]: [one#5372]
Input [4]: [one#5372, two#5373, three#5374, prod#5375L]

(25) BroadcastExchange
Input [1]: [one#5372]
Arguments: HashedRelationBroadcastMode(List(input[0, string, true]),false), [id=#381]

(26) BroadcastHashJoin
Left keys [1]: [one#5368]
Right keys [1]: [one#5372]
Join condition: None

(27) Project
Output [4]: [id#5365L, one#5368, two#5369, three#5370]
Input [5]: [id#5365L, one#5368, two#5369, three#5370, one#5372]

(28) Exchange
Input [4]: [id#5365L, one#5368, two#5369, three#5370]
Arguments: hashpartitioning(one#5368, 5), ENSURE_REQUIREMENTS, [id=#386]

(29) Sort
Input [4]: [id#5365L, one#5368, two#5369, three#5370]
Arguments: [one#5368 ASC NULLS FIRST], false, 0

(30) Scan parquet default.fact2
Output [3]: [one#5379, two#5380, three#5381]
Batched: true
Location: InMemoryFileIndex [file:/Users/weixiuli/git/master/sql/catalyst/spark-warehouse/org.apache.spark.sql.DynamicPartitionPruningV1SuiteAEOn/fact2/one=533/two=534/three=535, ... 999 entries]
PartitionFilters: [isnotnull(one#5379), dynamicpruningexpression(one#5379 IN dynamicpruning#5387)]
ReadSchema: struct<>

(31) Project
Output [1]: [one#5379]
Input [3]: [one#5379, two#5380, three#5381]

(32) Exchange
Input [1]: [one#5379]
Arguments: hashpartitioning(one#5379, 5), ENSURE_REQUIREMENTS, [id=#387]

(33) Sort
Input [1]: [one#5379]
Arguments: [one#5379 ASC NULLS FIRST], false, 0

(34) SortMergeJoin
Left keys [1]: [one#5368]
Right keys [1]: [one#5379]
Join condition: None

(35) Project
Output [4]: [id#5365L, one#5368, two#5369, three#5370]
Input [5]: [id#5365L, one#5368, two#5369, three#5370, one#5379]

(36) AdaptiveSparkPlan
Output [4]: [id#5365L, one#5368, two#5369, three#5370]
Arguments: isFinalPlan=true

```

after this pr:

```log
== Physical Plan ==
AdaptiveSparkPlan (35)
+- == Final Plan ==
   * Project (22)
   +- * SortMergeJoin LeftOuter (21)
      :- * Sort (13)
      :  +- AQEShuffleRead (12)
      :     +- ShuffleQueryStage (11)
      :        +- Exchange (10)
      :           +- * Project (9)
      :              +- * BroadcastHashJoin Inner BuildRight (8)
      :                 :- * ColumnarToRow (2)
      :                 :  +- Scan parquet default.fact1 (1)
      :                 +- BroadcastQueryStage (7)
      :                    +- BroadcastExchange (6)
      :                       +- * Project (5)
      :                          +- * ColumnarToRow (4)
      :                             +- Scan parquet default.dim (3)
      +- * Sort (20)
         +- AQEShuffleRead (19)
            +- ShuffleQueryStage (18)
               +- Exchange (17)
                  +- * Project (16)
                     +- * ColumnarToRow (15)
                        +- Scan parquet default.fact2 (14)
+- == Initial Plan ==
   Project (34)
   +- SortMergeJoin LeftOuter (33)
      :- Sort (29)
      :  +- Exchange (28)
      :     +- Project (27)
      :        +- BroadcastHashJoin Inner BuildRight (26)
      :           :- Scan parquet default.fact1 (23)
      :           +- BroadcastExchange (25)
      :              +- Project (24)
      :                 +- Scan parquet default.dim (3)
      +- Sort (32)
         +- Exchange (31)
            +- Project (30)
               +- Scan parquet default.fact2 (14)


(1) Scan parquet default.fact1
Output [4]: [id#5365L, one#5368, two#5369, three#5370]
Batched: true
Location: InMemoryFileIndex [file:/Users/weixiuli/git/master/sql/catalyst/spark-warehouse/org.apache.spark.sql.DynamicPartitionPruningV1SuiteAEOn/fact1/one=533/two=534/three=535, ... 999 entries]
PartitionFilters: [isnotnull(one#5368), dynamicpruningexpression(one#5368 IN dynamicpruning#5386)]
ReadSchema: struct<id:bigint>

(2) ColumnarToRow [codegen id : 3]
Input [4]: [id#5365L, one#5368, two#5369, three#5370]

(3) Scan parquet default.dim
Output [4]: [one#5372, two#5373, three#5374, prod#5375L]
Batched: true
Location: InMemoryFileIndex [file:/Users/weixiuli/git/master/sql/catalyst/spark-warehouse/org.apache.spark.sql.DynamicPartitionPruningV1SuiteAEOn/dim/one=10/two=11/three=12/prod=90]
PartitionFilters: [isnotnull(prod#5375L), (prod#5375L > 80), isnotnull(one#5372)]
ReadSchema: struct<>

(4) ColumnarToRow [codegen id : 1]
Input [4]: [one#5372, two#5373, three#5374, prod#5375L]

(5) Project [codegen id : 1]
Output [1]: [one#5372]
Input [4]: [one#5372, two#5373, three#5374, prod#5375L]

(6) BroadcastExchange
Input [1]: [one#5372]
Arguments: HashedRelationBroadcastMode(List(input[0, string, true]),false), [id=#387]

(7) BroadcastQueryStage
Output [1]: [one#5372]
Arguments: 0

(8) BroadcastHashJoin [codegen id : 3]
Left keys [1]: [one#5368]
Right keys [1]: [one#5372]
Join condition: None

(9) Project [codegen id : 3]
Output [4]: [id#5365L, one#5368, two#5369, three#5370]
Input [5]: [id#5365L, one#5368, two#5369, three#5370, one#5372]

(10) Exchange
Input [4]: [id#5365L, one#5368, two#5369, three#5370]
Arguments: hashpartitioning(one#5368, 5), ENSURE_REQUIREMENTS, [id=#477]

(11) ShuffleQueryStage
Output [4]: [id#5365L, one#5368, two#5369, three#5370]
Arguments: 2

(12) AQEShuffleRead
Input [4]: [id#5365L, one#5368, two#5369, three#5370]
Arguments: coalesced

(13) Sort [codegen id : 4]
Input [4]: [id#5365L, one#5368, two#5369, three#5370]
Arguments: [one#5368 ASC NULLS FIRST], false, 0

(14) Scan parquet default.fact2
Output [3]: [one#5379, two#5380, three#5381]
Batched: true
Location: InMemoryFileIndex [file:/Users/weixiuli/git/master/sql/catalyst/spark-warehouse/org.apache.spark.sql.DynamicPartitionPruningV1SuiteAEOn/fact2/one=533/two=534/three=535, ... 999 entries]
PartitionFilters: [isnotnull(one#5379)]
ReadSchema: struct<>

(15) ColumnarToRow [codegen id : 2]
Input [3]: [one#5379, two#5380, three#5381]

(16) Project [codegen id : 2]
Output [1]: [one#5379]
Input [3]: [one#5379, two#5380, three#5381]

(17) Exchange
Input [1]: [one#5379]
Arguments: hashpartitioning(one#5379, 5), ENSURE_REQUIREMENTS, [id=#411]

(18) ShuffleQueryStage
Output [1]: [one#5379]
Arguments: 1

(19) AQEShuffleRead
Input [1]: [one#5379]
Arguments: coalesced

(20) Sort [codegen id : 5]
Input [1]: [one#5379]
Arguments: [one#5379 ASC NULLS FIRST], false, 0

(21) SortMergeJoin [codegen id : 6]
Left keys [1]: [one#5368]
Right keys [1]: [one#5379]
Join condition: None

(22) Project [codegen id : 6]
Output [4]: [id#5365L, one#5368, two#5369, three#5370]
Input [5]: [id#5365L, one#5368, two#5369, three#5370, one#5379]

(23) Scan parquet default.fact1
Output [4]: [id#5365L, one#5368, two#5369, three#5370]
Batched: true
Location: InMemoryFileIndex [file:/Users/weixiuli/git/master/sql/catalyst/spark-warehouse/org.apache.spark.sql.DynamicPartitionPruningV1SuiteAEOn/fact1/one=533/two=534/three=535, ... 999 entries]
PartitionFilters: [isnotnull(one#5368), dynamicpruningexpression(one#5368 IN dynamicpruning#5386)]
ReadSchema: struct<id:bigint>

(24) Project
Output [1]: [one#5372]
Input [4]: [one#5372, two#5373, three#5374, prod#5375L]

(25) BroadcastExchange
Input [1]: [one#5372]
Arguments: HashedRelationBroadcastMode(List(input[0, string, true]),false), [id=#354]

(26) BroadcastHashJoin
Left keys [1]: [one#5368]
Right keys [1]: [one#5372]
Join condition: None

(27) Project
Output [4]: [id#5365L, one#5368, two#5369, three#5370]
Input [5]: [id#5365L, one#5368, two#5369, three#5370, one#5372]

(28) Exchange
Input [4]: [id#5365L, one#5368, two#5369, three#5370]
Arguments: hashpartitioning(one#5368, 5), ENSURE_REQUIREMENTS, [id=#359]

(29) Sort
Input [4]: [id#5365L, one#5368, two#5369, three#5370]
Arguments: [one#5368 ASC NULLS FIRST], false, 0

(30) Project
Output [1]: [one#5379]
Input [3]: [one#5379, two#5380, three#5381]

(31) Exchange
Input [1]: [one#5379]
Arguments: hashpartitioning(one#5379, 5), ENSURE_REQUIREMENTS, [id=#360]

(32) Sort
Input [1]: [one#5379]
Arguments: [one#5379 ASC NULLS FIRST], false, 0

(33) SortMergeJoin
Left keys [1]: [one#5368]
Right keys [1]: [one#5379]
Join condition: None

(34) Project
Output [4]: [id#5365L, one#5368, two#5369, three#5370]
Input [5]: [id#5365L, one#5368, two#5369, three#5370, one#5379]

(35) AdaptiveSparkPlan
Output [4]: [id#5365L, one#5368, two#5369, three#5370]
Arguments: isFinalPlan=true
```

Exist unittests.",https://api.github.com/repos/apache/spark/issues/34805/timeline,,spark,apache,weixiuli,39684231,MDQ6VXNlcjM5Njg0MjMx,https://avatars.githubusercontent.com/u/39684231?v=4,,https://api.github.com/users/weixiuli,https://github.com/weixiuli,https://api.github.com/users/weixiuli/followers,https://api.github.com/users/weixiuli/following{/other_user},https://api.github.com/users/weixiuli/gists{/gist_id},https://api.github.com/users/weixiuli/starred{/owner}{/repo},https://api.github.com/users/weixiuli/subscriptions,https://api.github.com/users/weixiuli/orgs,https://api.github.com/users/weixiuli/repos,https://api.github.com/users/weixiuli/events{/privacy},https://api.github.com/users/weixiuli/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34805,https://github.com/apache/spark/pull/34805,https://github.com/apache/spark/pull/34805.diff,https://github.com/apache/spark/pull/34805.patch,,https://api.github.com/repos/apache/spark/issues/34805/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
76,https://api.github.com/repos/apache/spark/issues/34800,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34800/labels{/name},https://api.github.com/repos/apache/spark/issues/34800/comments,https://api.github.com/repos/apache/spark/issues/34800/events,https://github.com/apache/spark/pull/34800,1070684037,PR_kwDOAQXtWs4vXb9K,34800,[SPARK-37538][SQL] Replace single projection expand,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,9,2021-12-03T15:06:07Z,2021-12-09T06:24:38Z,,CONTRIBUTOR,,False,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->
In the `Optimizer` replace all instances of `Expand` with only 1 projection with a `Project`.

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->
Both grouping sets and distinct aggregations can create `Expand` with only 1 projection. Removing those can improve the performance in two ways:
* Enable optimization rules, that can not work with `Expand`
* Avoid unnecessary copying - `ExpandExec` has `needCopyResult: Boolean = true`

### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->
No

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.
-->
New UT",https://api.github.com/repos/apache/spark/issues/34800/timeline,,spark,apache,tanelk,3342974,MDQ6VXNlcjMzNDI5NzQ=,https://avatars.githubusercontent.com/u/3342974?v=4,,https://api.github.com/users/tanelk,https://github.com/tanelk,https://api.github.com/users/tanelk/followers,https://api.github.com/users/tanelk/following{/other_user},https://api.github.com/users/tanelk/gists{/gist_id},https://api.github.com/users/tanelk/starred{/owner}{/repo},https://api.github.com/users/tanelk/subscriptions,https://api.github.com/users/tanelk/orgs,https://api.github.com/users/tanelk/repos,https://api.github.com/users/tanelk/events{/privacy},https://api.github.com/users/tanelk/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34800,https://github.com/apache/spark/pull/34800,https://github.com/apache/spark/pull/34800.diff,https://github.com/apache/spark/pull/34800.patch,,https://api.github.com/repos/apache/spark/issues/34800/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
77,https://api.github.com/repos/apache/spark/issues/34799,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34799/labels{/name},https://api.github.com/repos/apache/spark/issues/34799/comments,https://api.github.com/repos/apache/spark/issues/34799/events,https://github.com/apache/spark/pull/34799,1070491685,PR_kwDOAQXtWs4vWzJJ,34799,[SPARK-37527][SQL] Translate more standard aggregate functions for pushdown,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,23,2021-12-03T11:26:29Z,2021-12-08T08:39:02Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?
Currently, Spark aggregate pushdown will translate some standard aggregate functions, so that compile these functions to adapt specify database.
After this job, users could override `JdbcDialect.compileAggregate` to implement some aggregate functions supported by some database.
Because some aggregate functions will be converted show below, this PR no need to match them.

|Input|Parsed|Optimized|
|------|--------------------|----------|
|`Every`| `aggregate.BoolAnd` |`Min`|
|`Any`| `aggregate.BoolOr` |`Max`|
|`Some`| `aggregate.BoolOr` |`Max`|

### Why are the changes needed?
Make the implement of `*Dialect` could extends the aggregate functions by override `JdbcDialect.compileAggregate`.


### Does this PR introduce _any_ user-facing change?
Yes. Users could pushdown more aggregate functions.


### How was this patch tested?
Exists tests.
",https://api.github.com/repos/apache/spark/issues/34799/timeline,,spark,apache,beliefer,8486025,MDQ6VXNlcjg0ODYwMjU=,https://avatars.githubusercontent.com/u/8486025?v=4,,https://api.github.com/users/beliefer,https://github.com/beliefer,https://api.github.com/users/beliefer/followers,https://api.github.com/users/beliefer/following{/other_user},https://api.github.com/users/beliefer/gists{/gist_id},https://api.github.com/users/beliefer/starred{/owner}{/repo},https://api.github.com/users/beliefer/subscriptions,https://api.github.com/users/beliefer/orgs,https://api.github.com/users/beliefer/repos,https://api.github.com/users/beliefer/events{/privacy},https://api.github.com/users/beliefer/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34799,https://github.com/apache/spark/pull/34799,https://github.com/apache/spark/pull/34799.diff,https://github.com/apache/spark/pull/34799.patch,,https://api.github.com/repos/apache/spark/issues/34799/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
78,https://api.github.com/repos/apache/spark/issues/34794,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34794/labels{/name},https://api.github.com/repos/apache/spark/issues/34794/comments,https://api.github.com/repos/apache/spark/issues/34794/events,https://github.com/apache/spark/pull/34794,1070330861,PR_kwDOAQXtWs4vWRh-,34794,[SPARK-37532][CORE] Limit the length of RDD name,"[{'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,6,2021-12-03T08:17:01Z,2021-12-06T03:46:01Z,,CONTRIBUTOR,,False,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->

APIs like sc.newHadoopFile accepts a string representation of comma separate paths, which could be very very long, and we set it directly as the RDD name. This could be an unfriendly name on the UI and cost tons of driver memory for the whole RDD scope.



### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->

make the RDD name on UI more friendly
make it cost less driver memory

### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->

yes, for a single RDD like this, user will see a much short one

![image](https://user-images.githubusercontent.com/8326978/144568404-bbfa9074-52b7-4a2b-a71a-a73719233f4c.png)

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.
-->

passing GA
",https://api.github.com/repos/apache/spark/issues/34794/timeline,,spark,apache,yaooqinn,8326978,MDQ6VXNlcjgzMjY5Nzg=,https://avatars.githubusercontent.com/u/8326978?v=4,,https://api.github.com/users/yaooqinn,https://github.com/yaooqinn,https://api.github.com/users/yaooqinn/followers,https://api.github.com/users/yaooqinn/following{/other_user},https://api.github.com/users/yaooqinn/gists{/gist_id},https://api.github.com/users/yaooqinn/starred{/owner}{/repo},https://api.github.com/users/yaooqinn/subscriptions,https://api.github.com/users/yaooqinn/orgs,https://api.github.com/users/yaooqinn/repos,https://api.github.com/users/yaooqinn/events{/privacy},https://api.github.com/users/yaooqinn/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34794,https://github.com/apache/spark/pull/34794,https://github.com/apache/spark/pull/34794.diff,https://github.com/apache/spark/pull/34794.patch,,https://api.github.com/repos/apache/spark/issues/34794/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
79,https://api.github.com/repos/apache/spark/issues/34791,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34791/labels{/name},https://api.github.com/repos/apache/spark/issues/34791/comments,https://api.github.com/repos/apache/spark/issues/34791/events,https://github.com/apache/spark/pull/34791,1070249492,PR_kwDOAQXtWs4vWAvU,34791,[SPARK-37528][SQL][CORE] Support reorder tasks during scheduling by shuffle partition size in AQE,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,7,2021-12-03T06:01:32Z,2021-12-03T13:34:09Z,,CONTRIBUTOR,,False,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->
In order to let task know its input size, we need to add a new method in `org.apache.spark.Partition`. Then at SQL side, we can pass the data size into partition before executing the shuffle read stage (thanks to the stage level scheduler in AQE). So, overall the changes include:

- Add a new method `predictedInputBytes` in `org.apache.spark.Partition`
- Pass the data size to `ShuffledRowRDD.getPartitions`

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->

This PR tries to reorder tasks by `predictedInputBytes` with big first. Assume the larger amount of input data takes longer to execute. It can save the whole stage execution time. Let's say we have one stage with 4 tasks and the `defaultParallelism` is 2 and the 4 tasks have different execution time with [1s, 3s, 2s, 4s].

- in normal, the execution time of the stage is: 7s
- after reorder the tasks, the execution time of the stage is: 5s

### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->
yes, a new config `spark.scheduler.reorderTasks.enabled` to decide if we allow to reorder tasks.

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.
-->

Add test in:

- org.apache.spark.sql.execution.adaptive.AdaptiveQueryExecSuite
- org.apache.spark.scheduler.DAGSchedulerSuite
",https://api.github.com/repos/apache/spark/issues/34791/timeline,,spark,apache,ulysses-you,12025282,MDQ6VXNlcjEyMDI1Mjgy,https://avatars.githubusercontent.com/u/12025282?v=4,,https://api.github.com/users/ulysses-you,https://github.com/ulysses-you,https://api.github.com/users/ulysses-you/followers,https://api.github.com/users/ulysses-you/following{/other_user},https://api.github.com/users/ulysses-you/gists{/gist_id},https://api.github.com/users/ulysses-you/starred{/owner}{/repo},https://api.github.com/users/ulysses-you/subscriptions,https://api.github.com/users/ulysses-you/orgs,https://api.github.com/users/ulysses-you/repos,https://api.github.com/users/ulysses-you/events{/privacy},https://api.github.com/users/ulysses-you/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34791,https://github.com/apache/spark/pull/34791,https://github.com/apache/spark/pull/34791.diff,https://github.com/apache/spark/pull/34791.patch,,https://api.github.com/repos/apache/spark/issues/34791/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
80,https://api.github.com/repos/apache/spark/issues/34789,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34789/labels{/name},https://api.github.com/repos/apache/spark/issues/34789/comments,https://api.github.com/repos/apache/spark/issues/34789/events,https://github.com/apache/spark/pull/34789,1070182138,PR_kwDOAQXtWs4vVyty,34789,[SPARK-37519][SQL] Support Relation With LateralView,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,1,2021-12-03T03:28:41Z,2021-12-07T03:37:54Z,,NONE,,False,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->

Users can more convenient to use `LATERAL VIEW` with relation instead of creating a subqury


### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->

Before this change:
```
SELECT *
FROM ( SELECT CC1.C_AGE1 FROM PERSON1 AS P1
LATERAL VIEW EXPLODE(ARRAY(30, 60)) CC1 AS C_AGE1 ) AS P1
LEFT JOIN PERSON2 P2 ON P1.ID = P2.ID  AND P1.C_AGE1=P2.AGE;
```
After:
```
SELECT *
FROM PERSON1 AS P1
LATERAL VIEW EXPLODE(ARRAY(30, 60)) CC1 AS C_AGE1
LEFT JOIN PERSON2 P2 ON P1.ID = P2.ID  AND CC1.C_AGE1=P2.AGE;
```

### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->


### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.
-->
Add unit tests
",https://api.github.com/repos/apache/spark/issues/34789/timeline,,spark,apache,TongWei1105,68682646,MDQ6VXNlcjY4NjgyNjQ2,https://avatars.githubusercontent.com/u/68682646?v=4,,https://api.github.com/users/TongWei1105,https://github.com/TongWei1105,https://api.github.com/users/TongWei1105/followers,https://api.github.com/users/TongWei1105/following{/other_user},https://api.github.com/users/TongWei1105/gists{/gist_id},https://api.github.com/users/TongWei1105/starred{/owner}{/repo},https://api.github.com/users/TongWei1105/subscriptions,https://api.github.com/users/TongWei1105/orgs,https://api.github.com/users/TongWei1105/repos,https://api.github.com/users/TongWei1105/events{/privacy},https://api.github.com/users/TongWei1105/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34789,https://github.com/apache/spark/pull/34789,https://github.com/apache/spark/pull/34789.diff,https://github.com/apache/spark/pull/34789.patch,,https://api.github.com/repos/apache/spark/issues/34789/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
81,https://api.github.com/repos/apache/spark/issues/34785,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34785/labels{/name},https://api.github.com/repos/apache/spark/issues/34785/comments,https://api.github.com/repos/apache/spark/issues/34785/events,https://github.com/apache/spark/pull/34785,1069914068,PR_kwDOAQXtWs4vU61V,34785,[SPARK-37523][SQL] Support optimize skewed partitions in Distribution and Ordering if numPartitions is not specified,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,12,2021-12-02T19:36:31Z,2021-12-30T16:52:23Z,,CONTRIBUTOR,,False,"
### What changes were proposed in this pull request?
Support optimize skewed partitions in Distribution and Ordering if numPartitions is not specified

### Why are the changes needed?
When doing repartition in distribution and sort, we will use Rebalance operator instead of RepartitionByExpression to optimize skewed partitions when
1. numPartitions is not specified by the data source, and
2. sortOrder is specified. This is because the requested distribution needs to be guaranteed, which can only be achieved by using RangePartitioning, not HashPartitioning.

### Does this PR introduce _any_ user-facing change?
No


### How was this patch tested?
Existing and new tests
",https://api.github.com/repos/apache/spark/issues/34785/timeline,,spark,apache,huaxingao,13592258,MDQ6VXNlcjEzNTkyMjU4,https://avatars.githubusercontent.com/u/13592258?v=4,,https://api.github.com/users/huaxingao,https://github.com/huaxingao,https://api.github.com/users/huaxingao/followers,https://api.github.com/users/huaxingao/following{/other_user},https://api.github.com/users/huaxingao/gists{/gist_id},https://api.github.com/users/huaxingao/starred{/owner}{/repo},https://api.github.com/users/huaxingao/subscriptions,https://api.github.com/users/huaxingao/orgs,https://api.github.com/users/huaxingao/repos,https://api.github.com/users/huaxingao/events{/privacy},https://api.github.com/users/huaxingao/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34785,https://github.com/apache/spark/pull/34785,https://github.com/apache/spark/pull/34785.diff,https://github.com/apache/spark/pull/34785.patch,,https://api.github.com/repos/apache/spark/issues/34785/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
82,https://api.github.com/repos/apache/spark/issues/34781,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34781/labels{/name},https://api.github.com/repos/apache/spark/issues/34781/comments,https://api.github.com/repos/apache/spark/issues/34781/events,https://github.com/apache/spark/pull/34781,1069459520,PR_kwDOAQXtWs4vTaWA,34781,[WIP] Use error-classes for spark-core errors - 1st batch,"[{'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,6,2021-12-02T11:52:46Z,2021-12-02T18:31:04Z,,CONTRIBUTOR,,False,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->
This change is to refactor the 1st batch of errors in spark-core to use error-classes. 

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->
This is to follow the error class framework in spark-core.

### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->
No.

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.
-->
Existing unit tests. ",https://api.github.com/repos/apache/spark/issues/34781/timeline,,spark,apache,bozhang2820,44179472,MDQ6VXNlcjQ0MTc5NDcy,https://avatars.githubusercontent.com/u/44179472?v=4,,https://api.github.com/users/bozhang2820,https://github.com/bozhang2820,https://api.github.com/users/bozhang2820/followers,https://api.github.com/users/bozhang2820/following{/other_user},https://api.github.com/users/bozhang2820/gists{/gist_id},https://api.github.com/users/bozhang2820/starred{/owner}{/repo},https://api.github.com/users/bozhang2820/subscriptions,https://api.github.com/users/bozhang2820/orgs,https://api.github.com/users/bozhang2820/repos,https://api.github.com/users/bozhang2820/events{/privacy},https://api.github.com/users/bozhang2820/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34781,https://github.com/apache/spark/pull/34781,https://github.com/apache/spark/pull/34781.diff,https://github.com/apache/spark/pull/34781.patch,,https://api.github.com/repos/apache/spark/issues/34781/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
83,https://api.github.com/repos/apache/spark/issues/34780,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34780/labels{/name},https://api.github.com/repos/apache/spark/issues/34780/comments,https://api.github.com/repos/apache/spark/issues/34780/events,https://github.com/apache/spark/pull/34780,1069445095,PR_kwDOAQXtWs4vTXRU,34780,[SPARK-37517][SQL] Keep consistent order of columns with user specify for v1 table,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,9,2021-12-02T11:35:55Z,2021-12-17T12:32:33Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?
1. keep columns order with user specified instead of put partition columns at last.
2. Modify the `partitionSchema` and `dataSchema` implementation.

### Why are the changes needed?
discuss at [#34719](https://github.com/apache/spark/pull/34719#discussion_r758157813).

### Does this PR introduce _any_ user-facing change?
No

### How was this patch tested?
Add test case.
",https://api.github.com/repos/apache/spark/issues/34780/timeline,,spark,apache,Peng-Lei,41178002,MDQ6VXNlcjQxMTc4MDAy,https://avatars.githubusercontent.com/u/41178002?v=4,,https://api.github.com/users/Peng-Lei,https://github.com/Peng-Lei,https://api.github.com/users/Peng-Lei/followers,https://api.github.com/users/Peng-Lei/following{/other_user},https://api.github.com/users/Peng-Lei/gists{/gist_id},https://api.github.com/users/Peng-Lei/starred{/owner}{/repo},https://api.github.com/users/Peng-Lei/subscriptions,https://api.github.com/users/Peng-Lei/orgs,https://api.github.com/users/Peng-Lei/repos,https://api.github.com/users/Peng-Lei/events{/privacy},https://api.github.com/users/Peng-Lei/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34780,https://github.com/apache/spark/pull/34780,https://github.com/apache/spark/pull/34780.diff,https://github.com/apache/spark/pull/34780.patch,,https://api.github.com/repos/apache/spark/issues/34780/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
84,https://api.github.com/repos/apache/spark/issues/34779,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34779/labels{/name},https://api.github.com/repos/apache/spark/issues/34779/comments,https://api.github.com/repos/apache/spark/issues/34779/events,https://github.com/apache/spark/pull/34779,1069412780,PR_kwDOAQXtWs4vTQY8,34779,[SPARK-37518][SQL] Inject an early scan pushdown rule,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,14,2021-12-02T11:01:55Z,2021-12-14T01:22:16Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?
Currently, Spark supports push down filters, aggregates and limit. All the job is completed by `V2ScanRelationPushDown`.
But `V2ScanRelationPushDown` have a lot limit.
Users want apply custom rule for push down after `V2ScanRelationPushDown` failed.


### Why are the changes needed?
Easy for users to apply custom pushdown rules.


### Does this PR introduce _any_ user-facing change?
'Yes'.
Users can inject custom early scan pushdown rules.


### How was this patch tested?
New tests.
",https://api.github.com/repos/apache/spark/issues/34779/timeline,,spark,apache,beliefer,8486025,MDQ6VXNlcjg0ODYwMjU=,https://avatars.githubusercontent.com/u/8486025?v=4,,https://api.github.com/users/beliefer,https://github.com/beliefer,https://api.github.com/users/beliefer/followers,https://api.github.com/users/beliefer/following{/other_user},https://api.github.com/users/beliefer/gists{/gist_id},https://api.github.com/users/beliefer/starred{/owner}{/repo},https://api.github.com/users/beliefer/subscriptions,https://api.github.com/users/beliefer/orgs,https://api.github.com/users/beliefer/repos,https://api.github.com/users/beliefer/events{/privacy},https://api.github.com/users/beliefer/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34779,https://github.com/apache/spark/pull/34779,https://github.com/apache/spark/pull/34779.diff,https://github.com/apache/spark/pull/34779.patch,,https://api.github.com/repos/apache/spark/issues/34779/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
85,https://api.github.com/repos/apache/spark/issues/34773,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34773/labels{/name},https://api.github.com/repos/apache/spark/issues/34773/comments,https://api.github.com/repos/apache/spark/issues/34773/events,https://github.com/apache/spark/pull/34773,1069094826,PR_kwDOAQXtWs4vSN1a,34773,[SPARK-33898][SQL][FOLLOWUP] Unify the v2 behavior with v1 for `SHOW CREATE TABLE` command,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,3,2021-12-02T03:40:53Z,2021-12-03T01:22:57Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?
1. Move the `SHOW CREATE TABLE w/ char/varchar` to `CharVarcharDDLTestBase`
2. Fix the behavior different with v1 command that about the `TBLPROPERTIES`

### Why are the changes needed?
Before the [#PR](https://github.com/apache/spark/pull/34719) merge. We should handle some different behavior or bugs between v1 and v2 command.

### Does this PR introduce _any_ user-facing change?
No

### How was this patch tested?
existed test case.
",https://api.github.com/repos/apache/spark/issues/34773/timeline,,spark,apache,Peng-Lei,41178002,MDQ6VXNlcjQxMTc4MDAy,https://avatars.githubusercontent.com/u/41178002?v=4,,https://api.github.com/users/Peng-Lei,https://github.com/Peng-Lei,https://api.github.com/users/Peng-Lei/followers,https://api.github.com/users/Peng-Lei/following{/other_user},https://api.github.com/users/Peng-Lei/gists{/gist_id},https://api.github.com/users/Peng-Lei/starred{/owner}{/repo},https://api.github.com/users/Peng-Lei/subscriptions,https://api.github.com/users/Peng-Lei/orgs,https://api.github.com/users/Peng-Lei/repos,https://api.github.com/users/Peng-Lei/events{/privacy},https://api.github.com/users/Peng-Lei/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34773,https://github.com/apache/spark/pull/34773,https://github.com/apache/spark/pull/34773.diff,https://github.com/apache/spark/pull/34773.patch,,https://api.github.com/repos/apache/spark/issues/34773/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
86,https://api.github.com/repos/apache/spark/issues/34769,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34769/labels{/name},https://api.github.com/repos/apache/spark/issues/34769/comments,https://api.github.com/repos/apache/spark/issues/34769/events,https://github.com/apache/spark/pull/34769,1068338424,PR_kwDOAQXtWs4vPspz,34769,[SPARK-37463][SQL] Read/Write Timestamp ntz to Orc uses UTC timestamp,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,4,2021-12-01T12:16:54Z,2021-12-01T17:27:09Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?
This PR used to fix the issue
https://github.com/apache/spark/pull/33588#issuecomment-978719988

The root cause is Orc write/read timestamp with local timezone in default. The local timezone will be changed.
If the Orc writer write timestamp with local timezone(e.g. America/Los_Angeles), when the Orc reader reading the timestamp with local timezone(e.g. Europe/Amsterdam), the value of timestamp will be different.

If we let the Orc writer write timestamp with UTC timezone, when the Orc reader reading the timestamp with  UTC timezone too, the value of timestamp will be correct.

This PR let Orc write/read Timestamp with UTC timezone by call `useUTCTimestamp(true)` for readers or writers.

The related Orc source:
https://github.com/apache/orc/blob/3f1e57cf1cebe58027c1bd48c09eef4e9717a9e3/java/core/src/java/org/apache/orc/impl/WriterImpl.java#L525

https://github.com/apache/orc/blob/1f68ac0c7f2ae804b374500dcf1b4d7abe30ffeb/java/core/src/java/org/apache/orc/impl/TreeReaderFactory.java#L1184

Another problem is Spark 3.3 or newer read the Orc file written by Spark 3.2 or prior. Because the older Spark write timestamp with local timezone, no need to read them with UTC timezone. Otherwise, an incorrect value of timestamp occurs.

### Why are the changes needed?
Fix the bug for Orc timestamp.


### Does this PR introduce _any_ user-facing change?
Orc timestamp ntz is a new feature not release yet.


### How was this patch tested?
New tests.
",https://api.github.com/repos/apache/spark/issues/34769/timeline,,spark,apache,beliefer,8486025,MDQ6VXNlcjg0ODYwMjU=,https://avatars.githubusercontent.com/u/8486025?v=4,,https://api.github.com/users/beliefer,https://github.com/beliefer,https://api.github.com/users/beliefer/followers,https://api.github.com/users/beliefer/following{/other_user},https://api.github.com/users/beliefer/gists{/gist_id},https://api.github.com/users/beliefer/starred{/owner}{/repo},https://api.github.com/users/beliefer/subscriptions,https://api.github.com/users/beliefer/orgs,https://api.github.com/users/beliefer/repos,https://api.github.com/users/beliefer/events{/privacy},https://api.github.com/users/beliefer/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34769,https://github.com/apache/spark/pull/34769,https://github.com/apache/spark/pull/34769.diff,https://github.com/apache/spark/pull/34769.patch,,https://api.github.com/repos/apache/spark/issues/34769/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
87,https://api.github.com/repos/apache/spark/issues/34765,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34765/labels{/name},https://api.github.com/repos/apache/spark/issues/34765/comments,https://api.github.com/repos/apache/spark/issues/34765/events,https://github.com/apache/spark/pull/34765,1068014301,PR_kwDOAQXtWs4vOn5J,34765,[WIP][SPARK-37487][SQL][CORE] Avoid performing CollectMetrics twice if the operation is followed by global sort.,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,5,2021-12-01T06:26:59Z,2021-12-01T09:26:08Z,,MEMBER,,True,"### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->
This PR fixes an issue that `CollectMetrics` performs twice if it's followed by global sort like as follows.
```
val df = spark.range(100)
  .observe(
    name = ""my_event"",
    min($""id"").as(""min_val""),
    max($""id"").as(""max_val""),
    sum($""id""),
    count(when($""id"" % 2 === 0, 1)).as(""num_even""))
  .sort($""id"".desc)
```

The expected statistics calculated by `CollectMetrics` is `[0,99,4950,50]` but the actual result is `[0,99,9900,100]`.
The reason is that jobs for sampling can run before the global sort, which performs extra `CollectMetrics`.
https://github.com/apache/spark/blob/e7fa28930dce468df02b5915e1792ada758a96e3/core/src/main/scala/org/apache/spark/Partitioner.scala#L171
https://github.com/apache/spark/blob/e7fa28930dce468df02b5915e1792ada758a96e3/core/src/main/scala/org/apache/spark/Partitioner.scala#L195

The solution this PR proposes to introduce a property `spark.job.isSamplingJob` which is intended to be get/set internally.
Before the sampling jobs run, Spark sets the property, and reset it after the jobs finish.
Then, `CollectMetrics` can judge a task is whether of a sampling job or not.

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->
Bug fix.

### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->
No.

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.
-->
New test.",https://api.github.com/repos/apache/spark/issues/34765/timeline,,spark,apache,sarutak,4736016,MDQ6VXNlcjQ3MzYwMTY=,https://avatars.githubusercontent.com/u/4736016?v=4,,https://api.github.com/users/sarutak,https://github.com/sarutak,https://api.github.com/users/sarutak/followers,https://api.github.com/users/sarutak/following{/other_user},https://api.github.com/users/sarutak/gists{/gist_id},https://api.github.com/users/sarutak/starred{/owner}{/repo},https://api.github.com/users/sarutak/subscriptions,https://api.github.com/users/sarutak/orgs,https://api.github.com/users/sarutak/repos,https://api.github.com/users/sarutak/events{/privacy},https://api.github.com/users/sarutak/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34765,https://github.com/apache/spark/pull/34765,https://github.com/apache/spark/pull/34765.diff,https://github.com/apache/spark/pull/34765.patch,,https://api.github.com/repos/apache/spark/issues/34765/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
88,https://api.github.com/repos/apache/spark/issues/34755,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34755/labels{/name},https://api.github.com/repos/apache/spark/issues/34755/comments,https://api.github.com/repos/apache/spark/issues/34755/events,https://github.com/apache/spark/pull/34755,1066959758,PR_kwDOAQXtWs4vLJ-R,34755,[SPARK-37502][SQL] Support cast aware output partitioning and required if it can up cast,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,4,2021-11-30T08:53:27Z,2021-11-30T14:23:08Z,,CONTRIBUTOR,,False,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->
Enhance semantic equals at `Partitioning` to support cast aware output partitioning and required if it can up cast.

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->
If a `Cast` is up cast then it should be without any truncating or precision lose or possible runtime failures. So the output partitioning should be same with/without `Cast` if the `Cast` is up cast.

Let's say we have a query:
```sql
-- v1: c1 int
-- v2: c2 long

SELECT * FROM v2 JOIN (SELECT c1, count(*) FROM v1 GROUP BY c1) v1 ON v1.c1 = v2.c2
```

The executed plan contains three shuffle nodes which looks like:
```sql
SortMergeJoin
  Exchange(cast(c1 as bigint))
    HashAggregate
      Exchange(c1)
        Scan v1
  Exchange(c2)
    Scan v2
```

We can simplify the plan using two shuffle nodes:
```sql
SortMergeJoin
  HashAggregate
    Exchange(c1)
      Scan v1
  Exchange(c2)
    Scan v2
```

### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->
yes, the plan may be changed

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.
-->
Add test in:
- org.apache.spark.sql.catalyst.DistributionSuite
- org.apache.spark.sql.SQLQuerySuite",https://api.github.com/repos/apache/spark/issues/34755/timeline,,spark,apache,ulysses-you,12025282,MDQ6VXNlcjEyMDI1Mjgy,https://avatars.githubusercontent.com/u/12025282?v=4,,https://api.github.com/users/ulysses-you,https://github.com/ulysses-you,https://api.github.com/users/ulysses-you/followers,https://api.github.com/users/ulysses-you/following{/other_user},https://api.github.com/users/ulysses-you/gists{/gist_id},https://api.github.com/users/ulysses-you/starred{/owner}{/repo},https://api.github.com/users/ulysses-you/subscriptions,https://api.github.com/users/ulysses-you/orgs,https://api.github.com/users/ulysses-you/repos,https://api.github.com/users/ulysses-you/events{/privacy},https://api.github.com/users/ulysses-you/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34755,https://github.com/apache/spark/pull/34755,https://github.com/apache/spark/pull/34755.diff,https://github.com/apache/spark/pull/34755.patch,,https://api.github.com/repos/apache/spark/issues/34755/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
89,https://api.github.com/repos/apache/spark/issues/34752,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34752/labels{/name},https://api.github.com/repos/apache/spark/issues/34752/comments,https://api.github.com/repos/apache/spark/issues/34752/events,https://github.com/apache/spark/pull/34752,1066855178,PR_kwDOAQXtWs4vK0gX,34752,[SPARK-37515][STREAMING] minRatePerPartition should be multiplied with secsPerBatch,[],open,False,,[],,2,2021-11-30T06:43:11Z,2021-12-09T06:06:01Z,,NONE,,False,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->

`maxRatePerPartition` means ""max messages per partition per second"".
But minRatePerPartition does not. (""max messages per partition per a batch""). This is a bug.

minRatePerPartition should be multiplied with secsPerBatch


### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->


### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->

minRatePerPartition will work properly as described docs
 - https://github.com/apache/spark/blob/f97de309792f382ae823894e978f7e54f34f1a29/docs/configuration.md?plain=1#L2878-L2886

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.
-->

tested with `org.apache.spark.streaming.kafka010.DirectKafkaStreamSuite`

",https://api.github.com/repos/apache/spark/issues/34752/timeline,,spark,apache,sungpeo,13159599,MDQ6VXNlcjEzMTU5NTk5,https://avatars.githubusercontent.com/u/13159599?v=4,,https://api.github.com/users/sungpeo,https://github.com/sungpeo,https://api.github.com/users/sungpeo/followers,https://api.github.com/users/sungpeo/following{/other_user},https://api.github.com/users/sungpeo/gists{/gist_id},https://api.github.com/users/sungpeo/starred{/owner}{/repo},https://api.github.com/users/sungpeo/subscriptions,https://api.github.com/users/sungpeo/orgs,https://api.github.com/users/sungpeo/repos,https://api.github.com/users/sungpeo/events{/privacy},https://api.github.com/users/sungpeo/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34752,https://github.com/apache/spark/pull/34752,https://github.com/apache/spark/pull/34752.diff,https://github.com/apache/spark/pull/34752.patch,,https://api.github.com/repos/apache/spark/issues/34752/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
90,https://api.github.com/repos/apache/spark/issues/34742,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34742/labels{/name},https://api.github.com/repos/apache/spark/issues/34742/comments,https://api.github.com/repos/apache/spark/issues/34742/events,https://github.com/apache/spark/pull/34742,1066058077,PR_kwDOAQXtWs4vIQ1j,34742,[SPARK-37486][SQL][HIVE]  set the ContextClassLoader before using the `addJars` in `HiveClient` ,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,8,2021-11-29T13:46:37Z,2021-12-01T13:38:45Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?
In `HiveSessionResourceLoader`,  call the  function `addJars` of its supper class firstly and set the contextClassLoader in current thread, and then  use the `addJars` in `HiveClient`. to avoid that different class loaders load the same class. 

### Why are the changes needed?

when using livy to execute sql statements that will use the udf jars located in lakefs, a inner filesystem in Tencent Cloud DLC. it will threw the following exceptions:

`java.lang.LinkageError: loader constraint violation: loader (instance of sun/misc/Launcher$AppClassLoader) previously initiated loading for a different type with name`     

Maybe it happens  in some other filesystem.

This PR fix the bug  by  setting the setContextClassLoader before using the addJars in HiveClient

### Why are the changes needed?

fix  the bug that different class loaders  load the same class. 

### Does this PR introduce _any_ user-facing change?

NO

### How was this patch tested?

existing testsuites",https://api.github.com/repos/apache/spark/issues/34742/timeline,,spark,apache,kevincmchen,68981916,MDQ6VXNlcjY4OTgxOTE2,https://avatars.githubusercontent.com/u/68981916?v=4,,https://api.github.com/users/kevincmchen,https://github.com/kevincmchen,https://api.github.com/users/kevincmchen/followers,https://api.github.com/users/kevincmchen/following{/other_user},https://api.github.com/users/kevincmchen/gists{/gist_id},https://api.github.com/users/kevincmchen/starred{/owner}{/repo},https://api.github.com/users/kevincmchen/subscriptions,https://api.github.com/users/kevincmchen/orgs,https://api.github.com/users/kevincmchen/repos,https://api.github.com/users/kevincmchen/events{/privacy},https://api.github.com/users/kevincmchen/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34742,https://github.com/apache/spark/pull/34742,https://github.com/apache/spark/pull/34742.diff,https://github.com/apache/spark/pull/34742.patch,,https://api.github.com/repos/apache/spark/issues/34742/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
91,https://api.github.com/repos/apache/spark/issues/34741,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34741/labels{/name},https://api.github.com/repos/apache/spark/issues/34741/comments,https://api.github.com/repos/apache/spark/issues/34741/events,https://github.com/apache/spark/pull/34741,1065952187,PR_kwDOAQXtWs4vH6i7,34741,[SPARK-37463][SQL] Read/Write Timestamp ntz from/to Orc uses UTC time zone,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,29,2021-11-29T11:59:41Z,2021-12-28T08:04:39Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?
#33588 (comment) show Spark cannot read/write timestamp ntz and ltz correctly. Based on the discussion https://github.com/apache/spark/pull/34712#issuecomment-981402675, we just to fix read/write timestamp ntz to Orc uses UTC timestamp.

The root cause is Orc write/read timestamp with local timezone in default. The local timezone will be changed.
If the Orc writer write timestamp with local timezone(e.g. America/Los_Angeles), when the Orc reader reading the timestamp with other local timezone(e.g. Europe/Amsterdam), the value of timestamp will be different.

If we let the Orc writer write timestamp with UTC timezone, when the Orc reader reading the timestamp with UTC timezone too, the value of timestamp will be correct.

The related Orc source:
https://github.com/apache/orc/blob/3f1e57cf1cebe58027c1bd48c09eef4e9717a9e3/java/core/src/java/org/apache/orc/impl/WriterImpl.java#L525

https://github.com/apache/orc/blob/1f68ac0c7f2ae804b374500dcf1b4d7abe30ffeb/java/core/src/java/org/apache/orc/impl/TreeReaderFactory.java#L1184

### Why are the changes needed?
Fix the bug about read/write timestamp ntz from/to Orc with different times zone.


### Does this PR introduce _any_ user-facing change?
No. Orc timestamp ntz is a new feature not release yet.


### How was this patch tested?
New tests.
",https://api.github.com/repos/apache/spark/issues/34741/timeline,,spark,apache,beliefer,8486025,MDQ6VXNlcjg0ODYwMjU=,https://avatars.githubusercontent.com/u/8486025?v=4,,https://api.github.com/users/beliefer,https://github.com/beliefer,https://api.github.com/users/beliefer/followers,https://api.github.com/users/beliefer/following{/other_user},https://api.github.com/users/beliefer/gists{/gist_id},https://api.github.com/users/beliefer/starred{/owner}{/repo},https://api.github.com/users/beliefer/subscriptions,https://api.github.com/users/beliefer/orgs,https://api.github.com/users/beliefer/repos,https://api.github.com/users/beliefer/events{/privacy},https://api.github.com/users/beliefer/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34741,https://github.com/apache/spark/pull/34741,https://github.com/apache/spark/pull/34741.diff,https://github.com/apache/spark/pull/34741.patch,,https://api.github.com/repos/apache/spark/issues/34741/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
92,https://api.github.com/repos/apache/spark/issues/34731,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34731/labels{/name},https://api.github.com/repos/apache/spark/issues/34731/comments,https://api.github.com/repos/apache/spark/issues/34731/events,https://github.com/apache/spark/pull/34731,1065580403,PR_kwDOAQXtWs4vGtr-,34731,[SPARK-37153][PYTHON] Inline type hints for python/pyspark/profiler.py,"[{'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1982260031, 'node_id': 'MDU6TGFiZWwxOTgyMjYwMDMx', 'url': 'https://api.github.com/repos/apache/spark/labels/PYTHON', 'name': 'PYTHON', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,10,2021-11-29T04:00:10Z,2021-11-30T23:47:00Z,,CONTRIBUTOR,,False,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->Inline type hints for python/pyspark/profiler.py


### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->We can take advantage of static type checking within the functions by inlining the type hints.


### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->No


### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.
-->Existing tests
",https://api.github.com/repos/apache/spark/issues/34731/timeline,,spark,apache,dchvn,91461145,MDQ6VXNlcjkxNDYxMTQ1,https://avatars.githubusercontent.com/u/91461145?v=4,,https://api.github.com/users/dchvn,https://github.com/dchvn,https://api.github.com/users/dchvn/followers,https://api.github.com/users/dchvn/following{/other_user},https://api.github.com/users/dchvn/gists{/gist_id},https://api.github.com/users/dchvn/starred{/owner}{/repo},https://api.github.com/users/dchvn/subscriptions,https://api.github.com/users/dchvn/orgs,https://api.github.com/users/dchvn/repos,https://api.github.com/users/dchvn/events{/privacy},https://api.github.com/users/dchvn/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34731,https://github.com/apache/spark/pull/34731,https://github.com/apache/spark/pull/34731.diff,https://github.com/apache/spark/pull/34731.patch,,https://api.github.com/repos/apache/spark/issues/34731/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
93,https://api.github.com/repos/apache/spark/issues/34729,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34729/labels{/name},https://api.github.com/repos/apache/spark/issues/34729/comments,https://api.github.com/repos/apache/spark/issues/34729/events,https://github.com/apache/spark/pull/34729,1065132547,PR_kwDOAQXtWs4vFlnQ,34729,[SPARK-37475][SQL] Add scale parameter to floor and ceil functions,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,5,2021-11-27T22:31:29Z,2021-12-22T21:42:59Z,,CONTRIBUTOR,,False,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->
Adds `scale` parameter to `floor`/`ceil` functions in order to allow users to control the rounding position. This feature is proposed in the PR: https://github.com/apache/spark/pull/34593

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->
Currently we support Decimal RoundingModes : HALF_UP (round) and HALF_EVEN (bround). But we have use cases that needs RoundingMode.UP and RoundingMode.DOWN.

Floor and Ceil functions helps to do this but it doesn't support the position of the rounding. Adding scale parameter to the functions would help us control the rounding positions. 

Snowflake supports `scale` parameter to `floor`/`ceil` :
` FLOOR( <input_expr> [, <scale_expr> ] )`

REF:
https://docs.snowflake.com/en/sql-reference/functions/floor.html

### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->
Now users can pass `scale` parameter to the `floor` and `ceil` functions.
 ```
     > SELECT floor(-0.1);
       -1.0
      > SELECT floor(5);
       5
      > SELECT floor(3.1411, 3);
       3.141
      > SELECT floor(3.1411, -3);
       1000.0

      > SELECT ceil(-0.1);
       0.0
      > SELECT ceil(5);
       5
      > SELECT ceil(3.1411, 3);
       3.142
      > SELECT ceil(3.1411, -3);
       1000.0

```
### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.
-->
This patch was tested locally using unit test and git workflow.",https://api.github.com/repos/apache/spark/issues/34729/timeline,,spark,apache,sathiyapk,5880194,MDQ6VXNlcjU4ODAxOTQ=,https://avatars.githubusercontent.com/u/5880194?v=4,,https://api.github.com/users/sathiyapk,https://github.com/sathiyapk,https://api.github.com/users/sathiyapk/followers,https://api.github.com/users/sathiyapk/following{/other_user},https://api.github.com/users/sathiyapk/gists{/gist_id},https://api.github.com/users/sathiyapk/starred{/owner}{/repo},https://api.github.com/users/sathiyapk/subscriptions,https://api.github.com/users/sathiyapk/orgs,https://api.github.com/users/sathiyapk/repos,https://api.github.com/users/sathiyapk/events{/privacy},https://api.github.com/users/sathiyapk/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34729,https://github.com/apache/spark/pull/34729,https://github.com/apache/spark/pull/34729.diff,https://github.com/apache/spark/pull/34729.patch,,https://api.github.com/repos/apache/spark/issues/34729/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
94,https://api.github.com/repos/apache/spark/issues/34727,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34727/labels{/name},https://api.github.com/repos/apache/spark/issues/34727/comments,https://api.github.com/repos/apache/spark/issues/34727/events,https://github.com/apache/spark/pull/34727,1065066376,PR_kwDOAQXtWs4vFauv,34727,[SPARK-37467][SQL] Consolidate whole stage and non whole stage subexpression elimination,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,2,2021-11-27T15:25:13Z,2021-12-24T15:44:17Z,,CONTRIBUTOR,,False,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->
This PR consolidates the code paths for subexpression elimination in whole stage and non-whole stage codegen. Whole stage codegen seemed to be mostly a superset of the non-whole stage subexpression elimination, just with whole stage not using the codegen context to track subexpressions. Since subexpression values are replaced with empty blocks when evaluated, the context should be able to track the subexpressions across multiple operators. Not sure if there's corner cases I'm missing though.

It shouldn't result in any functionality changes, but there are slight differences in the generated code as a result of this:
- Subexpressions in whole stage always use mutable state for results instead of inlining results to support code splitting in non-whole stage
- Non-whole stage now supports the same inlining subexpressions if small enough as whole stage codegen
- Subexpressions are tracked across multiple physical operators in whole stage. They are still only calculated in each operator, but if you happen to have an expression in a later operator that was a subexpression in a previous operator, it will be used in the later operator.

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->
Currently, there are different code paths to handle subexpression elimination in whole stage and non-whole stage codegen. This makes it harder to add new capabilities to subexpression elimination having to deal with independent code paths.

### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->
No, just slight changes in generated code.

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.
-->
Existing unit tests.",https://api.github.com/repos/apache/spark/issues/34727/timeline,,spark,apache,Kimahriman,3536454,MDQ6VXNlcjM1MzY0NTQ=,https://avatars.githubusercontent.com/u/3536454?v=4,,https://api.github.com/users/Kimahriman,https://github.com/Kimahriman,https://api.github.com/users/Kimahriman/followers,https://api.github.com/users/Kimahriman/following{/other_user},https://api.github.com/users/Kimahriman/gists{/gist_id},https://api.github.com/users/Kimahriman/starred{/owner}{/repo},https://api.github.com/users/Kimahriman/subscriptions,https://api.github.com/users/Kimahriman/orgs,https://api.github.com/users/Kimahriman/repos,https://api.github.com/users/Kimahriman/events{/privacy},https://api.github.com/users/Kimahriman/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34727,https://github.com/apache/spark/pull/34727,https://github.com/apache/spark/pull/34727.diff,https://github.com/apache/spark/pull/34727.patch,,https://api.github.com/repos/apache/spark/issues/34727/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
95,https://api.github.com/repos/apache/spark/issues/34725,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34725/labels{/name},https://api.github.com/repos/apache/spark/issues/34725/comments,https://api.github.com/repos/apache/spark/issues/34725/events,https://github.com/apache/spark/pull/34725,1064925232,PR_kwDOAQXtWs4vFLrc,34725,[SPARK-37473][CORE] BypassMergeSortShuffleWriter may loss data when disk is missing however catagory is present,"[{'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,6,2021-11-27T07:11:55Z,2021-12-29T01:54:31Z,,NONE,,False,"### What changes were proposed in this pull request?
We think it has no data when the segment file not exists when all segment files produced by `BypassMergeSortShuffleWriter` is merging;

However, `file.exists()` may rerurn `false` when then the disk which segment file in on is missing and the root catagory exists; the missing disk only lead `file.exists()` return `false` but no exception. The task will run in peace without current segment file written.

The segment data will be ignored  and leading shuffle data loss.


### Why are the changes needed?
Shuffle may loss partition data leading corretness problems in final data.


### Does this PR introduce _any_ user-facing change?
No


### How was this patch tested?
 the task finished as normal without exception when I delete  a partition file before segment files merge， and the final data is incorrect;
The task will retry when apply this patch
",https://api.github.com/repos/apache/spark/issues/34725/timeline,,spark,apache,seayoun,45163307,MDQ6VXNlcjQ1MTYzMzA3,https://avatars.githubusercontent.com/u/45163307?v=4,,https://api.github.com/users/seayoun,https://github.com/seayoun,https://api.github.com/users/seayoun/followers,https://api.github.com/users/seayoun/following{/other_user},https://api.github.com/users/seayoun/gists{/gist_id},https://api.github.com/users/seayoun/starred{/owner}{/repo},https://api.github.com/users/seayoun/subscriptions,https://api.github.com/users/seayoun/orgs,https://api.github.com/users/seayoun/repos,https://api.github.com/users/seayoun/events{/privacy},https://api.github.com/users/seayoun/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34725,https://github.com/apache/spark/pull/34725,https://github.com/apache/spark/pull/34725.diff,https://github.com/apache/spark/pull/34725.patch,,https://api.github.com/repos/apache/spark/issues/34725/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
96,https://api.github.com/repos/apache/spark/issues/34719,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34719/labels{/name},https://api.github.com/repos/apache/spark/issues/34719/comments,https://api.github.com/repos/apache/spark/issues/34719/events,https://github.com/apache/spark/pull/34719,1064087353,PR_kwDOAQXtWs4vCxT7,34719,[SPARK-37381][SQL] Unify v1 and v2 SHOW CREATE TABLE tests,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,10,2021-11-26T04:11:13Z,2021-12-16T09:17:15Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?

1. Move the `SHOW CREATE TABLE` testcase from `DDLParserSuite.scala` to `ShowCreateTableParserSuite.scala`.
2. Move the `SHOW CREATE TABLE` testcase from `DataSourceV2SQLSuite.scala` to`v2.ShowCreateTableSuite` and extract some testcases to `ShowCreateTableBaseSuite`
3. Merge some testcases into one testcase in `sql/ShowCreateTableSuite.scala` and move it to `v1.ShowCreateTableSuite`
4. Move the testcase from `HiveShowCreateTableSuite` to `hive.ShowCreateTableSuite`
5. Extract some testcase from `v1.ShowCreateTableSuite` and `hive.ShowCreateTableSuite` to `ShowCreateTableBaseSuite`
6. Move the `SHOW CREATE TABLE` testcase from `CharVarcharTestSuite.scala` and `SQLViewSuite.scala` to `ShowCreateTableBaseSuite`
The changes follow the approach of [#30287](https://github.com/apache/spark/pull/30287) [#34305](https://github.com/apache/spark/pull/34305)

### Why are the changes needed?

1. The unification will allow to run common `SHOW CREATE TABLE` tests for both DSv1/Hive DSv1 and DSv2
2. We can detect missing features and differences between DSv1 and DSv2 implementations.

### Does this PR introduce _any_ user-facing change?
No

### How was this patch tested?
$  build/sbt -Phive-2.3 -Phive-thriftserver ""test:testOnly *ShowCreateTableSuite""

",https://api.github.com/repos/apache/spark/issues/34719/timeline,,spark,apache,Peng-Lei,41178002,MDQ6VXNlcjQxMTc4MDAy,https://avatars.githubusercontent.com/u/41178002?v=4,,https://api.github.com/users/Peng-Lei,https://github.com/Peng-Lei,https://api.github.com/users/Peng-Lei/followers,https://api.github.com/users/Peng-Lei/following{/other_user},https://api.github.com/users/Peng-Lei/gists{/gist_id},https://api.github.com/users/Peng-Lei/starred{/owner}{/repo},https://api.github.com/users/Peng-Lei/subscriptions,https://api.github.com/users/Peng-Lei/orgs,https://api.github.com/users/Peng-Lei/repos,https://api.github.com/users/Peng-Lei/events{/privacy},https://api.github.com/users/Peng-Lei/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34719,https://github.com/apache/spark/pull/34719,https://github.com/apache/spark/pull/34719.diff,https://github.com/apache/spark/pull/34719.patch,,https://api.github.com/repos/apache/spark/issues/34719/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
97,https://api.github.com/repos/apache/spark/issues/34709,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34709/labels{/name},https://api.github.com/repos/apache/spark/issues/34709/comments,https://api.github.com/repos/apache/spark/issues/34709/events,https://github.com/apache/spark/pull/34709,1063340697,PR_kwDOAQXtWs4vAeU6,34709,[WIP][SPARK-37259] Add option to unwrap query to support CTE for MSSQL JDBC driver,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,5,2021-11-25T09:25:23Z,2021-11-29T15:19:00Z,,NONE,,True,"### What changes were proposed in this pull request?
This PR adds the boolean option 'useRawQuery' to unwrap the query from 'select' statement, used to get schema, as it causes problems with CTE when using MSSQL JDBC driver. When set to 'true', the user has to also provide a schema of result in 'customSchema' option. The obvious downside of this approach is that the user is obligated to fetch schema beforehand when running the query. The advantage is that we are avoiding running query twice just to get schema, and users can run query without modification, unlike the solution described in https://github.com/apache/spark/pull/34693


### Why are the changes needed?
These changes are needed to support of CTE while using MSSQL JDBC


### Does this PR introduce _any_ user-facing change?
Added optiont 'useRawQuery' to JDBC options. Example:
```
JdbcMsSqlDF = (
    spark.read.format(""jdbc"")
    .option(""url"", f""jdbc:sqlserver://{server}:{port};databaseName={database};"")
    .option(""user"", user)
    .option(""password"", password)
    .option(""useRawQuery"", ""true"")  //<----------- Do not wrap the query
    .option(""customSchema"", schema)
    .option(""driver"", ""com.microsoft.sqlserver.jdbc.SQLServerDriver"")
    .option(""query"", query)
    .load()
)
```


### How was this patch tested?
The patch was tested manually, unit tests  are pending, it's still WIP.
",https://api.github.com/repos/apache/spark/issues/34709/timeline,,spark,apache,akhalymon-cv,30291478,MDQ6VXNlcjMwMjkxNDc4,https://avatars.githubusercontent.com/u/30291478?v=4,,https://api.github.com/users/akhalymon-cv,https://github.com/akhalymon-cv,https://api.github.com/users/akhalymon-cv/followers,https://api.github.com/users/akhalymon-cv/following{/other_user},https://api.github.com/users/akhalymon-cv/gists{/gist_id},https://api.github.com/users/akhalymon-cv/starred{/owner}{/repo},https://api.github.com/users/akhalymon-cv/subscriptions,https://api.github.com/users/akhalymon-cv/orgs,https://api.github.com/users/akhalymon-cv/repos,https://api.github.com/users/akhalymon-cv/events{/privacy},https://api.github.com/users/akhalymon-cv/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34709,https://github.com/apache/spark/pull/34709,https://github.com/apache/spark/pull/34709.diff,https://github.com/apache/spark/pull/34709.patch,,https://api.github.com/repos/apache/spark/issues/34709/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
98,https://api.github.com/repos/apache/spark/issues/34695,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34695/labels{/name},https://api.github.com/repos/apache/spark/issues/34695/comments,https://api.github.com/repos/apache/spark/issues/34695/events,https://github.com/apache/spark/pull/34695,1062019180,PR_kwDOAQXtWs4u8eC6,34695,[SPARK-32446][CORE] Add percentile distribution REST API & UI of peak memory metrics for all executors ,"[{'id': 1406605079, 'node_id': 'MDU6TGFiZWwxNDA2NjA1MDc5', 'url': 'https://api.github.com/repos/apache/spark/labels/WEB%20UI', 'name': 'WEB UI', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981471081, 'node_id': 'MDU6TGFiZWwxOTgxNDcxMDgx', 'url': 'https://api.github.com/repos/apache/spark/labels/DOCS', 'name': 'DOCS', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,49,2021-11-24T05:28:51Z,2021-12-21T10:02:08Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?
This pr continue the work of https://github.com/apache/spark/pull/29247 since origin author didn't reply for a long time.
Will add as co-author.

For the whole process of application, user may want to know each executor's peak memory usage to see the Resource utilization. The distribution of all executor's peak memory metrics usage  can help users know whether or not there is a skew/bottleneck among executor resource utilization in a given stage.

We define `activeOnly` and `quantiles` query parameter in the REST API for all executors peak memory metrics distribution:
```
applications/<application_id>/<application_attempt/executorPeakMemoryMetricsDistribution?activeOnly=[true (default) | false]&quantiles=0.05,0.25,0.5,0.75,0.95
```
1. withSummaries: default is false, define whether to show current stage's taskMetricsDistribution and executorMetricsDistribution
2. quantiles: default is `0.0,0.25,0.5,0.75,1.0` only effect when `withSummaries=true`, it define the quantiles we use when calculating metrics distributions.

When withSummaries=true, both task metrics in percentile distribution and executor metrics in percentile distribution are included in the REST API output.  The default value of withSummaries is false, i.e. no metrics percentile distribution will be included in the REST API output.

 

### Why are the changes needed?
Always user care about  executor peak usage distribution, this pr help users understand executor peak memory metrics distributions.


### Does this PR introduce _any_ user-facing change?
User can use below restful API to get all executor's peak memory metrics distribution:
```
applications/<application_id>/<application_attempt>/executorPeakMemoryMetricsDistribution
```

### How was this patch tested?
![image](https://user-images.githubusercontent.com/46485123/145008985-cfecba4d-f08f-4d5d-9a2d-1ce184eb1253.png)
",https://api.github.com/repos/apache/spark/issues/34695/timeline,,spark,apache,AngersZhuuuu,46485123,MDQ6VXNlcjQ2NDg1MTIz,https://avatars.githubusercontent.com/u/46485123?v=4,,https://api.github.com/users/AngersZhuuuu,https://github.com/AngersZhuuuu,https://api.github.com/users/AngersZhuuuu/followers,https://api.github.com/users/AngersZhuuuu/following{/other_user},https://api.github.com/users/AngersZhuuuu/gists{/gist_id},https://api.github.com/users/AngersZhuuuu/starred{/owner}{/repo},https://api.github.com/users/AngersZhuuuu/subscriptions,https://api.github.com/users/AngersZhuuuu/orgs,https://api.github.com/users/AngersZhuuuu/repos,https://api.github.com/users/AngersZhuuuu/events{/privacy},https://api.github.com/users/AngersZhuuuu/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34695,https://github.com/apache/spark/pull/34695,https://github.com/apache/spark/pull/34695.diff,https://github.com/apache/spark/pull/34695.patch,,https://api.github.com/repos/apache/spark/issues/34695/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
99,https://api.github.com/repos/apache/spark/issues/34693,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34693/labels{/name},https://api.github.com/repos/apache/spark/issues/34693/comments,https://api.github.com/repos/apache/spark/issues/34693/events,https://github.com/apache/spark/pull/34693,1061197484,PR_kwDOAQXtWs4u51Ij,34693,[SPARK-37259][SQL] Support CTE and TempTable queries with MSSQL JDBC,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,20,2021-11-23T12:09:33Z,2021-12-02T18:22:03Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?
Currently CTE queries from Spark are not supported with MSSQL server via JDBC. This is because MSSQL server doesn't support the nested CTE syntax that Spark builds from the original query (`options.tableOrQuery`) in `JDBCRDD.resolveTable()` and in `JDBCRDD.compute()`.
Unfortunately, it is non-trivial to split an arbitrary query it into ""with"" and ""regular"" clauses in `MsSqlServerDialect`. So instead, I'm proposing a new general JDBC option ""withClause"" that users can use if they have complex queries with CTE:
```
val withClause = ""WITH t AS (SELECT x, y FROM tbl)""
val query = ""SELECT * FROM t WHERE x > 10""
val df = spark.read.format(""jdbc"")
  .option(""url"", jdbcUrl)
  .option(""withClause"", withClause)
  .option(""query"", query)
  .load()
```
This change also works with MSSQL's temp table syntax:
```
val withClause = ""(SELECT * INTO #TempTable FROM (SELECT * FROM tbl WHERE x > 10) t)""
val query = ""SELECT * FROM #TempTable""
val df = spark.read.format(""jdbc"")
  .option(""url"", jdbcUrl)
  .option(""withClause"", withClause)
  .option(""query"", query)
  .load()
```

### Why are the changes needed?
To support CTE queries with MSSQL.

### Does this PR introduce _any_ user-facing change?
Yes, CTE queries are supported form now.

### How was this patch tested?
Added new integration UTs.
",https://api.github.com/repos/apache/spark/issues/34693/timeline,,spark,apache,peter-toth,7253827,MDQ6VXNlcjcyNTM4Mjc=,https://avatars.githubusercontent.com/u/7253827?v=4,,https://api.github.com/users/peter-toth,https://github.com/peter-toth,https://api.github.com/users/peter-toth/followers,https://api.github.com/users/peter-toth/following{/other_user},https://api.github.com/users/peter-toth/gists{/gist_id},https://api.github.com/users/peter-toth/starred{/owner}{/repo},https://api.github.com/users/peter-toth/subscriptions,https://api.github.com/users/peter-toth/orgs,https://api.github.com/users/peter-toth/repos,https://api.github.com/users/peter-toth/events{/privacy},https://api.github.com/users/peter-toth/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34693,https://github.com/apache/spark/pull/34693,https://github.com/apache/spark/pull/34693.diff,https://github.com/apache/spark/pull/34693.patch,,https://api.github.com/repos/apache/spark/issues/34693/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
100,https://api.github.com/repos/apache/spark/issues/34683,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34683/labels{/name},https://api.github.com/repos/apache/spark/issues/34683/comments,https://api.github.com/repos/apache/spark/issues/34683/events,https://github.com/apache/spark/pull/34683,1060204926,PR_kwDOAQXtWs4u2szv,34683,[SPARK-37283][SQL][FOLLOWUP] Avoid trying to store a table which contains timestamp_ntz types in Hive compatible format,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,6,2021-11-22T14:12:12Z,2021-11-24T04:43:36Z,,MEMBER,,False,"### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->
This is PR is to avoid trying to store a table which contains `timestamp_ntz` types in Hive compatible format.
In the current master, Spark tries to store such a table in Hive compatible format first, but it doesn't support `timestamp_ntz`.
As a result warning is logged like as follows though it's finally stored in Spark specific format.
```
CREATE TABLE tbl1(a TIMESTAMP_NTZ) USING Parquet
...
21/11/22 21:57:18 WARN HiveExternalCatalog: Could not persist `default`.`tbl1` in a Hive compatible way. Persisting it into Hive metastore in Spark SQL specific format.
org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.IllegalArgumentException: Error: type expected at the position 0 of 'timestamp_ntz' but 'timestamp_ntz' is found.
        at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:869)
        at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:874)
        at org.apache.spark.sql.hive.client.Shim_v0_12.createTable(HiveShim.scala:614)
        at org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$createTable$1(HiveClientImpl.scala:554)
        at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
        at org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$withHiveState$1(HiveClientImpl.scala:304)
        at org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:235)
        at org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:234)
        at org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:284)
        at org.apache.spark.sql.hive.client.HiveClientImpl.createTable(HiveClientImpl.scala:552)
        at org.apache.spark.sql.hive.HiveExternalCatalog.saveTableIntoHive(HiveExternalCatalog.scala:506)
        at org.apache.spark.sql.hive.HiveExternalCatalog.createDataSourceTable(HiveExternalCatalog.scala:404)
        at org.apache.spark.sql.hive.HiveExternalCatalog.$anonfun$createTable$1(HiveExternalCatalog.scala:273)
...
```
### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->
To fix the confusing behavior.

### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->
No.

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.
-->
Modified the test added in #34551",https://api.github.com/repos/apache/spark/issues/34683/timeline,,spark,apache,sarutak,4736016,MDQ6VXNlcjQ3MzYwMTY=,https://avatars.githubusercontent.com/u/4736016?v=4,,https://api.github.com/users/sarutak,https://github.com/sarutak,https://api.github.com/users/sarutak/followers,https://api.github.com/users/sarutak/following{/other_user},https://api.github.com/users/sarutak/gists{/gist_id},https://api.github.com/users/sarutak/starred{/owner}{/repo},https://api.github.com/users/sarutak/subscriptions,https://api.github.com/users/sarutak/orgs,https://api.github.com/users/sarutak/repos,https://api.github.com/users/sarutak/events{/privacy},https://api.github.com/users/sarutak/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34683,https://github.com/apache/spark/pull/34683,https://github.com/apache/spark/pull/34683.diff,https://github.com/apache/spark/pull/34683.patch,,https://api.github.com/repos/apache/spark/issues/34683/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
101,https://api.github.com/repos/apache/spark/issues/34680,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34680/labels{/name},https://api.github.com/repos/apache/spark/issues/34680/comments,https://api.github.com/repos/apache/spark/issues/34680/events,https://github.com/apache/spark/pull/34680,1059772347,PR_kwDOAQXtWs4u1SIC,34680,[SPARK-37421][PYTHON] Inline type hints for python/pyspark/mllib/evaluation.py,"[{'id': 1405803268, 'node_id': 'MDU6TGFiZWwxNDA1ODAzMjY4', 'url': 'https://api.github.com/repos/apache/spark/labels/MLLIB', 'name': 'MLLIB', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1982260031, 'node_id': 'MDU6TGFiZWwxOTgyMjYwMDMx', 'url': 'https://api.github.com/repos/apache/spark/labels/PYTHON', 'name': 'PYTHON', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,5,2021-11-22T07:09:51Z,2021-12-30T07:00:40Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?
Inline type hints for evaluation.py in python/pyspark/mllib/
### Why are the changes needed?
We can take advantage of static type checking within the functions by inlining the type hints.
### Does this PR introduce _any_ user-facing change?
No
### How was this patch tested?
Existing tests",https://api.github.com/repos/apache/spark/issues/34680/timeline,,spark,apache,dchvn,91461145,MDQ6VXNlcjkxNDYxMTQ1,https://avatars.githubusercontent.com/u/91461145?v=4,,https://api.github.com/users/dchvn,https://github.com/dchvn,https://api.github.com/users/dchvn/followers,https://api.github.com/users/dchvn/following{/other_user},https://api.github.com/users/dchvn/gists{/gist_id},https://api.github.com/users/dchvn/starred{/owner}{/repo},https://api.github.com/users/dchvn/subscriptions,https://api.github.com/users/dchvn/orgs,https://api.github.com/users/dchvn/repos,https://api.github.com/users/dchvn/events{/privacy},https://api.github.com/users/dchvn/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34680,https://github.com/apache/spark/pull/34680,https://github.com/apache/spark/pull/34680.diff,https://github.com/apache/spark/pull/34680.patch,,https://api.github.com/repos/apache/spark/issues/34680/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
102,https://api.github.com/repos/apache/spark/issues/34675,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34675/labels{/name},https://api.github.com/repos/apache/spark/issues/34675/comments,https://api.github.com/repos/apache/spark/issues/34675/events,https://github.com/apache/spark/pull/34675,1059522641,PR_kwDOAQXtWs4u0cwy,34675,[SPARK-37433][SQL] Uses TimeZone.getDefault when timeZoneId is None for ZoneAwareExpression,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,1,2021-11-21T23:36:59Z,2021-11-29T20:42:58Z,,CONTRIBUTOR,,False,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->
Calling `timeZoneId.get` on Option[String] leads to `NoSuchElementException: None.get`. This PR matches the value of Option[String] and uses `TimeZone.getDefault.toZoneId` when zoneId is None, this avoid unexpected exceptions to users. 

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->
Calling `.get` on Option variable never been a good idea. We can either use a default value or choose to throw a meaningful exception. In this case, TimeZone.getDefault will be a good fit for a default value.

### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->
No

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.
-->
Tested Locally and via Unit Test.",https://api.github.com/repos/apache/spark/issues/34675/timeline,,spark,apache,sathiyapk,5880194,MDQ6VXNlcjU4ODAxOTQ=,https://avatars.githubusercontent.com/u/5880194?v=4,,https://api.github.com/users/sathiyapk,https://github.com/sathiyapk,https://api.github.com/users/sathiyapk/followers,https://api.github.com/users/sathiyapk/following{/other_user},https://api.github.com/users/sathiyapk/gists{/gist_id},https://api.github.com/users/sathiyapk/starred{/owner}{/repo},https://api.github.com/users/sathiyapk/subscriptions,https://api.github.com/users/sathiyapk/orgs,https://api.github.com/users/sathiyapk/repos,https://api.github.com/users/sathiyapk/events{/privacy},https://api.github.com/users/sathiyapk/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34675,https://github.com/apache/spark/pull/34675,https://github.com/apache/spark/pull/34675.diff,https://github.com/apache/spark/pull/34675.patch,,https://api.github.com/repos/apache/spark/issues/34675/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
103,https://api.github.com/repos/apache/spark/issues/34674,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34674/labels{/name},https://api.github.com/repos/apache/spark/issues/34674/comments,https://api.github.com/repos/apache/spark/issues/34674/events,https://github.com/apache/spark/pull/34674,1059140193,PR_kwDOAQXtWs4uzVgD,34674,[SPARK-37419][PYTHON][ML] Rewrite _shared_params_code_gen.py  to inline type hints for ml/param/shared.py,"[{'id': 1405801475, 'node_id': 'MDU6TGFiZWwxNDA1ODAxNDc1', 'url': 'https://api.github.com/repos/apache/spark/labels/ML', 'name': 'ML', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1982260031, 'node_id': 'MDU6TGFiZWwxOTgyMjYwMDMx', 'url': 'https://api.github.com/repos/apache/spark/labels/PYTHON', 'name': 'PYTHON', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,9,2021-11-20T13:00:31Z,2021-12-19T07:16:57Z,,MEMBER,,False,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->

This PR modifies `_shared_params_code_gen.py ` to include type hints in generated code.

Additionally, it applies minor cleanup:

- Switch from manual replace to string interpolation.
- Added magic commas for consistent formatting of the `shared` list.
- Dropped unused arguments from `_gen_param_code`
- Added to type hints to `_gen_param_header` and `_gen_param_code` and dropped `_shared_params_code_gen.pyi` stub.


### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->

Core change is part of the bigger effort to migrate hints from stub files to corresponding Python modules

> Currently, we use stub files for type annotations, which don't support type checks within function bodies. So we inline type hints to support that.


### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->

No


### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.
-->

- Existing tests.
- Manual execution of `_shared_params_code_gen.py ` and  checking for regressions by comparing output with  the current version of `shared.py`
",https://api.github.com/repos/apache/spark/issues/34674/timeline,,spark,apache,zero323,1554276,MDQ6VXNlcjE1NTQyNzY=,https://avatars.githubusercontent.com/u/1554276?v=4,,https://api.github.com/users/zero323,https://github.com/zero323,https://api.github.com/users/zero323/followers,https://api.github.com/users/zero323/following{/other_user},https://api.github.com/users/zero323/gists{/gist_id},https://api.github.com/users/zero323/starred{/owner}{/repo},https://api.github.com/users/zero323/subscriptions,https://api.github.com/users/zero323/orgs,https://api.github.com/users/zero323/repos,https://api.github.com/users/zero323/events{/privacy},https://api.github.com/users/zero323/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34674,https://github.com/apache/spark/pull/34674,https://github.com/apache/spark/pull/34674.diff,https://github.com/apache/spark/pull/34674.patch,,https://api.github.com/repos/apache/spark/issues/34674/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
104,https://api.github.com/repos/apache/spark/issues/34672,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34672/labels{/name},https://api.github.com/repos/apache/spark/issues/34672/comments,https://api.github.com/repos/apache/spark/issues/34672/events,https://github.com/apache/spark/pull/34672,1058983769,PR_kwDOAQXtWs4uy3FB,34672,[SPARK-37394][CORE] Skip registering with ESS if a customized shuffle manager is configured,"[{'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,13,2021-11-19T23:06:57Z,2021-12-28T18:23:11Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?
Propose to skip registering with ESS if a customized shuffle manager (Remote Shuffle Service) is configured. Otherwise, when the dynamic allocation is enabled without an external shuffle service in place, the Spark executor still tries to connect to the external shuffle service which gets to a connection refused exception.


### Why are the changes needed?
To get dynamic allocation works with a 3rd party remote shuffle service.


### Does this PR introduce _any_ user-facing change?
No


### How was this patch tested?
Test locally on K8s with docker-desktop, DA enabled, no external shuffle service, running Uber's RSS locally.

With the default setting, when I run spark job and it will fail with the following error:

> 21/11/30 07:35:24 INFO BlockManager: Registering executor with local external shuffle service.                                                                                              
21/11/30 07:35:24 ERROR BlockManager: Failed to connect to external shuffle server, will retry 2 more times after waiting 5 seconds...
java.io.IOException: Failed to connect to /10.1.2.201:7337

Then apply this patch to Spark, rebuild Spark, and make the changes like the following in the ShuffleManager implementation, i.e `RssShuffleManager.scala`

```
diff --git a/src/main/scala/org/apache/spark/shuffle/RssShuffleManager.scala b/src/main/scala/org/apache/spark/shuffle/RssShuffleManager.scala
index 4b6e825..aeffd7d 100644
--- a/src/main/scala/org/apache/spark/shuffle/RssShuffleManager.scala
+++ b/src/main/scala/org/apache/spark/shuffle/RssShuffleManager.scala
@@ -442,5 +442,8 @@ class RssShuffleManager(conf: SparkConf) extends ShuffleManager with Logging {
       }
     new ServerConnectionCacheUpdateRefresher(serverConnectionResolver, MultiServerHeartbeatClient.getServerCache)
   }
+
+  override def supportExternalShuffleService: Boolean = false
+
```
restart RSS, and rerun the Spark job, the same job can be successfully completed.
",https://api.github.com/repos/apache/spark/issues/34672/timeline,,spark,apache,yangwwei,14214570,MDQ6VXNlcjE0MjE0NTcw,https://avatars.githubusercontent.com/u/14214570?v=4,,https://api.github.com/users/yangwwei,https://github.com/yangwwei,https://api.github.com/users/yangwwei/followers,https://api.github.com/users/yangwwei/following{/other_user},https://api.github.com/users/yangwwei/gists{/gist_id},https://api.github.com/users/yangwwei/starred{/owner}{/repo},https://api.github.com/users/yangwwei/subscriptions,https://api.github.com/users/yangwwei/orgs,https://api.github.com/users/yangwwei/repos,https://api.github.com/users/yangwwei/events{/privacy},https://api.github.com/users/yangwwei/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34672,https://github.com/apache/spark/pull/34672,https://github.com/apache/spark/pull/34672.diff,https://github.com/apache/spark/pull/34672.patch,,https://api.github.com/repos/apache/spark/issues/34672/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
105,https://api.github.com/repos/apache/spark/issues/34665,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34665/labels{/name},https://api.github.com/repos/apache/spark/issues/34665/comments,https://api.github.com/repos/apache/spark/issues/34665/events,https://github.com/apache/spark/pull/34665,1058208995,PR_kwDOAQXtWs4uwXRW,34665,[SPARK-37383][SQL]Print the parsing time for each phase of a SQL,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,1,2021-11-19T07:05:27Z,2021-11-19T10:43:31Z,,NONE,,False,"### What changes were proposed in this pull request?
The PR is proposed to:
print out the timeSpent for each phase of a SQL and the total timeSpent of the SQL for parsing

### Why are the changes needed?
the time spent for each parsing phase of a SQL is counted and recorded in QueryPlanningTracker ,  but it is not yet shown anywhere.
when SQL parsing is suspected to be slow, we cannot confirm which phase is slow，therefore, it is necessary to print out the SQL parsing time.

### Does this PR introduce _any_ user-facing change?
No

### How was this patch tested?
Existing tests.
",https://api.github.com/repos/apache/spark/issues/34665/timeline,,spark,apache,caican00,94670132,U_kgDOBaSNNA,https://avatars.githubusercontent.com/u/94670132?v=4,,https://api.github.com/users/caican00,https://github.com/caican00,https://api.github.com/users/caican00/followers,https://api.github.com/users/caican00/following{/other_user},https://api.github.com/users/caican00/gists{/gist_id},https://api.github.com/users/caican00/starred{/owner}{/repo},https://api.github.com/users/caican00/subscriptions,https://api.github.com/users/caican00/orgs,https://api.github.com/users/caican00/repos,https://api.github.com/users/caican00/events{/privacy},https://api.github.com/users/caican00/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34665,https://github.com/apache/spark/pull/34665,https://github.com/apache/spark/pull/34665.diff,https://github.com/apache/spark/pull/34665.patch,,https://api.github.com/repos/apache/spark/issues/34665/reactions,1,1,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
106,https://api.github.com/repos/apache/spark/issues/34659,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34659/labels{/name},https://api.github.com/repos/apache/spark/issues/34659/comments,https://api.github.com/repos/apache/spark/issues/34659/events,https://github.com/apache/spark/pull/34659,1058061822,PR_kwDOAQXtWs4uv4ZW,34659,[SPARK-34863][SQL] Support complex types for Parquet vectorized reader,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,20,2021-11-19T02:30:35Z,2021-12-23T17:16:48Z,,CONTRIBUTOR,,False,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->

This PR adds support for complex types (e.g., list, map, array) for Spark's vectorized Parquet reader. In particular, this introduces the following changes:
1. Added a new class `ParquetColumnVector` which encapsulates all the necessary information needed when reading a Parquet column, including the `ParquetColumn` for the Parquet column, the repetition & definition levels (only allocated for a leaf-node of a complex type), as well as the reader for the column. In addition, it also contains logic for assembling nested columnar batches, via interpreting Parquet repetition & definition levels. 
2. Changes are made in `VectorizedParquetRecordReader` to initialize a list of `ParquetColumnVector` for the columns read.
3. `VectorizedColumnReader` now also creates a reader for repetition column. Depending on whether maximum repetition level is 0, the batch read is now split into two code paths, e.g., `readBatch` versus `readBatchNested`.
4. Added logic to handle complex type in `VectorizedRleValuesReader`. For data types involving only struct or primitive types, it still goes with the old `readBatch` method which now also saves definition levels into a vector for later assembly. Otherwise, for data types involving array or map, a separate code path `readBatchNested` is introduced to handle repetition levels.
5. Modified `WritableColumnVector` to better support null structs. Currently it requires populating null entries to all child vectors when there is a null struct, however this will waste space and also doesn't work well with Parquet scan. This adds an extra field `structOffsets` which records the mapping from a row ID to the position of the row in the child vector, so that child vectors will only need to store real null elements.
This PR also introduced a new flag `spark.sql.parquet.enableNestedColumnVectorizedReader` which turns the feature on or off. By default it is on to facilitates all the Parquet related test coverage.

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->

Whenever read schema containing complex types, at the moment Spark will fallback to the row-based reader in parquet-mr, which is much slower. As benchmark shows, by adding support into the vectorized reader, we can get ~15x on average speed up on reading struct fields, and ~1.5x when reading array of struct and map.

### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->

With the PR Spark should now support reading complex types in its vectorized Parquet reader. A new config `spark.sql.parquet.enableNestedColumnVectorizedReader` is introduced to turn the feature on or off.

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->

Added new unit tests.",https://api.github.com/repos/apache/spark/issues/34659/timeline,,spark,apache,sunchao,506679,MDQ6VXNlcjUwNjY3OQ==,https://avatars.githubusercontent.com/u/506679?v=4,,https://api.github.com/users/sunchao,https://github.com/sunchao,https://api.github.com/users/sunchao/followers,https://api.github.com/users/sunchao/following{/other_user},https://api.github.com/users/sunchao/gists{/gist_id},https://api.github.com/users/sunchao/starred{/owner}{/repo},https://api.github.com/users/sunchao/subscriptions,https://api.github.com/users/sunchao/orgs,https://api.github.com/users/sunchao/repos,https://api.github.com/users/sunchao/events{/privacy},https://api.github.com/users/sunchao/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34659,https://github.com/apache/spark/pull/34659,https://github.com/apache/spark/pull/34659.diff,https://github.com/apache/spark/pull/34659.patch,,https://api.github.com/repos/apache/spark/issues/34659/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
107,https://api.github.com/repos/apache/spark/issues/34650,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34650/labels{/name},https://api.github.com/repos/apache/spark/issues/34650/comments,https://api.github.com/repos/apache/spark/issues/34650/events,https://github.com/apache/spark/pull/34650,1057628792,PR_kwDOAQXtWs4uuodp,34650,[WIP][SPARK-36664][CORE] Log time waiting for cluster resources,"[{'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,12,2021-11-18T17:43:10Z,2021-12-14T00:15:11Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?

Keep track of and communicate to the listener bus how long we are waiting for execs to be allocated from the underlying cluster manager.


### Why are the changes needed?

Sometimes the cluster manager may choke or otherwise not be able to allocate resources and we don't have a good way of detecting this situation making it difficult for the user to debug and tell apart from Spark not scaling up correctly.

### Does this PR introduce _any_ user-facing change?

New field in the listener bus message for when a executor is allocated.

### How was this patch tested?

New unit test in the listener suite.",https://api.github.com/repos/apache/spark/issues/34650/timeline,,spark,apache,holdenk,59893,MDQ6VXNlcjU5ODkz,https://avatars.githubusercontent.com/u/59893?v=4,,https://api.github.com/users/holdenk,https://github.com/holdenk,https://api.github.com/users/holdenk/followers,https://api.github.com/users/holdenk/following{/other_user},https://api.github.com/users/holdenk/gists{/gist_id},https://api.github.com/users/holdenk/starred{/owner}{/repo},https://api.github.com/users/holdenk/subscriptions,https://api.github.com/users/holdenk/orgs,https://api.github.com/users/holdenk/repos,https://api.github.com/users/holdenk/events{/privacy},https://api.github.com/users/holdenk/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34650,https://github.com/apache/spark/pull/34650,https://github.com/apache/spark/pull/34650.diff,https://github.com/apache/spark/pull/34650.patch,,https://api.github.com/repos/apache/spark/issues/34650/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
108,https://api.github.com/repos/apache/spark/issues/34647,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34647/labels{/name},https://api.github.com/repos/apache/spark/issues/34647/comments,https://api.github.com/repos/apache/spark/issues/34647/events,https://github.com/apache/spark/pull/34647,1057351837,PR_kwDOAQXtWs4utujo,34647,[SPARK-36180][SQL] Support TimestampNTZ type in Hive,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,15,2021-11-18T13:22:12Z,2021-11-23T07:07:09Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?
Spark not supports TimestampNTZ type in Hive.

```
[info] Caused by: java.lang.IllegalArgumentException: Error: type expected at the position 0 of 'timestamp_ntz:timestamp' but 'timestamp_ntz' is found.[info] Caused by: java.lang.IllegalArgumentException: Error: type expected at the position 0 of 'timestamp_ntz:timestamp' but 'timestamp_ntz' is found.
[info]  at org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils$TypeInfoParser.expect(TypeInfoUtils.java:372)
[info]  at org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils$TypeInfoParser.expect(TypeInfoUtils.java:355)
[info]  at org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils$TypeInfoParser.parseType(TypeInfoUtils.java:416)
[info]  at org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils$TypeInfoParser.parseTypeInfos(TypeInfoUtils.java:329)
[info]  at org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils.getTypeInfosFromTypeString(TypeInfoUtils.java:814)
[info]  at org.apache.hadoop.hive.serde2.lazy.LazySerDeParameters.extractColumnInfo(LazySerDeParameters.java:162)[info]  at org.apache.hadoop.hive.serde2.lazy.LazySerDeParameters.<init>(LazySerDeParameters.java:91)
[info]  at org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.initialize(LazySimpleSerDe.java:116)
[info]  at org.apache.hadoop.hive.serde2.AbstractSerDe.initialize(AbstractSerDe.java:54)
[info]  at org.apache.hadoop.hive.serde2.SerDeUtils.initializeSerDe(SerDeUtils.java:533)
[info]  at org.apache.hadoop.hive.metastore.MetaStoreUtils.getDeserializer(MetaStoreUtils.java:453)
[info]  at org.apache.hadoop.hive.metastore.MetaStoreUtils.getDeserializer(MetaStoreUtils.java:440)
[info]  at org.apache.hadoop.hive.ql.metadata.Table.getDeserializerFromMetaStore(Table.java:281)
[info]  at org.apache.hadoop.hive.ql.metadata.Table.checkValidity(Table.java:199)
[info]  at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:842)
...
```

Hive only providers the timestamp type, so Spark should write both timestamp_ltz and timestamp_ntz as Hive' timestamp.
When Spark read schema form Hive, We should restore the timestamp_ntz type.

### Why are the changes needed?
The hive 2.3.9 does not have 2 timestamp or a type named timestamp_ntz.
FYI, In hive 3.0, the will be a timestamp with local timezone added.


### Does this PR introduce _any_ user-facing change?
'No'. timestamp_ntz is new and not public yet


### How was this patch tested?
New test
",https://api.github.com/repos/apache/spark/issues/34647/timeline,,spark,apache,beliefer,8486025,MDQ6VXNlcjg0ODYwMjU=,https://avatars.githubusercontent.com/u/8486025?v=4,,https://api.github.com/users/beliefer,https://github.com/beliefer,https://api.github.com/users/beliefer/followers,https://api.github.com/users/beliefer/following{/other_user},https://api.github.com/users/beliefer/gists{/gist_id},https://api.github.com/users/beliefer/starred{/owner}{/repo},https://api.github.com/users/beliefer/subscriptions,https://api.github.com/users/beliefer/orgs,https://api.github.com/users/beliefer/repos,https://api.github.com/users/beliefer/events{/privacy},https://api.github.com/users/beliefer/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34647,https://github.com/apache/spark/pull/34647,https://github.com/apache/spark/pull/34647.diff,https://github.com/apache/spark/pull/34647.patch,,https://api.github.com/repos/apache/spark/issues/34647/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
109,https://api.github.com/repos/apache/spark/issues/34646,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34646/labels{/name},https://api.github.com/repos/apache/spark/issues/34646/comments,https://api.github.com/repos/apache/spark/issues/34646/events,https://github.com/apache/spark/pull/34646,1057323406,PR_kwDOAQXtWs4utofT,34646,[SPARK-37372][K8S] Removing redundant label addition and refactoring related test case,"[{'id': 1406605057, 'node_id': 'MDU6TGFiZWwxNDA2NjA1MDU3', 'url': 'https://api.github.com/repos/apache/spark/labels/KUBERNETES', 'name': 'KUBERNETES', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,6,2021-11-18T12:53:57Z,2021-12-10T08:49:02Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?
1. Remove redundant Pod label addtions in driver and executor :

https://github.com/apache/spark/blob/3d0663c45f900332ff1ea826f45cbf2c79eba3cd/resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/features/BasicDriverFeatureStep.scala#L144-L145

https://github.com/apache/spark/blob/3d0663c45f900332ff1ea826f45cbf2c79eba3cd/resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/features/BasicExecutorFeatureStep.scala#L278-L283

also add a check assert(metadata.getLabels === conf.labels.asJava) to make sure metadata be set correctly

2. Rename DRIVER_LABELS to CUSTOM_DRIVER_LABELS, LABELS  to CUSTOM_LABELS, make their names more clear.

Related: 
- https://github.com/apache/spark/pull/34460
- https://github.com/apache/spark/pull/33508

### Why are the changes needed?
These labels are already included by conf.labels as preset labels, we don't need do a extra addition.

### Does this PR introduce _any_ user-facing change?
NO


### How was this patch tested?
UT
",https://api.github.com/repos/apache/spark/issues/34646/timeline,,spark,apache,Yikun,1736354,MDQ6VXNlcjE3MzYzNTQ=,https://avatars.githubusercontent.com/u/1736354?v=4,,https://api.github.com/users/Yikun,https://github.com/Yikun,https://api.github.com/users/Yikun/followers,https://api.github.com/users/Yikun/following{/other_user},https://api.github.com/users/Yikun/gists{/gist_id},https://api.github.com/users/Yikun/starred{/owner}{/repo},https://api.github.com/users/Yikun/subscriptions,https://api.github.com/users/Yikun/orgs,https://api.github.com/users/Yikun/repos,https://api.github.com/users/Yikun/events{/privacy},https://api.github.com/users/Yikun/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34646,https://github.com/apache/spark/pull/34646,https://github.com/apache/spark/pull/34646.diff,https://github.com/apache/spark/pull/34646.patch,,https://api.github.com/repos/apache/spark/issues/34646/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
110,https://api.github.com/repos/apache/spark/issues/34640,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34640/labels{/name},https://api.github.com/repos/apache/spark/issues/34640/comments,https://api.github.com/repos/apache/spark/issues/34640/events,https://github.com/apache/spark/pull/34640,1056864144,PR_kwDOAQXtWs4usHyb,34640,[SPARK-31585][SQL] Introduce Z-order expression,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,9,2021-11-18T02:27:33Z,2021-12-01T06:44:58Z,,CONTRIBUTOR,,False,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->
This PR is to introduce a new expression in Spark - `ZOrder`. The motivation is Z-order enables to sort tuples in a way, to allow efficiently data skipping for columnar file format (Parquet and ORC).

For query with filter on combination of multiple columns, example:

```sql
SELECT *
FROM table
WHERE x = 0 OR y = 0
```

Parquet/ORC cannot skip file/row-groups efficiently when reading, even though the table is sorted (locally or globally) on any columns. However when table is Z-order sorted on multiple columns, Parquet/ORC can skip file/row-groups efficiently when reading. We should add the feature in Spark to allow OSS Spark users benefitted in running these queries.

With this PR, user can do Z-order sort when writing the table with followed syntax:

```sql
INSERT INTO t
SELECT ...
FROM ...
SORT BY ZORDER(x, y, ...)
```

or

```sql
INSERT INTO t
SELECT ...
FROM ...
ORDER BY ZORDER(x, y, ...)
```

Then when reading the table with filter on `x` and `y`, the performance can be improved by skipping more files and row-groups. More details below for micro benchmark.

This PR adds the support for Z-order on integer types (byte, short, int, and long). For other data types such as float and string will be added as followup. Code-gen support for expression will be also added later.

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->
To improve the query performance when filtering on multiple columns. Seeing 1x-6x run-time improvement in micro benchmark below.

```scala
override def runBenchmarkSuite(mainArgs: Array[String]): Unit = {
    def prepareTable(dir: File, numRows: Int): Unit = {
      import spark.implicits._
      val df = spark.range(numRows).map(_ => (Random.nextLong, Random.nextLong))
        .toDF(""x"", ""y"")
      val zorderedDf = df.sort(Column(ZOrder(Seq($""x"".expr, $""y"".expr))))

      saveAsTable(df, dir, """")
      saveAsTable(zorderedDf, dir, ""ZOrder"")
    }

    def saveAsTable(df: DataFrame, dir: File, suffix: String): Unit = {
      val blockSize = org.apache.parquet.hadoop.ParquetWriter.DEFAULT_PAGE_SIZE
      val orcPath = dir.getCanonicalPath + ""/orc"" + suffix
      val parquetPath = dir.getCanonicalPath + ""/parquet"" + suffix

      df.write.mode(""overwrite"")
        .option(""orc.dictionary.key.threshold"", 0.8)
        .option(""orc.compress.size"", blockSize)
        .option(""orc.stripe.size"", blockSize).orc(orcPath)
      spark.read.orc(orcPath).createOrReplaceTempView(""orcTable"" + suffix)

      df.write.mode(""overwrite"")
        .option(""parquet.block.size"", blockSize).parquet(parquetPath)
      spark.read.parquet(parquetPath).createOrReplaceTempView(""parquetTable"" + suffix)
    }

    def withTempTable(tableNames: String*)(f: => Unit): Unit = {
      try f finally tableNames.foreach(spark.catalog.dropTempView)
    }

    runBenchmark(s""ZOrder"") {
      withTempPath { dir =>
        withTempTable(""orcTable"", ""parquetTable"", ""orcTableZOrder"", ""parquetTableZOrder"") {
          prepareTable(dir, 1024 * 1024 * 15)
          val benchmark = new Benchmark(""zorder"", 1024 * 1024 * 15,
            minNumIters = 5, output = output)

          benchmark.addCase(""Parquet no sort"") { _ =>
            spark.sql(s""SELECT * FROM parquetTable WHERE x = 0 OR y = 0"").noop()
          }

          benchmark.addCase(""Parquet z-order sort on (x, y)"") { _ =>
            spark.sql(s""SELECT * FROM parquetTableZOrder WHERE x = 0 OR y = 0"").noop()
          }

          benchmark.addCase(""ORC no sort"") { _ =>
            spark.sql(s""SELECT * FROM orcTable WHERE x = 0 OR y = 0"").noop()
          }

          benchmark.addCase(""ORC z-order sort on (x, y)"") { _ =>
            spark.sql(s""SELECT * FROM orcTableZOrder WHERE x = 0 OR y = 0"").noop()
          }

          benchmark.run()
        }
      }
    }
  }
```

* Compare the performance between reading table having no sort, and table having local Z-order sort on `(x, y)`.
Seeing 6x run-time improvement for Parquet, and 1x for ORC:

```
Java HotSpot(TM) 64-Bit Server VM 1.8.0_181-b13 on Mac OS X 10.16
Intel(R) Core(TM) i9-9980HK CPU @ 2.40GHz
zorder:                                   Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)   Relative
------------------------------------------------------------------------------------------------------------------------
Parquet no sort                                     274            287          11         57.5          17.4       1.0X
Parquet z-order sort on (x, y)                       37             41           2        420.2           2.4       7.3X
ORC no sort                                         674            754          47         23.3          42.8       0.4X
ORC z-order sort on (x, y)                          262            282          11         59.9          16.7       1.0X
```

* Compare the performance between reading table having no sort, and table having local sort on `(x, y)`.
No performance improvement as expected.

```
Java HotSpot(TM) 64-Bit Server VM 1.8.0_181-b13 on Mac OS X 10.16
Intel(R) Core(TM) i9-9980HK CPU @ 2.40GHz
zorder:                                   Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)   Relative
------------------------------------------------------------------------------------------------------------------------
Parquet no sort                                     285            319          29         55.1          18.1       1.0X
Parquet sort on (x, y)                              278            290          10         56.5          17.7       1.0X
ORC no sort                                         823            842          21         19.1          52.4       0.3X
ORC sort on (x, y)                                  748            760          16         21.0          47.6       0.4X
```

### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->
Yes. The added expression can be used by user - `zorder`.

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.
-->
Added unit test in `ZOrderExpressionSuite.scala`.",https://api.github.com/repos/apache/spark/issues/34640/timeline,,spark,apache,c21,4629931,MDQ6VXNlcjQ2Mjk5MzE=,https://avatars.githubusercontent.com/u/4629931?v=4,,https://api.github.com/users/c21,https://github.com/c21,https://api.github.com/users/c21/followers,https://api.github.com/users/c21/following{/other_user},https://api.github.com/users/c21/gists{/gist_id},https://api.github.com/users/c21/starred{/owner}{/repo},https://api.github.com/users/c21/subscriptions,https://api.github.com/users/c21/orgs,https://api.github.com/users/c21/repos,https://api.github.com/users/c21/events{/privacy},https://api.github.com/users/c21/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34640,https://github.com/apache/spark/pull/34640,https://github.com/apache/spark/pull/34640.diff,https://github.com/apache/spark/pull/34640.patch,,https://api.github.com/repos/apache/spark/issues/34640/reactions,2,2,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
111,https://api.github.com/repos/apache/spark/issues/34637,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34637/labels{/name},https://api.github.com/repos/apache/spark/issues/34637/comments,https://api.github.com/repos/apache/spark/issues/34637/events,https://github.com/apache/spark/pull/34637,1056786558,PR_kwDOAQXtWs4ur3s3,34637,Spark-37349 add SQL Rest API parsing logic,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1406627200, 'node_id': 'MDU6TGFiZWwxNDA2NjI3MjAw', 'url': 'https://api.github.com/repos/apache/spark/labels/BUILD', 'name': 'BUILD', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,13,2021-11-18T00:27:25Z,2021-11-22T18:31:20Z,,CONTRIBUTOR,,False,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
Following up on https://issues.apache.org/jira/browse/SPARK-31440, values like
`""value"" : ""total (min, med, max (stageId: taskId))\n177.0 B (59.0 B, 59.0 B, 59.0 B (stage 1.0: task 5))""` are currently shown from Rest API calls which are not easily digested in its current form.New processing logic of the values is introduced along with the creation of the following class in the SQL Rest API to organize the metric values: 
```
case class Value private[spark] (stageId: Option[String] = None, taskId: Option[String] = None,
                                 amount: Option[String] = None, min: Option[String] = None,
                                 med: Option[String] = None, max: Option[String] = None)
```
Which after processing, would make the output look like 
`{
      ""value"" : {
        ""stageId"" : ""1.0"",
        ""taskId"" : ""5"",
        ""amount"" : ""177.0 B"",
        ""min"" : ""59.0 B"",
        ""med"" : ""59.0 B"",
        ""max"" : ""59.0 B""
      }`

Currently not in the PR but could be added if there is interest is the normalization of metrics for aggregation purposes such as the following:
- The conversion of hour, minute and second time units to milliseconds.
- PB,TB, GB, MB, KB units are converted to Bytes.
- Comma is removed from Comma formatted Long values (e.g: 8389632)

<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->


### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->
To organize and process new metric fields in a more user friendly manner.


### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->
Yes, see output below which are gathered from `Check Sql Rest Api Endpoints` Unit Test in SqlResourceWithActualMetricsSuite.scala with AQE set to true.
Before Changes:
[BeforeSpark37349UT.txt](https://github.com/apache/spark/files/7566623/BeforeSpark37349UT.txt)
After changes:
[AfterSpark37349UT.txt](https://github.com/apache/spark/files/7566624/AfterSpark37349UT.txt)

Backward Compatibility:
API Changes were made in `sql/core/src/main/scala/org/apache/spark/status/api/v1/sql/api.scala`.
### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.
-->
Added new Unit Test, manual testing locally.",https://api.github.com/repos/apache/spark/issues/34637/timeline,,spark,apache,yliou,16739760,MDQ6VXNlcjE2NzM5NzYw,https://avatars.githubusercontent.com/u/16739760?v=4,,https://api.github.com/users/yliou,https://github.com/yliou,https://api.github.com/users/yliou/followers,https://api.github.com/users/yliou/following{/other_user},https://api.github.com/users/yliou/gists{/gist_id},https://api.github.com/users/yliou/starred{/owner}{/repo},https://api.github.com/users/yliou/subscriptions,https://api.github.com/users/yliou/orgs,https://api.github.com/users/yliou/repos,https://api.github.com/users/yliou/events{/privacy},https://api.github.com/users/yliou/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34637,https://github.com/apache/spark/pull/34637,https://github.com/apache/spark/pull/34637.diff,https://github.com/apache/spark/pull/34637.patch,,https://api.github.com/repos/apache/spark/issues/34637/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
112,https://api.github.com/repos/apache/spark/issues/34636,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34636/labels{/name},https://api.github.com/repos/apache/spark/issues/34636/comments,https://api.github.com/repos/apache/spark/issues/34636/events,https://github.com/apache/spark/pull/34636,1056619938,PR_kwDOAQXtWs4urT7N,34636,[WIP][SPARK-37359][K8S] Cleanup the Spark Kubernetes Integration tests,"[{'id': 1406605057, 'node_id': 'MDU6TGFiZWwxNDA2NjA1MDU3', 'url': 'https://api.github.com/repos/apache/spark/labels/KUBERNETES', 'name': 'KUBERNETES', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,22,2021-11-17T20:50:29Z,2021-12-20T09:59:08Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?

Drop remove reason stats check to make the test less flaky & disable Spark R test [hasn't passed for a long time]
Try and avoid PV/PVC delete/create race condition

### Why are the changes needed?

Or K8s test suite is broken so people ignore it. This is not good.

Listener bus message is not always delivered and printed
SparkR tests have been broken for a long time and I don't see any interest in fixing them
PV/PVC creation/deletion can have a race condition during integration tests.


### Does this PR introduce _any_ user-facing change?

Test only change

### How was this patch tested?

WIP (waiting on CI for k8s int).",https://api.github.com/repos/apache/spark/issues/34636/timeline,,spark,apache,holdenk,59893,MDQ6VXNlcjU5ODkz,https://avatars.githubusercontent.com/u/59893?v=4,,https://api.github.com/users/holdenk,https://github.com/holdenk,https://api.github.com/users/holdenk/followers,https://api.github.com/users/holdenk/following{/other_user},https://api.github.com/users/holdenk/gists{/gist_id},https://api.github.com/users/holdenk/starred{/owner}{/repo},https://api.github.com/users/holdenk/subscriptions,https://api.github.com/users/holdenk/orgs,https://api.github.com/users/holdenk/repos,https://api.github.com/users/holdenk/events{/privacy},https://api.github.com/users/holdenk/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34636,https://github.com/apache/spark/pull/34636,https://github.com/apache/spark/pull/34636.diff,https://github.com/apache/spark/pull/34636.patch,,https://api.github.com/repos/apache/spark/issues/34636/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
113,https://api.github.com/repos/apache/spark/issues/34629,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34629/labels{/name},https://api.github.com/repos/apache/spark/issues/34629/comments,https://api.github.com/repos/apache/spark/issues/34629/events,https://github.com/apache/spark/pull/34629,1055872510,PR_kwDOAQXtWs4upAwz,34629,[SPARK-37355][CORE]Avoid Block Manager registrations when Executor is shutting down,"[{'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,3,2021-11-17T09:00:01Z,2021-12-23T05:41:07Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?

Avoid BlockManager registrations when executor is shutting down.

### Why are the changes needed?

The block manager should not do re-register if the executor is shutting down by driver.


### Does this PR introduce _any_ user-facing change?

No

### How was this patch tested?

Existing tests.
",https://api.github.com/repos/apache/spark/issues/34629/timeline,,spark,apache,wankunde,3626747,MDQ6VXNlcjM2MjY3NDc=,https://avatars.githubusercontent.com/u/3626747?v=4,,https://api.github.com/users/wankunde,https://github.com/wankunde,https://api.github.com/users/wankunde/followers,https://api.github.com/users/wankunde/following{/other_user},https://api.github.com/users/wankunde/gists{/gist_id},https://api.github.com/users/wankunde/starred{/owner}{/repo},https://api.github.com/users/wankunde/subscriptions,https://api.github.com/users/wankunde/orgs,https://api.github.com/users/wankunde/repos,https://api.github.com/users/wankunde/events{/privacy},https://api.github.com/users/wankunde/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34629,https://github.com/apache/spark/pull/34629,https://github.com/apache/spark/pull/34629.diff,https://github.com/apache/spark/pull/34629.patch,,https://api.github.com/repos/apache/spark/issues/34629/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
114,https://api.github.com/repos/apache/spark/issues/34623,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34623/labels{/name},https://api.github.com/repos/apache/spark/issues/34623/comments,https://api.github.com/repos/apache/spark/issues/34623/events,https://github.com/apache/spark/pull/34623,1055660235,PR_kwDOAQXtWs4uoUJn,34623,[SPARK-37347][SQL] Spark Thrift Server (STS) driver fullFC becourse of timeoutExecutor not shutdown correctly,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,7,2021-11-17T03:22:26Z,2021-11-19T08:51:50Z,,NONE,,False,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->
Add timeoutExecutor shutdown method in SparkExecuteStatementOperation and shut down when statement finished or error.

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->
When spark.sql.thriftServer.queryTimeout or java.sql.Statement.setQueryTimeout is setted >0 , SparkExecuteStatementOperation add timeoutExecutor to kill time-consumed query in SPARK-26533. But timeoutExecutor is not shutdown correctly when statement is finished, it can only be shutdown when timeout. When we set timeout to a long time for example 1 hour, the long-running STS driver will FullGC and the application is not available for a long time.

### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->
No

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.
-->
No",https://api.github.com/repos/apache/spark/issues/34623/timeline,,spark,apache,lk246,39769581,MDQ6VXNlcjM5NzY5NTgx,https://avatars.githubusercontent.com/u/39769581?v=4,,https://api.github.com/users/lk246,https://github.com/lk246,https://api.github.com/users/lk246/followers,https://api.github.com/users/lk246/following{/other_user},https://api.github.com/users/lk246/gists{/gist_id},https://api.github.com/users/lk246/starred{/owner}{/repo},https://api.github.com/users/lk246/subscriptions,https://api.github.com/users/lk246/orgs,https://api.github.com/users/lk246/repos,https://api.github.com/users/lk246/events{/privacy},https://api.github.com/users/lk246/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34623,https://github.com/apache/spark/pull/34623,https://github.com/apache/spark/pull/34623.diff,https://github.com/apache/spark/pull/34623.patch,,https://api.github.com/repos/apache/spark/issues/34623/reactions,2,0,0,0,0,0,2,0,0,,,,,,,,,,,,,,,,,,
115,https://api.github.com/repos/apache/spark/issues/34622,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34622/labels{/name},https://api.github.com/repos/apache/spark/issues/34622/comments,https://api.github.com/repos/apache/spark/issues/34622/events,https://github.com/apache/spark/pull/34622,1055256977,PR_kwDOAQXtWs4unH1x,34622,[SPARK-37340][UI] Display StageIds in Operators for SQL UI,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1406605079, 'node_id': 'MDU6TGFiZWwxNDA2NjA1MDc5', 'url': 'https://api.github.com/repos/apache/spark/labels/WEB%20UI', 'name': 'WEB UI', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,11,2021-11-16T19:45:10Z,2021-11-30T05:22:09Z,,CONTRIBUTOR,,False,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
Add explicit stageId to operator mapping in the Spark UI that is a more general version of https://issues.apache.org/jira/browse/SPARK-30209, where a stageId-> operator mapping is done with the following algorithm.
 1. Read SparkGraph to get every Node's name and respective AccumulatorIDs.
 2. Gets each stage's AccumulatorIDs.
 3. Maps Operators to stages by checking for non-zero intersection of Step 1 and 2's AccumulatorIDs.
 4. Connect SparkGraphNodes to respective StageIDs for rendering in SQL UI.
As a result, some operators without max metrics values will also have stageIds in the UI. In some cases, there is no operator->StageID mapping made because no stageIds have accumulatorIds that are a part of the Operator's accumulatorIds. URL links at the top to go to the succeeded jobs and completed stages that were executed as a part of the selected query are also provided.
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->


### Why are the changes needed?
Makes for easier and quicker debugging and navigation.
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->


### Does this PR introduce _any_ user-facing change?
Yes, `Succeeded Jobs:` and `Completed Stages:`listed at the top of the UI, along with `Stages:` in some of the operators.
<img width=""697"" alt=""Screen Shot 2021-11-16 at 11 35 51 AM"" src=""https://user-images.githubusercontent.com/16739760/142054791-8229d142-41cd-4706-a53e-7abb51e5901c.png"">

<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->


### How was this patch tested?
Manual test locally in SQL UI.
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.
-->
",https://api.github.com/repos/apache/spark/issues/34622/timeline,,spark,apache,yliou,16739760,MDQ6VXNlcjE2NzM5NzYw,https://avatars.githubusercontent.com/u/16739760?v=4,,https://api.github.com/users/yliou,https://github.com/yliou,https://api.github.com/users/yliou/followers,https://api.github.com/users/yliou/following{/other_user},https://api.github.com/users/yliou/gists{/gist_id},https://api.github.com/users/yliou/starred{/owner}{/repo},https://api.github.com/users/yliou/subscriptions,https://api.github.com/users/yliou/orgs,https://api.github.com/users/yliou/repos,https://api.github.com/users/yliou/events{/privacy},https://api.github.com/users/yliou/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34622,https://github.com/apache/spark/pull/34622,https://github.com/apache/spark/pull/34622.diff,https://github.com/apache/spark/pull/34622.patch,,https://api.github.com/repos/apache/spark/issues/34622/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
116,https://api.github.com/repos/apache/spark/issues/34616,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34616/labels{/name},https://api.github.com/repos/apache/spark/issues/34616/comments,https://api.github.com/repos/apache/spark/issues/34616/events,https://github.com/apache/spark/pull/34616,1054717965,PR_kwDOAQXtWs4ulWaO,34616,[SPARK-37344][SQL][DOC] spark-sql cli will keep `\` when match `\;` after spark3,"[{'id': 1981471081, 'node_id': 'MDU6TGFiZWwxOTgxNDcxMDgx', 'url': 'https://api.github.com/repos/apache/spark/labels/DOCS', 'name': 'DOCS', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,14,2021-11-16T10:38:46Z,2021-12-20T03:31:01Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?
Before Spark 3, if we pass a SQL like `select split('Spark SQL', '\;')`  to spark-sql, after process, it will actually execute  `select split('Spark SQL', ';')`.

spark-sql with verbose log:
```
[info]   2021-11-16 18:05:34.86 - stdout> spark-sql> select split('dawdawdawd','\;');
[info]   2021-11-16 18:05:34.875 - stdout> select split('dawdawdawd',';')
```

But after Spark 3 It will execute  `select split('Spark SQL', '\;')`
spark-sql with verbose log:
```
[info]   2021-11-16 18:05:34.86 - stdout> spark-sql> select split('dawdawdawd','\;');
[info]   2021-11-16 18:05:34.875 - stdout> select split('dawdawdawd','\;')
```


In this PR we doc this change.

This change is caused by hive commit : https://github.com/apache/hive/commit/65a65826a0d351a3d918bdb98595bdd106d37adb#diff-79277c3cfeb5bc38066fbbe2b90dcee5c870100b8b1e106d53ed0d56817bd0ee

Related JIRA ID : https://issues.apache.org/jira/browse/HIVE-15297 


### Why are the changes needed?
Update migration guide


### Does this PR introduce _any_ user-facing change?
No

### How was this patch tested?

Not need

![image](https://user-images.githubusercontent.com/46485123/142100077-a3c9151d-4a8d-4817-874a-c28dd03131ff.png)
",https://api.github.com/repos/apache/spark/issues/34616/timeline,,spark,apache,AngersZhuuuu,46485123,MDQ6VXNlcjQ2NDg1MTIz,https://avatars.githubusercontent.com/u/46485123?v=4,,https://api.github.com/users/AngersZhuuuu,https://github.com/AngersZhuuuu,https://api.github.com/users/AngersZhuuuu/followers,https://api.github.com/users/AngersZhuuuu/following{/other_user},https://api.github.com/users/AngersZhuuuu/gists{/gist_id},https://api.github.com/users/AngersZhuuuu/starred{/owner}{/repo},https://api.github.com/users/AngersZhuuuu/subscriptions,https://api.github.com/users/AngersZhuuuu/orgs,https://api.github.com/users/AngersZhuuuu/repos,https://api.github.com/users/AngersZhuuuu/events{/privacy},https://api.github.com/users/AngersZhuuuu/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34616,https://github.com/apache/spark/pull/34616,https://github.com/apache/spark/pull/34616.diff,https://github.com/apache/spark/pull/34616.patch,,https://api.github.com/repos/apache/spark/issues/34616/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
117,https://api.github.com/repos/apache/spark/issues/34604,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34604/labels{/name},https://api.github.com/repos/apache/spark/issues/34604/comments,https://api.github.com/repos/apache/spark/issues/34604/events,https://github.com/apache/spark/pull/34604,1053756296,PR_kwDOAQXtWs4uiZ84,34604,[SPARK-37329][YARN] File system delegation tokens are leaked,"[{'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,7,2021-11-15T14:54:15Z,2021-11-20T14:07:27Z,,NONE,,False,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->
Explicitly cancel the delegation token that's not taken care of by YARN.


### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->
Leaking file system delegation tokens create burden for the file system components (for example, KMS), and in the worst case, cause performance regression or even making FS inaccessible.

### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->
No

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.
-->
Manually tested on a small cluster, verify the kms delegation tokens are created and cancelled properly.",https://api.github.com/repos/apache/spark/issues/34604/timeline,,spark,apache,jojochuang,2691807,MDQ6VXNlcjI2OTE4MDc=,https://avatars.githubusercontent.com/u/2691807?v=4,,https://api.github.com/users/jojochuang,https://github.com/jojochuang,https://api.github.com/users/jojochuang/followers,https://api.github.com/users/jojochuang/following{/other_user},https://api.github.com/users/jojochuang/gists{/gist_id},https://api.github.com/users/jojochuang/starred{/owner}{/repo},https://api.github.com/users/jojochuang/subscriptions,https://api.github.com/users/jojochuang/orgs,https://api.github.com/users/jojochuang/repos,https://api.github.com/users/jojochuang/events{/privacy},https://api.github.com/users/jojochuang/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34604,https://github.com/apache/spark/pull/34604,https://github.com/apache/spark/pull/34604.diff,https://github.com/apache/spark/pull/34604.patch,,https://api.github.com/repos/apache/spark/issues/34604/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
118,https://api.github.com/repos/apache/spark/issues/34602,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34602/labels{/name},https://api.github.com/repos/apache/spark/issues/34602/comments,https://api.github.com/repos/apache/spark/issues/34602/events,https://github.com/apache/spark/pull/34602,1053617296,PR_kwDOAQXtWs4uh86l,34602,[SPARK-37328][SQL] Fix bug that OptimizeSkewedJoin may not work after it was moved from queryStageOptimizerRules to queryStagePreparationRules.,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,13,2021-11-15T12:35:58Z,2021-12-21T14:43:16Z,,NONE,,False,"
### What changes were proposed in this pull request?
Fix the issue that OptimizeSkewedJoin may not work.
Since OptimizeSkewedJoin was moved from `queryStageOptimizerRules` to `queryStagePreparationRules,` the position OptimizeSkewedJoin was applied has been moved from `newQueryStage()` to `reOptimize()`. The plan OptimizeSkewedJoin applied on changed from plan of new stage which is about to submit to whole spark plan.
In the cases where skewedJoin is not last stage, OptimizeSkewedJoin may not work because the number of collected shuffleStages is more than 2.


### Why are the changes needed?
Bug fix.

### Does this PR introduce _any_ user-facing change?
No


### How was this patch tested?
New test.
",https://api.github.com/repos/apache/spark/issues/34602/timeline,,spark,apache,Liulietong,20419086,MDQ6VXNlcjIwNDE5MDg2,https://avatars.githubusercontent.com/u/20419086?v=4,,https://api.github.com/users/Liulietong,https://github.com/Liulietong,https://api.github.com/users/Liulietong/followers,https://api.github.com/users/Liulietong/following{/other_user},https://api.github.com/users/Liulietong/gists{/gist_id},https://api.github.com/users/Liulietong/starred{/owner}{/repo},https://api.github.com/users/Liulietong/subscriptions,https://api.github.com/users/Liulietong/orgs,https://api.github.com/users/Liulietong/repos,https://api.github.com/users/Liulietong/events{/privacy},https://api.github.com/users/Liulietong/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34602,https://github.com/apache/spark/pull/34602,https://github.com/apache/spark/pull/34602.diff,https://github.com/apache/spark/pull/34602.patch,,https://api.github.com/repos/apache/spark/issues/34602/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
119,https://api.github.com/repos/apache/spark/issues/34593,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34593/labels{/name},https://api.github.com/repos/apache/spark/issues/34593/comments,https://api.github.com/repos/apache/spark/issues/34593/events,https://github.com/apache/spark/pull/34593,1053066585,PR_kwDOAQXtWs4ugKGk,34593,"[SPARK-37324][SQL] Adds support for decimal rounding mode up, down, half_down","[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,51,2021-11-14T23:07:33Z,2021-11-26T02:55:04Z,,CONTRIBUTOR,,False,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?

1. Adds support for Decimal RoundingMode.UP, DOWN and HALF_DOWN by letting users to pass the rounding mode as argument to round function.
2. bround function calls round function with the argument ""half_even""


### Why are the changes needed?

Currently there's no easier and straight forward way to round decimals with the mode UP, DOWN and HALF_DOWN. People need to use UDF or complex operations to do the same.

Opening support for the other rounding modes might interest a lot of use cases.

**SAP Hana Sql ROUND function does it :** 

`ROUND(<number> [, <position> [, <rounding_mode>]])`

REF : https://help.sap.com/viewer/7c78579ce9b14a669c1f3295b0d8ca16/Cloud/en-US/20e6a27575191014bd54a07fd86c585d.html


**Sql Server does something similar to this :**

`ROUND ( numeric_expression , length [ ,function ] )`

REF : https://docs.microsoft.com/en-us/sql/t-sql/functions/round-transact-sql?view=sql-server-ver15 


### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->
Now, users can specify the rounding mode while calling round function for round modes up, down, half_down. Calling round function without rounding mode will default to half_up.

```
> SELECT round(3.145, 2)
3.15

>SELECT round(3.145, 2, 'down')
3.14
```

```
df.withColumn(""value_rounded"", round('value, 0)

df.withColumn(""value_rounded"", round('value, 0, ""down"")
```

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.
-->
This patch was tested locally using unit test and git workflow.",https://api.github.com/repos/apache/spark/issues/34593/timeline,,spark,apache,sathiyapk,5880194,MDQ6VXNlcjU4ODAxOTQ=,https://avatars.githubusercontent.com/u/5880194?v=4,,https://api.github.com/users/sathiyapk,https://github.com/sathiyapk,https://api.github.com/users/sathiyapk/followers,https://api.github.com/users/sathiyapk/following{/other_user},https://api.github.com/users/sathiyapk/gists{/gist_id},https://api.github.com/users/sathiyapk/starred{/owner}{/repo},https://api.github.com/users/sathiyapk/subscriptions,https://api.github.com/users/sathiyapk/orgs,https://api.github.com/users/sathiyapk/repos,https://api.github.com/users/sathiyapk/events{/privacy},https://api.github.com/users/sathiyapk/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34593,https://github.com/apache/spark/pull/34593,https://github.com/apache/spark/pull/34593.diff,https://github.com/apache/spark/pull/34593.patch,,https://api.github.com/repos/apache/spark/issues/34593/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
120,https://api.github.com/repos/apache/spark/issues/34569,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34569/labels{/name},https://api.github.com/repos/apache/spark/issues/34569/comments,https://api.github.com/repos/apache/spark/issues/34569/events,https://github.com/apache/spark/pull/34569,1051688614,PR_kwDOAQXtWs4ucUd8,34569,[SPARK-37301][CORE] ConcurrentModificationException caused by CollectionAccumulator serialization in the heartbeat thread,"[{'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,5,2021-11-12T08:08:52Z,2021-11-15T09:56:08Z,,NONE,,False,"### What changes were proposed in this pull request?

In our production environment, you can use the following code to reproduce the problem:

```scala
val acc = sc.collectionAccumulator[String](""test_acc"")
    
sc.parallelize(Array(0)).foreach(_ => {
  var i = 0
  var stop = false
  val start = System.currentTimeMillis()
  while (!stop) {
    acc.add(i.toString)
    if (i % 10000 == 0) {
      acc.reset()
      if ((System.currentTimeMillis() - start) / 1000 > 120) {
        stop = true
      }
    }
    i = i + 1
  }
})
sc.stop()
```

This code can make the executor fail to send heartbeats, even more than the default 60 times, and then the executor exits.

```tex
21/11/11 21:00:23 WARN Executor: Issue communicating with driver in heartbeater
org.apache.spark.SparkException: Exception thrown in awaitResult: 
	at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)
	at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
	at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:103)
	at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1007)
	at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:212)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2019)
	at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.ConcurrentModificationException
	at java.util.ArrayList.writeObject(ArrayList.java:766)
	at sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:1028)
	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1496)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)
	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
	at java.io.ObjectOutputStream.writeArray(ObjectOutputStream.java:1378)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1174)
	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)
	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)
	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
	at java.io.ObjectOutputStream.writeArray(ObjectOutputStream.java:1378)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1174)
	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)
	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
	at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348)
	at org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:44)
	at org.apache.spark.rpc.netty.RequestMessage.serialize(NettyRpcEnv.scala:601)
	at org.apache.spark.rpc.netty.NettyRpcEnv.askAbortable(NettyRpcEnv.scala:244)
	at org.apache.spark.rpc.netty.NettyRpcEndpointRef.askAbortable(NettyRpcEnv.scala:555)
	at org.apache.spark.rpc.netty.NettyRpcEndpointRef.ask(NettyRpcEnv.scala:559)
	at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:102)
	... 12 more
21/11/11 21:00:23 ERROR Executor: Exit as unable to send heartbeats to driver more than 60 times
```

The reason is that when the heartbeat thread serializes the Collection Accumulator, the task thread may modify the Collection Accumulator


### Why are the changes needed?

Avoid heartbeat reporting failure, which may cause application failure


### Does this PR introduce _any_ user-facing change?

No


### How was this patch tested?
Existing tests and manual tests
",https://api.github.com/repos/apache/spark/issues/34569/timeline,,spark,apache,mcdull-zhang,63445864,MDQ6VXNlcjYzNDQ1ODY0,https://avatars.githubusercontent.com/u/63445864?v=4,,https://api.github.com/users/mcdull-zhang,https://github.com/mcdull-zhang,https://api.github.com/users/mcdull-zhang/followers,https://api.github.com/users/mcdull-zhang/following{/other_user},https://api.github.com/users/mcdull-zhang/gists{/gist_id},https://api.github.com/users/mcdull-zhang/starred{/owner}{/repo},https://api.github.com/users/mcdull-zhang/subscriptions,https://api.github.com/users/mcdull-zhang/orgs,https://api.github.com/users/mcdull-zhang/repos,https://api.github.com/users/mcdull-zhang/events{/privacy},https://api.github.com/users/mcdull-zhang/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34569,https://github.com/apache/spark/pull/34569,https://github.com/apache/spark/pull/34569.diff,https://github.com/apache/spark/pull/34569.patch,,https://api.github.com/repos/apache/spark/issues/34569/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
121,https://api.github.com/repos/apache/spark/issues/34568,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34568/labels{/name},https://api.github.com/repos/apache/spark/issues/34568/comments,https://api.github.com/repos/apache/spark/issues/34568/events,https://github.com/apache/spark/pull/34568,1051668730,PR_kwDOAQXtWs4ucQZf,34568,[SPARK-37287][SQL] Pull out dynamic partition and bucket sort from FileFormatWriter,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1406587328, 'node_id': 'MDU6TGFiZWwxNDA2NTg3MzI4', 'url': 'https://api.github.com/repos/apache/spark/labels/STRUCTURED%20STREAMING', 'name': 'STRUCTURED STREAMING', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,20,2021-11-12T07:38:19Z,2021-11-30T07:26:08Z,,CONTRIBUTOR,,False,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->

- Add a new trait `V1Write` to hold some sort infos of v1 write. e.g., partition columns, bucket spec.
- Then let the following writing command extend the `V1Write`, includes both datasource and hive
  - InsertIntoHadoopFsRelationCommand
  - CreateDataSourceTableAsSelectCommand
  - InsertIntoHiveTable
  - CreateHiveTableAsSelectBase
- Add a new rule `V1Writes` to decide if we should add a `Sort` operator based its `V1Write.requiredOrdering`.  This rule should be similar with `V2Writes`.
- So now we can remove the `SortExec` in `FileFormatWriter.write`.

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->
`FileFormatWriter.write` now is used by all V1 write which includes datasource and hive table. However it contains a sort which is based on dynamic partition and bucket columns that can not be seen in plan directly.

V2 write has a better approach that it satisfies the order or even distribution by using rule `V2Writes`.

V1 write should do the similar thing with V2 write.

### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->
no.

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.
-->
this is a code refactor, so it should pass CI",https://api.github.com/repos/apache/spark/issues/34568/timeline,,spark,apache,ulysses-you,12025282,MDQ6VXNlcjEyMDI1Mjgy,https://avatars.githubusercontent.com/u/12025282?v=4,,https://api.github.com/users/ulysses-you,https://github.com/ulysses-you,https://api.github.com/users/ulysses-you/followers,https://api.github.com/users/ulysses-you/following{/other_user},https://api.github.com/users/ulysses-you/gists{/gist_id},https://api.github.com/users/ulysses-you/starred{/owner}{/repo},https://api.github.com/users/ulysses-you/subscriptions,https://api.github.com/users/ulysses-you/orgs,https://api.github.com/users/ulysses-you/repos,https://api.github.com/users/ulysses-you/events{/privacy},https://api.github.com/users/ulysses-you/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34568,https://github.com/apache/spark/pull/34568,https://github.com/apache/spark/pull/34568.diff,https://github.com/apache/spark/pull/34568.patch,,https://api.github.com/repos/apache/spark/issues/34568/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
122,https://api.github.com/repos/apache/spark/issues/34558,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34558/labels{/name},https://api.github.com/repos/apache/spark/issues/34558/comments,https://api.github.com/repos/apache/spark/issues/34558/events,https://github.com/apache/spark/pull/34558,1051004592,PR_kwDOAQXtWs4uaHfU,34558,[SPARK-37019][SQL] Add codegen support to array higher-order functions,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,1,2021-11-11T13:55:00Z,2021-12-23T14:35:50Z,,CONTRIBUTOR,,False,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->
This PR adds codegen support to array based higher order functions except ArraySort. This is my first time playing around with codegen, so definitely looking for any feedback.

A few notes:
- Disabled subexpression elimination for lambda functions (this already was the case because it was CodegenFallback). I plan to explore supprting subexpression elimination inside lambda functions later on, as it will require special handling.
- I set the AtomicReference for all lambda values as well in case a child expression reverts to interpreted evaluation for any reason (CodegenFallback or otherwise)
- I can try to get through ArraySort too. Not sure how complicated it will be, mostly need to figure out how to make ArraySortLike support a custom comparator I think

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->
To improve performance of array higher-order function operations, letting the children be codegen'd and participate in WholeStageCodegen

### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->
No, only performance improvements.

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->
Existing unit tests, let me know if there's other codegen-specific unit tests I should add.",https://api.github.com/repos/apache/spark/issues/34558/timeline,,spark,apache,Kimahriman,3536454,MDQ6VXNlcjM1MzY0NTQ=,https://avatars.githubusercontent.com/u/3536454?v=4,,https://api.github.com/users/Kimahriman,https://github.com/Kimahriman,https://api.github.com/users/Kimahriman/followers,https://api.github.com/users/Kimahriman/following{/other_user},https://api.github.com/users/Kimahriman/gists{/gist_id},https://api.github.com/users/Kimahriman/starred{/owner}{/repo},https://api.github.com/users/Kimahriman/subscriptions,https://api.github.com/users/Kimahriman/orgs,https://api.github.com/users/Kimahriman/repos,https://api.github.com/users/Kimahriman/events{/privacy},https://api.github.com/users/Kimahriman/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34558,https://github.com/apache/spark/pull/34558,https://github.com/apache/spark/pull/34558.diff,https://github.com/apache/spark/pull/34558.patch,,https://api.github.com/repos/apache/spark/issues/34558/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
123,https://api.github.com/repos/apache/spark/issues/34553,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34553/labels{/name},https://api.github.com/repos/apache/spark/issues/34553/comments,https://api.github.com/repos/apache/spark/issues/34553/events,https://github.com/apache/spark/pull/34553,1050751053,PR_kwDOAQXtWs4uZSif,34553,[SPARK-37285] [ML] Add Weight of Evidence and Information value to ml.feature,"[{'id': 1405801475, 'node_id': 'MDU6TGFiZWwxNDA1ODAxNDc1', 'url': 'https://api.github.com/repos/apache/spark/labels/ML', 'name': 'ML', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,1,2021-11-11T09:04:48Z,2021-11-12T08:19:42Z,,NONE,,False,"JIRA Issue: https://issues.apache.org/jira/browse/SPARK-37285

The following shows the differences with this [PR-10803] :
1. More accurate algorithm to reduce the variance when the classification data skewed (all 1 or 0 in the bin of a feature) .
2. Programming based on RDD and better performance.
3. Separate IV and WOE modules to avoid coupling.",https://api.github.com/repos/apache/spark/issues/34553/timeline,,spark,apache,taosiyuan163,24226312,MDQ6VXNlcjI0MjI2MzEy,https://avatars.githubusercontent.com/u/24226312?v=4,,https://api.github.com/users/taosiyuan163,https://github.com/taosiyuan163,https://api.github.com/users/taosiyuan163/followers,https://api.github.com/users/taosiyuan163/following{/other_user},https://api.github.com/users/taosiyuan163/gists{/gist_id},https://api.github.com/users/taosiyuan163/starred{/owner}{/repo},https://api.github.com/users/taosiyuan163/subscriptions,https://api.github.com/users/taosiyuan163/orgs,https://api.github.com/users/taosiyuan163/repos,https://api.github.com/users/taosiyuan163/events{/privacy},https://api.github.com/users/taosiyuan163/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34553,https://github.com/apache/spark/pull/34553,https://github.com/apache/spark/pull/34553.diff,https://github.com/apache/spark/pull/34553.patch,,https://api.github.com/repos/apache/spark/issues/34553/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
124,https://api.github.com/repos/apache/spark/issues/34535,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34535/labels{/name},https://api.github.com/repos/apache/spark/issues/34535/comments,https://api.github.com/repos/apache/spark/issues/34535/events,https://github.com/apache/spark/pull/34535,1048679938,PR_kwDOAQXtWs4uSg_W,34535,[SPARK-37201][SQL] GeneratorNestedColumnAliasing support Generate with Filter,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,5,2021-11-09T14:34:53Z,2021-11-15T06:05:50Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?
In current ` GeneratorNestedColumnAliasing`, spark only support  push down case with Project as below
```
Project [v1#225, el#226]
   +- Project [struct#220.v1 AS v1#225, el#226, struct#220]
      +- Generate explode(array#221), false, [el#226]
         +- SubqueryAlias spark_catalog.default.table
            +- Relation default.table[struct#220,array#221] parquet
```

In this pr we support push dow with Project and Filter as below
```
Project [v1#225, el#226]
 +- Project [struct#220.v1 AS v1#225, el#226, struct#220]
    +- Filter ((el#226 = cx1) AND (struct#220.v2 = v3))
      +- Generate explode(array#221), false, [el#226]
         +- SubqueryAlias spark_catalog.default.table
            +- Relation default.table[struct#220,array#221] parquet
```

### Why are the changes needed?
Improve GeneratorNestedColumnAliasing to support more case


### Does this PR introduce _any_ user-facing change?
No


### How was this patch tested?
Add UT
",https://api.github.com/repos/apache/spark/issues/34535/timeline,,spark,apache,AngersZhuuuu,46485123,MDQ6VXNlcjQ2NDg1MTIz,https://avatars.githubusercontent.com/u/46485123?v=4,,https://api.github.com/users/AngersZhuuuu,https://github.com/AngersZhuuuu,https://api.github.com/users/AngersZhuuuu/followers,https://api.github.com/users/AngersZhuuuu/following{/other_user},https://api.github.com/users/AngersZhuuuu/gists{/gist_id},https://api.github.com/users/AngersZhuuuu/starred{/owner}{/repo},https://api.github.com/users/AngersZhuuuu/subscriptions,https://api.github.com/users/AngersZhuuuu/orgs,https://api.github.com/users/AngersZhuuuu/repos,https://api.github.com/users/AngersZhuuuu/events{/privacy},https://api.github.com/users/AngersZhuuuu/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34535,https://github.com/apache/spark/pull/34535,https://github.com/apache/spark/pull/34535.diff,https://github.com/apache/spark/pull/34535.patch,,https://api.github.com/repos/apache/spark/issues/34535/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
125,https://api.github.com/repos/apache/spark/issues/34532,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34532/labels{/name},https://api.github.com/repos/apache/spark/issues/34532/comments,https://api.github.com/repos/apache/spark/issues/34532/events,https://github.com/apache/spark/pull/34532,1048561122,PR_kwDOAQXtWs4uSIC_,34532,[SPARK-37256][SQL] Replace `ScalaObjectMapper` with `ClassTagExtensions` to fix compilation warning,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1406587328, 'node_id': 'MDU6TGFiZWwxNDA2NTg3MzI4', 'url': 'https://api.github.com/repos/apache/spark/labels/STRUCTURED%20STREAMING', 'name': 'STRUCTURED STREAMING', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,16,2021-11-09T12:40:41Z,2021-12-15T19:22:09Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?
There are some compilation warning log like follows:
```
[WARNING] [Warn] /spark/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/RebaseDateTime.scala:268: [deprecation @ org.apache.spark.sql.catalyst.util.RebaseDateTime.loadRebaseRecords.mapper.$anon | origin=com.fasterxml.jackson.module.scala.ScalaObjectMapper | version=2.12.1] trait ScalaObjectMapper in package scala is deprecated (since 2.12.1): ScalaObjectMapper is deprecated because Manifests are not supported in Scala3 
```

Refer to the recommendations of `jackson-module-scala`, this PR use `ClassTagExtensions`  instead of `ScalaObjectMapper`  to fix this compilation warning


### Why are the changes needed?
Fix compilation warning

### Does this PR introduce _any_ user-facing change?
No


### How was this patch tested?
Pass the Jenkins or GitHub Action

",https://api.github.com/repos/apache/spark/issues/34532/timeline,,spark,apache,LuciferYang,1475305,MDQ6VXNlcjE0NzUzMDU=,https://avatars.githubusercontent.com/u/1475305?v=4,,https://api.github.com/users/LuciferYang,https://github.com/LuciferYang,https://api.github.com/users/LuciferYang/followers,https://api.github.com/users/LuciferYang/following{/other_user},https://api.github.com/users/LuciferYang/gists{/gist_id},https://api.github.com/users/LuciferYang/starred{/owner}{/repo},https://api.github.com/users/LuciferYang/subscriptions,https://api.github.com/users/LuciferYang/orgs,https://api.github.com/users/LuciferYang/repos,https://api.github.com/users/LuciferYang/events{/privacy},https://api.github.com/users/LuciferYang/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34532,https://github.com/apache/spark/pull/34532,https://github.com/apache/spark/pull/34532.diff,https://github.com/apache/spark/pull/34532.patch,,https://api.github.com/repos/apache/spark/issues/34532/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
126,https://api.github.com/repos/apache/spark/issues/34513,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34513/labels{/name},https://api.github.com/repos/apache/spark/issues/34513/comments,https://api.github.com/repos/apache/spark/issues/34513/events,https://github.com/apache/spark/pull/34513,1047078487,PR_kwDOAQXtWs4uNTNT,34513,[SPARK-37234][PYTHON] Inline type hints for python/pyspark/mllib/stat/_statistics.py,"[{'id': 1405803268, 'node_id': 'MDU6TGFiZWwxNDA1ODAzMjY4', 'url': 'https://api.github.com/repos/apache/spark/labels/MLLIB', 'name': 'MLLIB', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1982260031, 'node_id': 'MDU6TGFiZWwxOTgyMjYwMDMx', 'url': 'https://api.github.com/repos/apache/spark/labels/PYTHON', 'name': 'PYTHON', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,33,2021-11-08T06:48:26Z,2021-11-26T04:06:08Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?
Inline type hints for python/pyspark/mllib/stat/_statistics.py
### Why are the changes needed?
We can take advantage of static type checking within the functions by inlining the type hints.
### Does this PR introduce _any_ user-facing change?
No
### How was this patch tested?
Existing tests",https://api.github.com/repos/apache/spark/issues/34513/timeline,,spark,apache,dchvn,91461145,MDQ6VXNlcjkxNDYxMTQ1,https://avatars.githubusercontent.com/u/91461145?v=4,,https://api.github.com/users/dchvn,https://github.com/dchvn,https://api.github.com/users/dchvn/followers,https://api.github.com/users/dchvn/following{/other_user},https://api.github.com/users/dchvn/gists{/gist_id},https://api.github.com/users/dchvn/starred{/owner}{/repo},https://api.github.com/users/dchvn/subscriptions,https://api.github.com/users/dchvn/orgs,https://api.github.com/users/dchvn/repos,https://api.github.com/users/dchvn/events{/privacy},https://api.github.com/users/dchvn/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34513,https://github.com/apache/spark/pull/34513,https://github.com/apache/spark/pull/34513.diff,https://github.com/apache/spark/pull/34513.patch,,https://api.github.com/repos/apache/spark/issues/34513/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
127,https://api.github.com/repos/apache/spark/issues/34504,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34504/labels{/name},https://api.github.com/repos/apache/spark/issues/34504/comments,https://api.github.com/repos/apache/spark/issues/34504/events,https://github.com/apache/spark/pull/34504,1046540120,PR_kwDOAQXtWs4uLsuz,34504,[SPARK-37226][SQL] Filter push down through window if partitionSpec isEmpty,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,21,2021-11-06T16:03:09Z,2021-12-01T08:50:07Z,,MEMBER,,False,"### What changes were proposed in this pull request?

This pr enhance `PushPredicateThroughNonJoin` to support filter push down through window if window partition is empty. For example:
```scala
spark.sql(""CREATE TABLE t1 using parquet AS SELECT id AS a, id AS b FROM range(1000)"")
spark.sql(""SELECT * FROM (SELECT *, ROW_NUMBER() OVER(ORDER BY a) AS rn FROM t1) t WHERE rn > 100 and rn <= 200"").explain(true)
```
After this pr:
```
== Optimized Logical Plan ==
Filter ((rn#3 > 100) AND (rn#3 <= 200))
+- Window [row_number() windowspecdefinition(a#5L ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS rn#3], [a#5L ASC NULLS FIRST]
   +- GlobalLimit 200
      +- LocalLimit 200
         +- Sort [a#5L ASC NULLS FIRST], true
            +- Relation default.t1[a#5L,b#6L] parquet
```

### Why are the changes needed?

Improve query performance.

### Does this PR introduce _any_ user-facing change?

No.


### How was this patch tested?

Unit test and benchmark test:
```scala
import org.apache.spark.benchmark.Benchmark
val numRows = 1024 * 1024 * 100
spark.sql(s""CREATE TABLE t1 using parquet AS SELECT id as a, id as b FROM range(${numRows}L)"")
val benchmark = new Benchmark(""Benchmark filter push down through window"", numRows, minNumIters = 5)

Seq(1, 1000).foreach { threshold =>
  val name = s""Filter push down through window ${if (threshold > 1) ""(Enabled)"" else ""(Disabled)""}""
  benchmark.addCase(name) { _ =>
    withSQLConf(""spark.sql.execution.topKSortFallbackThreshold"" -> s""$threshold"") {
      spark.sql(""SELECT * FROM (SELECT *, ROW_NUMBER() OVER(ORDER BY a) AS rn FROM t1) t WHERE rn > 100 and rn <= 200"").write.format(""noop"").mode(""Overwrite"").save()
    }
  }
}
benchmark.run()
```
Benchmark result:
```
Java HotSpot(TM) 64-Bit Server VM 1.8.0_251-b08 on Mac OS X 10.15.7
Intel(R) Core(TM) i9-9980HK CPU @ 2.40GHz
Benchmark filter push down through window:  Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)   Relative
--------------------------------------------------------------------------------------------------------------------------
Filter push down through window (Disabled)          79219          87062         NaN          1.3         755.5       1.0X
Filter push down through window (Enabled)            5339           5821         425         19.6          50.9      14.8X

```

Production benchmark:

Before this PR | After this PR
--- | ---
![image](https://user-images.githubusercontent.com/5399861/141800881-c0721682-69b3-4861-80fa-0b0ee324aeeb.png)  | ![image](https://user-images.githubusercontent.com/5399861/141878272-48252c7f-108b-4834-9d9b-e8a54eb05e75.png)
",https://api.github.com/repos/apache/spark/issues/34504/timeline,,spark,apache,wangyum,5399861,MDQ6VXNlcjUzOTk4NjE=,https://avatars.githubusercontent.com/u/5399861?v=4,,https://api.github.com/users/wangyum,https://github.com/wangyum,https://api.github.com/users/wangyum/followers,https://api.github.com/users/wangyum/following{/other_user},https://api.github.com/users/wangyum/gists{/gist_id},https://api.github.com/users/wangyum/starred{/owner}{/repo},https://api.github.com/users/wangyum/subscriptions,https://api.github.com/users/wangyum/orgs,https://api.github.com/users/wangyum/repos,https://api.github.com/users/wangyum/events{/privacy},https://api.github.com/users/wangyum/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34504,https://github.com/apache/spark/pull/34504,https://github.com/apache/spark/pull/34504.diff,https://github.com/apache/spark/pull/34504.patch,,https://api.github.com/repos/apache/spark/issues/34504/reactions,1,1,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
128,https://api.github.com/repos/apache/spark/issues/34500,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34500/labels{/name},https://api.github.com/repos/apache/spark/issues/34500/comments,https://api.github.com/repos/apache/spark/issues/34500/events,https://github.com/apache/spark/pull/34500,1046424098,PR_kwDOAQXtWs4uLXaZ,34500,[WIP][SPARK-33574][CORE] Improve locality for push-based shuffle especially for join-like operations,"[{'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,1,2021-11-06T05:31:02Z,2021-11-06T05:47:21Z,,CONTRIBUTOR,,False,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->
Reuse merger locations in the case where multiple stages output is consumed by a single stage (e.g.: Joins).

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->
For sibling shuffle map stages, i.e. ones that share common child stages, reusing merger locations between them can help to further increase shuffle locality ratio when push based shuffle is enabled. Examples include Joins where the reduce stage needs to fetch shuffle data from multiple parent shuffle map stages.

### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->
Yes. This PR introduces a client-side config for push-based shuffle `spark.shuffle.push.reuse.merger.locations`. If push-based shuffle is turned-off then users will not see any change.

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.
-->
UTs to be added.",https://api.github.com/repos/apache/spark/issues/34500/timeline,,spark,apache,rmcyang,31781684,MDQ6VXNlcjMxNzgxNjg0,https://avatars.githubusercontent.com/u/31781684?v=4,,https://api.github.com/users/rmcyang,https://github.com/rmcyang,https://api.github.com/users/rmcyang/followers,https://api.github.com/users/rmcyang/following{/other_user},https://api.github.com/users/rmcyang/gists{/gist_id},https://api.github.com/users/rmcyang/starred{/owner}{/repo},https://api.github.com/users/rmcyang/subscriptions,https://api.github.com/users/rmcyang/orgs,https://api.github.com/users/rmcyang/repos,https://api.github.com/users/rmcyang/events{/privacy},https://api.github.com/users/rmcyang/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34500,https://github.com/apache/spark/pull/34500,https://github.com/apache/spark/pull/34500.diff,https://github.com/apache/spark/pull/34500.patch,,https://api.github.com/repos/apache/spark/issues/34500/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
129,https://api.github.com/repos/apache/spark/issues/34491,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34491/labels{/name},https://api.github.com/repos/apache/spark/issues/34491/comments,https://api.github.com/repos/apache/spark/issues/34491/events,https://github.com/apache/spark/pull/34491,1045631417,PR_kwDOAQXtWs4uIzDo,34491,[SPARK-37215][YARN] Support Application Timeouts on YARN,"[{'id': 1427970157, 'node_id': 'MDU6TGFiZWwxNDI3OTcwMTU3', 'url': 'https://api.github.com/repos/apache/spark/labels/YARN', 'name': 'YARN', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,"[{'login': 'yaooqinn', 'id': 8326978, 'node_id': 'MDQ6VXNlcjgzMjY5Nzg=', 'avatar_url': 'https://avatars.githubusercontent.com/u/8326978?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/yaooqinn', 'html_url': 'https://github.com/yaooqinn', 'followers_url': 'https://api.github.com/users/yaooqinn/followers', 'following_url': 'https://api.github.com/users/yaooqinn/following{/other_user}', 'gists_url': 'https://api.github.com/users/yaooqinn/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/yaooqinn/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/yaooqinn/subscriptions', 'organizations_url': 'https://api.github.com/users/yaooqinn/orgs', 'repos_url': 'https://api.github.com/users/yaooqinn/repos', 'events_url': 'https://api.github.com/users/yaooqinn/events{/privacy}', 'received_events_url': 'https://api.github.com/users/yaooqinn/received_events', 'type': 'User', 'site_admin': False}]",,15,2021-11-05T09:38:48Z,2021-11-15T21:47:38Z,,CONTRIBUTOR,,False,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->

Since #YARN-3813/2.9.0/3.0.0, YARN supports ApplicationTimeouts. It helps enforce lifetime application SLAs. Currently, lifetime indicates the overall time spent by an application in YARN. It is calculated from its submit time to finish time, including running time and the waiting time for resource allocation.

YARN allows admins to set lifetime of an application at leaf-queue. It also allows users to set it programmatically. During application submission, user can set it in  `ApplicationSubmissionContext#setApplicationTimeouts(Map<ApplicationTimeoutType, Long> applicationTimeouts)`.

So far, YARN supports for one timeout type - LIFETIME.

In this PR, we `setApplicationTimeouts` when the YARN dependency is available, e.g. the default Hadoop 3.3 or user-specified Hadoop 2.9+ when Hadoop is provided at compile phase.


### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->

This can enforce the application SLAs.
For example, when the YARN queue is hit its limits of app concurrency or cpu/mem, an app will be pending for a very long time or even get stuck in  `ACCEPTED` state forever and do nothing. 

Sometimes, users also may want their app to succeed or timeout/failed with proper time constraints.

This is necessary for end-users use spark througth serverless spark platform like apache kyuubi(incubating) to prevent issue like https://github.com/apache/incubator-kyuubi/issues/1039, https://github.com/apache/incubator-kyuubi/issues/278 and so on.

### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->

yes, we add a new conf but do not change the current behavior

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.
-->

new tests added
",https://api.github.com/repos/apache/spark/issues/34491/timeline,,spark,apache,yaooqinn,8326978,MDQ6VXNlcjgzMjY5Nzg=,https://avatars.githubusercontent.com/u/8326978?v=4,,https://api.github.com/users/yaooqinn,https://github.com/yaooqinn,https://api.github.com/users/yaooqinn/followers,https://api.github.com/users/yaooqinn/following{/other_user},https://api.github.com/users/yaooqinn/gists{/gist_id},https://api.github.com/users/yaooqinn/starred{/owner}{/repo},https://api.github.com/users/yaooqinn/subscriptions,https://api.github.com/users/yaooqinn/orgs,https://api.github.com/users/yaooqinn/repos,https://api.github.com/users/yaooqinn/events{/privacy},https://api.github.com/users/yaooqinn/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34491,https://github.com/apache/spark/pull/34491,https://github.com/apache/spark/pull/34491.diff,https://github.com/apache/spark/pull/34491.patch,,https://api.github.com/repos/apache/spark/issues/34491/reactions,0,0,0,0,0,0,0,0,0,yaooqinn,8326978.0,MDQ6VXNlcjgzMjY5Nzg=,https://avatars.githubusercontent.com/u/8326978?v=4,,https://api.github.com/users/yaooqinn,https://github.com/yaooqinn,https://api.github.com/users/yaooqinn/followers,https://api.github.com/users/yaooqinn/following{/other_user},https://api.github.com/users/yaooqinn/gists{/gist_id},https://api.github.com/users/yaooqinn/starred{/owner}{/repo},https://api.github.com/users/yaooqinn/subscriptions,https://api.github.com/users/yaooqinn/orgs,https://api.github.com/users/yaooqinn/repos,https://api.github.com/users/yaooqinn/events{/privacy},https://api.github.com/users/yaooqinn/received_events,User,False
130,https://api.github.com/repos/apache/spark/issues/34489,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34489/labels{/name},https://api.github.com/repos/apache/spark/issues/34489/comments,https://api.github.com/repos/apache/spark/issues/34489/events,https://github.com/apache/spark/pull/34489,1045534130,PR_kwDOAQXtWs4uIey8,34489,[SPARK-37210][SQL] Write to static partition in dynamic write mode,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,6,2021-11-05T07:47:05Z,2021-12-01T07:38:41Z,,MEMBER,,False,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->
When using static partition writing, dynamicPartitionOverwrite should also be set to true. See [SPARK-37210](https://issues.apache.org/jira/browse/SPARK-37210) for details

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->
An error occurred while concurrently writing to different static partitions.

### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->
No

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.
-->
See [SPARK-37210](https://issues.apache.org/jira/browse/SPARK-37210) for specific test.",https://api.github.com/repos/apache/spark/issues/34489/timeline,,spark,apache,wForget,17894939,MDQ6VXNlcjE3ODk0OTM5,https://avatars.githubusercontent.com/u/17894939?v=4,,https://api.github.com/users/wForget,https://github.com/wForget,https://api.github.com/users/wForget/followers,https://api.github.com/users/wForget/following{/other_user},https://api.github.com/users/wForget/gists{/gist_id},https://api.github.com/users/wForget/starred{/owner}{/repo},https://api.github.com/users/wForget/subscriptions,https://api.github.com/users/wForget/orgs,https://api.github.com/users/wForget/repos,https://api.github.com/users/wForget/events{/privacy},https://api.github.com/users/wForget/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34489,https://github.com/apache/spark/pull/34489,https://github.com/apache/spark/pull/34489.diff,https://github.com/apache/spark/pull/34489.patch,,https://api.github.com/repos/apache/spark/issues/34489/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
131,https://api.github.com/repos/apache/spark/issues/34471,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34471/labels{/name},https://api.github.com/repos/apache/spark/issues/34471/comments,https://api.github.com/repos/apache/spark/issues/34471/events,https://github.com/apache/spark/pull/34471,1042897473,PR_kwDOAQXtWs4uAbSo,34471,[SPARK-36879][SQL] Support Parquet v2 data page encoding (DELTA_BINARY_PACKED) for the vectorized path ,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1982110661, 'node_id': 'MDU6TGFiZWwxOTgyMTEwNjYx', 'url': 'https://api.github.com/repos/apache/spark/labels/INFRA', 'name': 'INFRA', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,40,2021-11-02T22:13:05Z,2021-12-23T17:45:16Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?
Implements a vectorized version of the parquet reader for DELTA_BINARY_PACKED encoding
This PR includes a previous PR for this issue which passed the read request thru to the parquet implementation and which was not vectorized. The current PR builds on top of that PR (hence both are included).

### Why are the changes needed?
Currently Spark throws an exception when reading data with these encodings if vectorized reader is enabled

### Does this PR introduce _any_ user-facing change?
No

### How was this patch tested?
Additional unit tests for the encoding for both long and integer types (mirroring the unit tests in the Parquet implementation)",https://api.github.com/repos/apache/spark/issues/34471/timeline,,spark,apache,parthchandra,6529136,MDQ6VXNlcjY1MjkxMzY=,https://avatars.githubusercontent.com/u/6529136?v=4,,https://api.github.com/users/parthchandra,https://github.com/parthchandra,https://api.github.com/users/parthchandra/followers,https://api.github.com/users/parthchandra/following{/other_user},https://api.github.com/users/parthchandra/gists{/gist_id},https://api.github.com/users/parthchandra/starred{/owner}{/repo},https://api.github.com/users/parthchandra/subscriptions,https://api.github.com/users/parthchandra/orgs,https://api.github.com/users/parthchandra/repos,https://api.github.com/users/parthchandra/events{/privacy},https://api.github.com/users/parthchandra/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34471,https://github.com/apache/spark/pull/34471,https://github.com/apache/spark/pull/34471.diff,https://github.com/apache/spark/pull/34471.patch,,https://api.github.com/repos/apache/spark/issues/34471/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
132,https://api.github.com/repos/apache/spark/issues/34468,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34468/labels{/name},https://api.github.com/repos/apache/spark/issues/34468/comments,https://api.github.com/repos/apache/spark/issues/34468/events,https://github.com/apache/spark/pull/34468,1042074184,PR_kwDOAQXtWs4t-EtD,34468,[SPARK-37194][SQL] Avoid unnecessary sort in FileFormatWriter if it's not dynamic partition,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1406587328, 'node_id': 'MDU6TGFiZWwxNDA2NTg3MzI4', 'url': 'https://api.github.com/repos/apache/spark/labels/STRUCTURED%20STREAMING', 'name': 'STRUCTURED STREAMING', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,7,2021-11-02T09:32:15Z,2021-11-02T17:33:32Z,,CONTRIBUTOR,,False,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->
Pass a new parameter `dynamicPartition` to `FileFormatWriter.write` so that we can distinguish if we need local sort or not.

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->
Avoid unnecessary sort in FileFormatWriter if it's not dynamic partition

`FileFormatWriter.write` will sort the partition and bucket column before writing. I think this code path assumed the input `partitionColumns` are dynamic but actually it's not. It now is used by three code path:
- `FileStreamSink`; it should be always dynamic partition
- `SaveAsHiveFile`; it followed the assuming that `InsertIntoHiveTable` has removed the static partition and `InsertIntoHiveDirCommand` has no partition
- `InsertIntoHadoopFsRelationCommand`; it passed `partitionColumns` into `FileFormatWriter.write` without removing static partition because we need it to generate the partition path in `DynamicPartitionDataWriter`

It shows that the unnecessary sort only affected the `InsertIntoHadoopFsRelationCommand` if we write data with static partition.


### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->
no.

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.
-->
It should not affect the existed behavior, just improve perf. And for perf number, I did a simple benchmak:

```sql

CREATE TABLE test (id long) USING PARQUET PARTITIONED BY (d string);

-- before this PR, it tooks 1.82  seconds
-- after this PR,  it tooks 1.072 seconds
INSERT OVERWRITE TABLE test PARTITION(d='a') SELECT id FROM range(10000000);
```

",https://api.github.com/repos/apache/spark/issues/34468/timeline,,spark,apache,ulysses-you,12025282,MDQ6VXNlcjEyMDI1Mjgy,https://avatars.githubusercontent.com/u/12025282?v=4,,https://api.github.com/users/ulysses-you,https://github.com/ulysses-you,https://api.github.com/users/ulysses-you/followers,https://api.github.com/users/ulysses-you/following{/other_user},https://api.github.com/users/ulysses-you/gists{/gist_id},https://api.github.com/users/ulysses-you/starred{/owner}{/repo},https://api.github.com/users/ulysses-you/subscriptions,https://api.github.com/users/ulysses-you/orgs,https://api.github.com/users/ulysses-you/repos,https://api.github.com/users/ulysses-you/events{/privacy},https://api.github.com/users/ulysses-you/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34468,https://github.com/apache/spark/pull/34468,https://github.com/apache/spark/pull/34468.diff,https://github.com/apache/spark/pull/34468.patch,,https://api.github.com/repos/apache/spark/issues/34468/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
133,https://api.github.com/repos/apache/spark/issues/34457,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34457/labels{/name},https://api.github.com/repos/apache/spark/issues/34457/comments,https://api.github.com/repos/apache/spark/issues/34457/events,https://github.com/apache/spark/pull/34457,1041189443,PR_kwDOAQXtWs4t7XzP,34457,[SPARK-37178][ML] Add Target Encoding to ml.feature,"[{'id': 1405801475, 'node_id': 'MDU6TGFiZWwxNDA1ODAxNDc1', 'url': 'https://api.github.com/repos/apache/spark/labels/ML', 'name': 'ML', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,3,2021-11-01T13:45:19Z,2021-11-02T08:12:14Z,,NONE,,False,JIRA Issue: https://issues.apache.org/jira/browse/SPARK-37178,https://api.github.com/repos/apache/spark/issues/34457/timeline,,spark,apache,taosiyuan163,24226312,MDQ6VXNlcjI0MjI2MzEy,https://avatars.githubusercontent.com/u/24226312?v=4,,https://api.github.com/users/taosiyuan163,https://github.com/taosiyuan163,https://api.github.com/users/taosiyuan163/followers,https://api.github.com/users/taosiyuan163/following{/other_user},https://api.github.com/users/taosiyuan163/gists{/gist_id},https://api.github.com/users/taosiyuan163/starred{/owner}{/repo},https://api.github.com/users/taosiyuan163/subscriptions,https://api.github.com/users/taosiyuan163/orgs,https://api.github.com/users/taosiyuan163/repos,https://api.github.com/users/taosiyuan163/events{/privacy},https://api.github.com/users/taosiyuan163/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34457,https://github.com/apache/spark/pull/34457,https://github.com/apache/spark/pull/34457.diff,https://github.com/apache/spark/pull/34457.patch,,https://api.github.com/repos/apache/spark/issues/34457/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
134,https://api.github.com/repos/apache/spark/issues/34453,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34453/labels{/name},https://api.github.com/repos/apache/spark/issues/34453/comments,https://api.github.com/repos/apache/spark/issues/34453/events,https://github.com/apache/spark/pull/34453,1040721196,PR_kwDOAQXtWs4t502o,34453,[SPARK-37173][SQL] SparkGetFunctionOperation return builtin function only once,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981471081, 'node_id': 'MDU6TGFiZWwxOTgxNDcxMDgx', 'url': 'https://api.github.com/repos/apache/spark/labels/DOCS', 'name': 'DOCS', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,34,2021-11-01T03:29:59Z,2021-12-20T03:30:37Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?
According to https://github.com/apache/spark/pull/25252/files#r738489764, if we use wild pattern, it will return too much rows.

In this pr we return common builtin functions only once

### Why are the changes needed?
Improve performance

### Does this PR introduce _any_ user-facing change?
No

### How was this patch tested?
WIP
",https://api.github.com/repos/apache/spark/issues/34453/timeline,,spark,apache,AngersZhuuuu,46485123,MDQ6VXNlcjQ2NDg1MTIz,https://avatars.githubusercontent.com/u/46485123?v=4,,https://api.github.com/users/AngersZhuuuu,https://github.com/AngersZhuuuu,https://api.github.com/users/AngersZhuuuu/followers,https://api.github.com/users/AngersZhuuuu/following{/other_user},https://api.github.com/users/AngersZhuuuu/gists{/gist_id},https://api.github.com/users/AngersZhuuuu/starred{/owner}{/repo},https://api.github.com/users/AngersZhuuuu/subscriptions,https://api.github.com/users/AngersZhuuuu/orgs,https://api.github.com/users/AngersZhuuuu/repos,https://api.github.com/users/AngersZhuuuu/events{/privacy},https://api.github.com/users/AngersZhuuuu/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34453,https://github.com/apache/spark/pull/34453,https://github.com/apache/spark/pull/34453.diff,https://github.com/apache/spark/pull/34453.patch,,https://api.github.com/repos/apache/spark/issues/34453/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
135,https://api.github.com/repos/apache/spark/issues/34439,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34439/labels{/name},https://api.github.com/repos/apache/spark/issues/34439/comments,https://api.github.com/repos/apache/spark/issues/34439/events,https://github.com/apache/spark/pull/34439,1039429213,PR_kwDOAQXtWs4t19yw,34439,[SPARK-37095][PYTHON] Inline type hints for files in python/pyspark/broadcast.py,"[{'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1982260031, 'node_id': 'MDU6TGFiZWwxOTgyMjYwMDMx', 'url': 'https://api.github.com/repos/apache/spark/labels/PYTHON', 'name': 'PYTHON', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,24,2021-10-29T10:39:34Z,2021-12-13T05:11:09Z,,CONTRIBUTOR,,False,"Lead-authored-by: dchvn nguyen <dgd_contributor@viettel.com.vn>
Co-authored-by: zero323 <mszymkiewicz@gmail.com>

### What changes were proposed in this pull request?
Inline type hints for python/pyspark/broadcast.py
### Why are the changes needed?
We can take advantage of static type checking within the functions by inlining the type hints.
### Does this PR introduce _any_ user-facing change?
No
### How was this patch tested?
Existing tests",https://api.github.com/repos/apache/spark/issues/34439/timeline,,spark,apache,dchvn,91461145,MDQ6VXNlcjkxNDYxMTQ1,https://avatars.githubusercontent.com/u/91461145?v=4,,https://api.github.com/users/dchvn,https://github.com/dchvn,https://api.github.com/users/dchvn/followers,https://api.github.com/users/dchvn/following{/other_user},https://api.github.com/users/dchvn/gists{/gist_id},https://api.github.com/users/dchvn/starred{/owner}{/repo},https://api.github.com/users/dchvn/subscriptions,https://api.github.com/users/dchvn/orgs,https://api.github.com/users/dchvn/repos,https://api.github.com/users/dchvn/events{/privacy},https://api.github.com/users/dchvn/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34439,https://github.com/apache/spark/pull/34439,https://github.com/apache/spark/pull/34439.diff,https://github.com/apache/spark/pull/34439.patch,,https://api.github.com/repos/apache/spark/issues/34439/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
136,https://api.github.com/repos/apache/spark/issues/34407,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34407/labels{/name},https://api.github.com/repos/apache/spark/issues/34407/comments,https://api.github.com/repos/apache/spark/issues/34407/events,https://github.com/apache/spark/pull/34407,1037961916,PR_kwDOAQXtWs4txNtD,34407,[SPARK-37047][SQL][FOLLOWUP] lpad/rpad should work in non-ANSI mode if parameters str and pad are different types,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,4,2021-10-28T00:14:09Z,2021-10-28T02:11:07Z,,CONTRIBUTOR,,False,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->

This is a followup of #34154 and #34370 . Now `lpad`/`rpad` allow the `str` and `pad` parameters to be of different types in non-ANSI mode. The result type in this case is a character string.

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->

The changes in this PR restore the behavior (in non-ANSI mode) to that prior to #34154 and #34370 when the `lpad` and `rpad` functions take an `str` and `pad` argument and these arguments are of different types.

### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->

No. The overloads for the `lpad` and `rpad` functions have not been released yet.

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.
-->

Existing tests are enough (and have been updated appropriately).
",https://api.github.com/repos/apache/spark/issues/34407/timeline,,spark,apache,mkaravel,6397014,MDQ6VXNlcjYzOTcwMTQ=,https://avatars.githubusercontent.com/u/6397014?v=4,,https://api.github.com/users/mkaravel,https://github.com/mkaravel,https://api.github.com/users/mkaravel/followers,https://api.github.com/users/mkaravel/following{/other_user},https://api.github.com/users/mkaravel/gists{/gist_id},https://api.github.com/users/mkaravel/starred{/owner}{/repo},https://api.github.com/users/mkaravel/subscriptions,https://api.github.com/users/mkaravel/orgs,https://api.github.com/users/mkaravel/repos,https://api.github.com/users/mkaravel/events{/privacy},https://api.github.com/users/mkaravel/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34407,https://github.com/apache/spark/pull/34407,https://github.com/apache/spark/pull/34407.diff,https://github.com/apache/spark/pull/34407.patch,,https://api.github.com/repos/apache/spark/issues/34407/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
137,https://api.github.com/repos/apache/spark/issues/34406,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34406/labels{/name},https://api.github.com/repos/apache/spark/issues/34406/comments,https://api.github.com/repos/apache/spark/issues/34406/events,https://github.com/apache/spark/pull/34406,1037798773,PR_kwDOAQXtWs4twruD,34406,Minor fix to docs for read_csv,"[{'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1982260031, 'node_id': 'MDU6TGFiZWwxOTgyMjYwMDMx', 'url': 'https://api.github.com/repos/apache/spark/labels/PYTHON', 'name': 'PYTHON', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,1,2021-10-27T19:46:40Z,2021-10-27T20:09:44Z,,NONE,,False,"Fixed documentation of the escapechar parameter of read_csv function
(docs incorrectly say this is for escaping the delimiter - it is for escaping a quotechar)

<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->
Minor update to function documentation.

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->
Currently the documentation for the escapechar parameter is incorrect.

### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->
Fixes the documentation

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.
-->
No testing needed",https://api.github.com/repos/apache/spark/issues/34406/timeline,,spark,apache,tnixon,1181780,MDQ6VXNlcjExODE3ODA=,https://avatars.githubusercontent.com/u/1181780?v=4,,https://api.github.com/users/tnixon,https://github.com/tnixon,https://api.github.com/users/tnixon/followers,https://api.github.com/users/tnixon/following{/other_user},https://api.github.com/users/tnixon/gists{/gist_id},https://api.github.com/users/tnixon/starred{/owner}{/repo},https://api.github.com/users/tnixon/subscriptions,https://api.github.com/users/tnixon/orgs,https://api.github.com/users/tnixon/repos,https://api.github.com/users/tnixon/events{/privacy},https://api.github.com/users/tnixon/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34406,https://github.com/apache/spark/pull/34406,https://github.com/apache/spark/pull/34406.diff,https://github.com/apache/spark/pull/34406.patch,,https://api.github.com/repos/apache/spark/issues/34406/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
138,https://api.github.com/repos/apache/spark/issues/34404,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34404/labels{/name},https://api.github.com/repos/apache/spark/issues/34404/comments,https://api.github.com/repos/apache/spark/issues/34404/events,https://github.com/apache/spark/pull/34404,1037629184,PR_kwDOAQXtWs4twI_P,34404,[DO NOT MERGE][PYTHON] testing lint-python,"[{'id': 1406627200, 'node_id': 'MDU6TGFiZWwxNDA2NjI3MjAw', 'url': 'https://api.github.com/repos/apache/spark/labels/BUILD', 'name': 'BUILD', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1982260031, 'node_id': 'MDU6TGFiZWwxOTgyMjYwMDMx', 'url': 'https://api.github.com/repos/apache/spark/labels/PYTHON', 'name': 'PYTHON', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,8,2021-10-27T16:35:01Z,2021-10-27T21:11:05Z,,CONTRIBUTOR,,False,DO NOT MERGE TYVM,https://api.github.com/repos/apache/spark/issues/34404/timeline,,spark,apache,shaneknapp,1606572,MDQ6VXNlcjE2MDY1NzI=,https://avatars.githubusercontent.com/u/1606572?v=4,,https://api.github.com/users/shaneknapp,https://github.com/shaneknapp,https://api.github.com/users/shaneknapp/followers,https://api.github.com/users/shaneknapp/following{/other_user},https://api.github.com/users/shaneknapp/gists{/gist_id},https://api.github.com/users/shaneknapp/starred{/owner}{/repo},https://api.github.com/users/shaneknapp/subscriptions,https://api.github.com/users/shaneknapp/orgs,https://api.github.com/users/shaneknapp/repos,https://api.github.com/users/shaneknapp/events{/privacy},https://api.github.com/users/shaneknapp/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34404,https://github.com/apache/spark/pull/34404,https://github.com/apache/spark/pull/34404.diff,https://github.com/apache/spark/pull/34404.patch,,https://api.github.com/repos/apache/spark/issues/34404/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
139,https://api.github.com/repos/apache/spark/issues/34402,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34402/labels{/name},https://api.github.com/repos/apache/spark/issues/34402/comments,https://api.github.com/repos/apache/spark/issues/34402/events,https://github.com/apache/spark/pull/34402,1037518916,PR_kwDOAQXtWs4tvxz2,34402,[SPARK-30220] Enable using Exists/In subqueries outside of the Filter node,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,20,2021-10-27T14:49:47Z,2021-12-14T10:44:50Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?
Enable using Exists/In subqueries in other nodes besides `Filter`: `Aggregate`, `Project` and `Window`.
This is allready mostly supported, but it was blocked in the `Analyzer`. Only requires a small tweak in the `Optimizer`.

### Why are the changes needed?
One of the last open feature parities between PostgreSQL and Spark: [SPARK-30374](https://issues.apache.org/jira/browse/SPARK-30374)

### Does this PR introduce _any_ user-facing change?
No

### How was this patch tested?
Existing and new UTs
",https://api.github.com/repos/apache/spark/issues/34402/timeline,,spark,apache,tanelk,3342974,MDQ6VXNlcjMzNDI5NzQ=,https://avatars.githubusercontent.com/u/3342974?v=4,,https://api.github.com/users/tanelk,https://github.com/tanelk,https://api.github.com/users/tanelk/followers,https://api.github.com/users/tanelk/following{/other_user},https://api.github.com/users/tanelk/gists{/gist_id},https://api.github.com/users/tanelk/starred{/owner}{/repo},https://api.github.com/users/tanelk/subscriptions,https://api.github.com/users/tanelk/orgs,https://api.github.com/users/tanelk/repos,https://api.github.com/users/tanelk/events{/privacy},https://api.github.com/users/tanelk/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34402,https://github.com/apache/spark/pull/34402,https://github.com/apache/spark/pull/34402.diff,https://github.com/apache/spark/pull/34402.patch,,https://api.github.com/repos/apache/spark/issues/34402/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
140,https://api.github.com/repos/apache/spark/issues/34396,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34396/labels{/name},https://api.github.com/repos/apache/spark/issues/34396/comments,https://api.github.com/repos/apache/spark/issues/34396/events,https://github.com/apache/spark/pull/34396,1036868643,PR_kwDOAQXtWs4ttsS6,34396,[SPARK-37124][SQL] Support RowToColumnarExec with Arrow format,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,11,2021-10-27T01:00:10Z,2021-11-23T02:00:21Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?
This Jira is aim to support Arrow format in RowToColumnarExec.

### Why are the changes needed?
Current ArrowColumnVector is not fully equivalent to OnHeap/OffHeapColumnVector in spark, so RowToColumnarExec doesn't support write to Arrow format so far.

since Arrow API is now being more stable, and using pandas udf will perform much better than python udf.

### What has been done in this pull request?
I am  proposing to support RowToColumnarExec with Arrow.

What I did in this PR is to add a load api in ArrowColumnVector to load arrowRecordBatch to ArrowColumnVector, then called inside RowToColumnarExec doExecute.

### How was this patch tested?
UTs are also added to test this new API and RowToColumnarExec with ArrowFormat.

### Does this PR introduce _any_ user-facing change?
NO

Signed-off-by: Chendi Xue <chendi.xue@intel.com>",https://api.github.com/repos/apache/spark/issues/34396/timeline,,spark,apache,xuechendi,4355494,MDQ6VXNlcjQzNTU0OTQ=,https://avatars.githubusercontent.com/u/4355494?v=4,,https://api.github.com/users/xuechendi,https://github.com/xuechendi,https://api.github.com/users/xuechendi/followers,https://api.github.com/users/xuechendi/following{/other_user},https://api.github.com/users/xuechendi/gists{/gist_id},https://api.github.com/users/xuechendi/starred{/owner}{/repo},https://api.github.com/users/xuechendi/subscriptions,https://api.github.com/users/xuechendi/orgs,https://api.github.com/users/xuechendi/repos,https://api.github.com/users/xuechendi/events{/privacy},https://api.github.com/users/xuechendi/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34396,https://github.com/apache/spark/pull/34396,https://github.com/apache/spark/pull/34396.diff,https://github.com/apache/spark/pull/34396.patch,,https://api.github.com/repos/apache/spark/issues/34396/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
141,https://api.github.com/repos/apache/spark/issues/34386,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34386/labels{/name},https://api.github.com/repos/apache/spark/issues/34386/comments,https://api.github.com/repos/apache/spark/issues/34386/events,https://github.com/apache/spark/pull/34386,1035682757,PR_kwDOAQXtWs4tp0U0,34386,[WIP] - Changes to PySpark doc homepage and User Guide,"[{'id': 1981471081, 'node_id': 'MDU6TGFiZWwxOTgxNDcxMDgx', 'url': 'https://api.github.com/repos/apache/spark/labels/DOCS', 'name': 'DOCS', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1982110661, 'node_id': 'MDU6TGFiZWwxOTgyMTEwNjYx', 'url': 'https://api.github.com/repos/apache/spark/labels/INFRA', 'name': 'INFRA', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1982260031, 'node_id': 'MDU6TGFiZWwxOTgyMjYwMDMx', 'url': 'https://api.github.com/repos/apache/spark/labels/PYTHON', 'name': 'PYTHON', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,14,2021-10-26T00:12:19Z,2021-12-03T16:50:43Z,,NONE,,True,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
WIP - fixes to PySpark documentation front page and User guide 
-->


### Why are the changes needed?
<!--
Make the content on the documentation page more coherent
-->


### Does this PR introduce _any_ user-facing change?
<!--
The docs are user-facing and will be the front page of PySpark home page and the User guide
-->


### How was this patch tested?
<!--
The docs were built and tested using sphinx and Github pages
-->
",https://api.github.com/repos/apache/spark/issues/34386/timeline,,spark,apache,srijith-rajamohan,78916489,MDQ6VXNlcjc4OTE2NDg5,https://avatars.githubusercontent.com/u/78916489?v=4,,https://api.github.com/users/srijith-rajamohan,https://github.com/srijith-rajamohan,https://api.github.com/users/srijith-rajamohan/followers,https://api.github.com/users/srijith-rajamohan/following{/other_user},https://api.github.com/users/srijith-rajamohan/gists{/gist_id},https://api.github.com/users/srijith-rajamohan/starred{/owner}{/repo},https://api.github.com/users/srijith-rajamohan/subscriptions,https://api.github.com/users/srijith-rajamohan/orgs,https://api.github.com/users/srijith-rajamohan/repos,https://api.github.com/users/srijith-rajamohan/events{/privacy},https://api.github.com/users/srijith-rajamohan/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34386,https://github.com/apache/spark/pull/34386,https://github.com/apache/spark/pull/34386.diff,https://github.com/apache/spark/pull/34386.patch,,https://api.github.com/repos/apache/spark/issues/34386/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
142,https://api.github.com/repos/apache/spark/issues/34378,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34378/labels{/name},https://api.github.com/repos/apache/spark/issues/34378/comments,https://api.github.com/repos/apache/spark/issues/34378/events,https://github.com/apache/spark/pull/34378,1034800338,PR_kwDOAQXtWs4tm9Ax,34378,spark executor pod anti affinity when run on k8s,[],open,False,,[],,5,2021-10-25T07:57:08Z,2021-11-04T05:21:18Z,,NONE,,False,I have realized the executor pod anti affinity. This prevents too many pods from starting on one node. ,https://api.github.com/repos/apache/spark/issues/34378/timeline,,spark,apache,wfxxh,22764286,MDQ6VXNlcjIyNzY0Mjg2,https://avatars.githubusercontent.com/u/22764286?v=4,,https://api.github.com/users/wfxxh,https://github.com/wfxxh,https://api.github.com/users/wfxxh/followers,https://api.github.com/users/wfxxh/following{/other_user},https://api.github.com/users/wfxxh/gists{/gist_id},https://api.github.com/users/wfxxh/starred{/owner}{/repo},https://api.github.com/users/wfxxh/subscriptions,https://api.github.com/users/wfxxh/orgs,https://api.github.com/users/wfxxh/repos,https://api.github.com/users/wfxxh/events{/privacy},https://api.github.com/users/wfxxh/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34378,https://github.com/apache/spark/pull/34378,https://github.com/apache/spark/pull/34378.diff,https://github.com/apache/spark/pull/34378.patch,,https://api.github.com/repos/apache/spark/issues/34378/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
143,https://api.github.com/repos/apache/spark/issues/34367,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34367/labels{/name},https://api.github.com/repos/apache/spark/issues/34367/comments,https://api.github.com/repos/apache/spark/issues/34367/events,https://github.com/apache/spark/pull/34367,1033447009,PR_kwDOAQXtWs4tiv1X,34367,[SPARK-37099][SQL] Impl a rank-based filter to optimize top-k computation,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,62,2021-10-22T10:51:01Z,2021-12-31T07:07:34Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?
add a new node `RankLimit` to filter out uncessary rows based on rank computed on partial dataset.

it supports this pattern:

```
 select (... (row_number|rank|dense_rank)() over ( [partition by ...] order by ... ) as rn)
    where rn (==|<|<=) k and other conditions
```

For these three rank functions (row_number|rank|dense_rank), the rank of a key computed on partitial dataset  always  <=  its final rank computed on the whole dataset.

so we can safely discard rows with partitial rank > `k`, anywhere.



### Why are the changes needed?
1, reduce the shuffle write;
2, solve skewed-window problem, a practical case was optimized from 2.5h to 26min


### Does this PR introduce _any_ user-facing change?
a new config is added


### How was this patch tested?

1, added testsuits, practical cases on our production system

2, 10TiB TPC-DS - q67:

Before this PR | After this PR
--- | ---
Job Duration=58min|Job Duration=11min
Stage Duration=50min|Stage Duration=3sec
Stage Shuffle=58.0 GiB|Stage Shuffle=9.9 MiB
![image](https://user-images.githubusercontent.com/7322292/147652153-80890751-1c6d-4c54-8baf-1b036e829ca9.png)|![image](https://user-images.githubusercontent.com/7322292/147652272-128d3013-c2d0-4676-ab79-050d3349d0b2.png)
![image](https://user-images.githubusercontent.com/7322292/147808906-ed68e493-d0a3-4134-964a-a037721f4fbb.png)|![image](https://user-images.githubusercontent.com/7322292/147808939-a605f85a-bb31-49fa-9dd9-a9af23ec5df0.png)


3, added benchmark:

```

[info] Java HotSpot(TM) 64-Bit Server VM 1.8.0_301-b09 on Linux 5.11.0-41-generic
[info] Intel(R) Core(TM) i7-8850H CPU @ 2.60GHz
[info] Benchmark Top-K:                                      Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)   Relative
[info] ------------------------------------------------------------------------------------------------------------------------------------
[info] ROW_NUMBER WITHOUT PARTITION                                  10688          11377         664          2.0         509.6       1.0X
[info] ROW_NUMBER WITHOUT PARTITION (RANKLIMIT Sorting)               2678           2962         137          7.8         127.7       4.0X
[info] ROW_NUMBER WITHOUT PARTITION (RANKLIMIT TakeOrdered)           1585           1611          19         13.2          75.6       6.7X
[info] RANK WITHOUT PARTITION                                        11504          12056         406          1.8         548.6       0.9X
[info] RANK WITHOUT PARTITION (RANKLIMIT)                             3020           3148          89          6.9         144.0       3.5X
[info] DENSE_RANK WITHOUT PARTITION                                  11728          11915         216          1.8         559.3       0.9X
[info] DENSE_RANK WITHOUT PARTITION (RANKLIMIT)                       2632           2906         182          8.0         125.5       4.1X
[info] ROW_NUMBER WITH PARTITION                                     23139          24025         500          0.9        1103.4       0.5X
[info] ROW_NUMBER WITH PARTITION (RANKLIMIT Sorting)                  7034           7575         361          3.0         335.4       1.5X
[info] ROW_NUMBER WITH PARTITION (RANKLIMIT TakeOrdered)              5958           6391         311          3.5         284.1       1.8X
[info] RANK WITH PARTITION                                           24942          26005         795          0.8        1189.4       0.4X
[info] RANK WITH PARTITION (RANKLIMIT)                                7217           7517         219          2.9         344.1       1.5X
[info] DENSE_RANK WITH PARTITION                                     24843          26726         221          0.8        1184.6       0.4X
[info] DENSE_RANK WITH PARTITION (RANKLIMIT)                          7455           7978         560          2.8         355.5       1.4X
```",https://api.github.com/repos/apache/spark/issues/34367/timeline,,spark,apache,zhengruifeng,7322292,MDQ6VXNlcjczMjIyOTI=,https://avatars.githubusercontent.com/u/7322292?v=4,,https://api.github.com/users/zhengruifeng,https://github.com/zhengruifeng,https://api.github.com/users/zhengruifeng/followers,https://api.github.com/users/zhengruifeng/following{/other_user},https://api.github.com/users/zhengruifeng/gists{/gist_id},https://api.github.com/users/zhengruifeng/starred{/owner}{/repo},https://api.github.com/users/zhengruifeng/subscriptions,https://api.github.com/users/zhengruifeng/orgs,https://api.github.com/users/zhengruifeng/repos,https://api.github.com/users/zhengruifeng/events{/privacy},https://api.github.com/users/zhengruifeng/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34367,https://github.com/apache/spark/pull/34367,https://github.com/apache/spark/pull/34367.diff,https://github.com/apache/spark/pull/34367.patch,,https://api.github.com/repos/apache/spark/issues/34367/reactions,2,1,0,0,0,0,0,0,1,,,,,,,,,,,,,,,,,,
144,https://api.github.com/repos/apache/spark/issues/34366,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34366/labels{/name},https://api.github.com/repos/apache/spark/issues/34366/comments,https://api.github.com/repos/apache/spark/issues/34366/events,https://github.com/apache/spark/pull/34366,1033381850,PR_kwDOAQXtWs4tiibR,34366,[SPARK-37097][YARN] yarn-cluster mode don't need to retry when AM container exit code 0 but application failed.,"[{'id': 1427970157, 'node_id': 'MDU6TGFiZWwxNDI3OTcwMTU3', 'url': 'https://api.github.com/repos/apache/spark/labels/YARN', 'name': 'YARN', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,10,2021-10-22T09:37:33Z,2021-11-10T15:17:47Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?

Some yarn-cluster application meet such exception.
```
21/10/20 03:31:55 ERROR Client: Application diagnostics message: Application application_1632999510150_2163647 failed 1 times (global limit =8; local limit is =1) due to AM Container for appattempt_1632999510150_2163647_000001 exited with  exitCode: 0
Failing this attempt.Diagnostics: For more detailed output, check the application tracking page: http://ip-xx-xx-xx-xx.idata-server.shopee.io:8088/cluster/app/application_1632999510150_2163647 Then click on links to logs of each attempt.
. Failing the application.
Exception in thread ""main"" org.apache.spark.SparkException: Application application_1632999510150_2163647 finished with failed status
```

It's caused by below situation:
1. yarn-cluster mode application usr code finished, AM shutdown hook triggered
2. AM call unregister from RM but timeout, since AM shutdown hook have try catch, won't throw exception, so AM container exit with code 0(application user code running success).
3. Since RM lose connection with AM, then treat this container as failed final status.
4. Then client side got application report as final status failed but am container exit code 0. client treat it as failed, then retry.


it's a unnecessary retry. we can avoid it.


### Why are the changes needed?
Avoid unnecessary retry

### Does this PR introduce _any_ user-facing change?
No


### How was this patch tested?

Manual tested",https://api.github.com/repos/apache/spark/issues/34366/timeline,,spark,apache,AngersZhuuuu,46485123,MDQ6VXNlcjQ2NDg1MTIz,https://avatars.githubusercontent.com/u/46485123?v=4,,https://api.github.com/users/AngersZhuuuu,https://github.com/AngersZhuuuu,https://api.github.com/users/AngersZhuuuu/followers,https://api.github.com/users/AngersZhuuuu/following{/other_user},https://api.github.com/users/AngersZhuuuu/gists{/gist_id},https://api.github.com/users/AngersZhuuuu/starred{/owner}{/repo},https://api.github.com/users/AngersZhuuuu/subscriptions,https://api.github.com/users/AngersZhuuuu/orgs,https://api.github.com/users/AngersZhuuuu/repos,https://api.github.com/users/AngersZhuuuu/events{/privacy},https://api.github.com/users/AngersZhuuuu/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34366,https://github.com/apache/spark/pull/34366,https://github.com/apache/spark/pull/34366.diff,https://github.com/apache/spark/pull/34366.patch,,https://api.github.com/repos/apache/spark/issues/34366/reactions,1,1,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
145,https://api.github.com/repos/apache/spark/issues/34363,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34363/labels{/name},https://api.github.com/repos/apache/spark/issues/34363/comments,https://api.github.com/repos/apache/spark/issues/34363/events,https://github.com/apache/spark/pull/34363,1033166604,PR_kwDOAQXtWs4th3Eu,34363,[SPARK-37083][PYTHON] Inline type hints for python/pyspark/accumulators.py,"[{'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1982260031, 'node_id': 'MDU6TGFiZWwxOTgyMjYwMDMx', 'url': 'https://api.github.com/repos/apache/spark/labels/PYTHON', 'name': 'PYTHON', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,32,2021-10-22T04:26:57Z,2021-12-14T22:46:03Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?
Inline type hints for python/pyspark/accumulators.py

### Why are the changes needed?
We can take advantage of static type checking within the functions by inlining the type hints.

### Does this PR introduce _any_ user-facing change?
No

### How was this patch tested?
Existing tests",https://api.github.com/repos/apache/spark/issues/34363/timeline,,spark,apache,dchvn,91461145,MDQ6VXNlcjkxNDYxMTQ1,https://avatars.githubusercontent.com/u/91461145?v=4,,https://api.github.com/users/dchvn,https://github.com/dchvn,https://api.github.com/users/dchvn/followers,https://api.github.com/users/dchvn/following{/other_user},https://api.github.com/users/dchvn/gists{/gist_id},https://api.github.com/users/dchvn/starred{/owner}{/repo},https://api.github.com/users/dchvn/subscriptions,https://api.github.com/users/dchvn/orgs,https://api.github.com/users/dchvn/repos,https://api.github.com/users/dchvn/events{/privacy},https://api.github.com/users/dchvn/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34363,https://github.com/apache/spark/pull/34363,https://github.com/apache/spark/pull/34363.diff,https://github.com/apache/spark/pull/34363.patch,,https://api.github.com/repos/apache/spark/issues/34363/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
146,https://api.github.com/repos/apache/spark/issues/34362,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34362/labels{/name},https://api.github.com/repos/apache/spark/issues/34362/comments,https://api.github.com/repos/apache/spark/issues/34362/events,https://github.com/apache/spark/pull/34362,1033077696,PR_kwDOAQXtWs4thlEd,34362,[SPARK-37090][BUILD] Upgrade libthrift to avoid security vulnerabilities,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1406627200, 'node_id': 'MDU6TGFiZWwxNDA2NjI3MjAw', 'url': 'https://api.github.com/repos/apache/spark/labels/BUILD', 'name': 'BUILD', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,12,2021-10-22T01:07:03Z,2021-11-04T06:36:14Z,,MEMBER,,True,"### What changes were proposed in this pull request?

This pr backport HIVE-21498 to upgrade libthrift to 0.13.0.

### Why are the changes needed?

To addresses CVEs:

Component Name | Component Version Name | Vulnerability | Fixed version
-- | -- | -- | --
Apache Thrift | 0.11.0-4. | [CVE-2019-0205](https://nvd.nist.gov/vuln/detail/CVE-2019-0205) | 0.13.0
Apache Thrift | 0.11.0-4. | CVE-2019-0210 | 0.13.0

### Does this PR introduce _any_ user-facing change?

No.

### How was this patch tested?

Existing test.",https://api.github.com/repos/apache/spark/issues/34362/timeline,,spark,apache,wangyum,5399861,MDQ6VXNlcjUzOTk4NjE=,https://avatars.githubusercontent.com/u/5399861?v=4,,https://api.github.com/users/wangyum,https://github.com/wangyum,https://api.github.com/users/wangyum/followers,https://api.github.com/users/wangyum/following{/other_user},https://api.github.com/users/wangyum/gists{/gist_id},https://api.github.com/users/wangyum/starred{/owner}{/repo},https://api.github.com/users/wangyum/subscriptions,https://api.github.com/users/wangyum/orgs,https://api.github.com/users/wangyum/repos,https://api.github.com/users/wangyum/events{/privacy},https://api.github.com/users/wangyum/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34362,https://github.com/apache/spark/pull/34362,https://github.com/apache/spark/pull/34362.diff,https://github.com/apache/spark/pull/34362.patch,,https://api.github.com/repos/apache/spark/issues/34362/reactions,2,2,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
147,https://api.github.com/repos/apache/spark/issues/34359,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34359/labels{/name},https://api.github.com/repos/apache/spark/issues/34359/comments,https://api.github.com/repos/apache/spark/issues/34359/events,https://github.com/apache/spark/pull/34359,1032748808,PR_kwDOAQXtWs4tgfti,34359,[SPARK-36986][SQL] Improving external schema management flexibility on DataSet and StructType,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,1,2021-10-21T17:28:13Z,2021-12-14T09:58:51Z,,NONE,,False,"### What changes were proposed in this pull request?
These are the following proposed improvements:
1 - ability to retrieve from StructType, the field's name and schema in one single call, requesting to return a tupple by index. 
2 - Allowing for a dataset to be created from a schema, and passing the corresponding internal rows which the internal types map with the schema already defined externally. 


### Why are the changes needed?
Explanations provided for the respective changes mentioned earlier
1- Avoids two client calls/loops to obtain consolidated field info, when looping through a schema and updating field values on a genericRow data type.
2 - This allows to create Spark fields based on any data structure, without depending on Spark's internal conversions (in particular for Json parsing), and improves performance by skipping the CatalystConverts job of converting native Java types into Spark types.



### Does this PR introduce _any_ user-facing change?
This PR augments the current DataSet API and the StructType object manipulation. 


### How was this patch tested?
Unit tests were added (included in the PR), and bench tests were done locally to validate the performance of the new dataset API call. 
",https://api.github.com/repos/apache/spark/issues/34359/timeline,,spark,apache,risinga,3072864,MDQ6VXNlcjMwNzI4NjQ=,https://avatars.githubusercontent.com/u/3072864?v=4,,https://api.github.com/users/risinga,https://github.com/risinga,https://api.github.com/users/risinga/followers,https://api.github.com/users/risinga/following{/other_user},https://api.github.com/users/risinga/gists{/gist_id},https://api.github.com/users/risinga/starred{/owner}{/repo},https://api.github.com/users/risinga/subscriptions,https://api.github.com/users/risinga/orgs,https://api.github.com/users/risinga/repos,https://api.github.com/users/risinga/events{/privacy},https://api.github.com/users/risinga/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34359,https://github.com/apache/spark/pull/34359,https://github.com/apache/spark/pull/34359.diff,https://github.com/apache/spark/pull/34359.patch,,https://api.github.com/repos/apache/spark/issues/34359/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
148,https://api.github.com/repos/apache/spark/issues/34354,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34354/labels{/name},https://api.github.com/repos/apache/spark/issues/34354/comments,https://api.github.com/repos/apache/spark/issues/34354/events,https://github.com/apache/spark/pull/34354,1032266821,PR_kwDOAQXtWs4te6Yj,34354,"[SPARK-37085][PYTHON][SQL] Add list/tuple overloads to array, struct, create_map, map_concat","[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1982260031, 'node_id': 'MDU6TGFiZWwxOTgyMjYwMDMx', 'url': 'https://api.github.com/repos/apache/spark/labels/PYTHON', 'name': 'PYTHON', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,25,2021-10-21T09:29:22Z,2021-12-29T14:13:44Z,,MEMBER,,False,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->

This PR adds overloads to the following `pyspark.sql.functions`:

- `array`
- `struct`
- `create_map`
- `map_concat`

to support calls with a single `list` or `tuple` argument, i.e.

```python
array([""foo"", ""bar""])
```


### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->

These calls are supported by the current implementation, but don't type check.


### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->

Type checker only, as described above.

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.
-->

Existing tests and manual tests (to be added in SPARK-36989)",https://api.github.com/repos/apache/spark/issues/34354/timeline,,spark,apache,zero323,1554276,MDQ6VXNlcjE1NTQyNzY=,https://avatars.githubusercontent.com/u/1554276?v=4,,https://api.github.com/users/zero323,https://github.com/zero323,https://api.github.com/users/zero323/followers,https://api.github.com/users/zero323/following{/other_user},https://api.github.com/users/zero323/gists{/gist_id},https://api.github.com/users/zero323/starred{/owner}{/repo},https://api.github.com/users/zero323/subscriptions,https://api.github.com/users/zero323/orgs,https://api.github.com/users/zero323/repos,https://api.github.com/users/zero323/events{/privacy},https://api.github.com/users/zero323/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34354,https://github.com/apache/spark/pull/34354,https://github.com/apache/spark/pull/34354.diff,https://github.com/apache/spark/pull/34354.patch,,https://api.github.com/repos/apache/spark/issues/34354/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
149,https://api.github.com/repos/apache/spark/issues/34352,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34352/labels{/name},https://api.github.com/repos/apache/spark/issues/34352/comments,https://api.github.com/repos/apache/spark/issues/34352/events,https://github.com/apache/spark/pull/34352,1032186107,PR_kwDOAQXtWs4tepwm,34352,[SPARK-37018][SQL] Spark SQL should support create function with Aggregator,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,21,2021-10-21T08:06:29Z,2021-12-27T00:59:28Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?
Spark SQL not supports to create function of `Aggregator` yet and deprecated `UserDefinedAggregateFunction`.
If we want remove `UserDefinedAggregateFunction`, Spark SQL should provide a new option.
Note: This PR replaces https://github.com/apache/spark/pull/34303.


### Why are the changes needed?
We need to provide a new way to create user defined aggregate function so as remove `UserDefinedAggregateFunction` in future.


### Does this PR introduce _any_ user-facing change?
Yes. Users will create user defined aggregate function by implement `Aggregator`.


### How was this patch tested?
New tests.
",https://api.github.com/repos/apache/spark/issues/34352/timeline,,spark,apache,beliefer,8486025,MDQ6VXNlcjg0ODYwMjU=,https://avatars.githubusercontent.com/u/8486025?v=4,,https://api.github.com/users/beliefer,https://github.com/beliefer,https://api.github.com/users/beliefer/followers,https://api.github.com/users/beliefer/following{/other_user},https://api.github.com/users/beliefer/gists{/gist_id},https://api.github.com/users/beliefer/starred{/owner}{/repo},https://api.github.com/users/beliefer/subscriptions,https://api.github.com/users/beliefer/orgs,https://api.github.com/users/beliefer/repos,https://api.github.com/users/beliefer/events{/privacy},https://api.github.com/users/beliefer/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34352,https://github.com/apache/spark/pull/34352,https://github.com/apache/spark/pull/34352.diff,https://github.com/apache/spark/pull/34352.patch,,https://api.github.com/repos/apache/spark/issues/34352/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
150,https://api.github.com/repos/apache/spark/issues/34342,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34342/labels{/name},https://api.github.com/repos/apache/spark/issues/34342/comments,https://api.github.com/repos/apache/spark/issues/34342/events,https://github.com/apache/spark/pull/34342,1031725508,PR_kwDOAQXtWs4tdK8u,34342,[SPARK-37092][CORE][SQL] Add error class prefix to error message and enforce testing,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,9,2021-10-20T18:48:00Z,2021-10-22T00:49:26Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?

Adds the error class prefix to error messages and checks that error classes are encountered in tests.
Incidentally:
- Cleans up existing error classes that overlap in functionality
- Removes unused error classes
- Adds tests for untested error classes

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->


### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->


### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->
",https://api.github.com/repos/apache/spark/issues/34342/timeline,,spark,apache,karenfeng,4754931,MDQ6VXNlcjQ3NTQ5MzE=,https://avatars.githubusercontent.com/u/4754931?v=4,,https://api.github.com/users/karenfeng,https://github.com/karenfeng,https://api.github.com/users/karenfeng/followers,https://api.github.com/users/karenfeng/following{/other_user},https://api.github.com/users/karenfeng/gists{/gist_id},https://api.github.com/users/karenfeng/starred{/owner}{/repo},https://api.github.com/users/karenfeng/subscriptions,https://api.github.com/users/karenfeng/orgs,https://api.github.com/users/karenfeng/repos,https://api.github.com/users/karenfeng/events{/privacy},https://api.github.com/users/karenfeng/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34342,https://github.com/apache/spark/pull/34342,https://github.com/apache/spark/pull/34342.diff,https://github.com/apache/spark/pull/34342.patch,,https://api.github.com/repos/apache/spark/issues/34342/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
151,https://api.github.com/repos/apache/spark/issues/34339,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34339/labels{/name},https://api.github.com/repos/apache/spark/issues/34339/comments,https://api.github.com/repos/apache/spark/issues/34339/events,https://github.com/apache/spark/pull/34339,1031588532,PR_kwDOAQXtWs4tcuhD,34339,[SPARK-37074][SQL] Push extra predicates through non-join,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,4,2021-10-20T16:09:12Z,2021-10-21T05:35:08Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?
In the `Optimizer` partially push some predicates through a non-join nodes, that produce new columns: `Aggregate`, `Generate`, `Window`.

### Why are the changes needed?
Performance improvements

### Does this PR introduce _any_ user-facing change?
No

### How was this patch tested?
New UTs",https://api.github.com/repos/apache/spark/issues/34339/timeline,,spark,apache,tanelk,3342974,MDQ6VXNlcjMzNDI5NzQ=,https://avatars.githubusercontent.com/u/3342974?v=4,,https://api.github.com/users/tanelk,https://github.com/tanelk,https://api.github.com/users/tanelk/followers,https://api.github.com/users/tanelk/following{/other_user},https://api.github.com/users/tanelk/gists{/gist_id},https://api.github.com/users/tanelk/starred{/owner}{/repo},https://api.github.com/users/tanelk/subscriptions,https://api.github.com/users/tanelk/orgs,https://api.github.com/users/tanelk/repos,https://api.github.com/users/tanelk/events{/privacy},https://api.github.com/users/tanelk/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34339,https://github.com/apache/spark/pull/34339,https://github.com/apache/spark/pull/34339.diff,https://github.com/apache/spark/pull/34339.patch,,https://api.github.com/repos/apache/spark/issues/34339/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
152,https://api.github.com/repos/apache/spark/issues/34334,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34334/labels{/name},https://api.github.com/repos/apache/spark/issues/34334/comments,https://api.github.com/repos/apache/spark/issues/34334/events,https://github.com/apache/spark/pull/34334,1030889485,PR_kwDOAQXtWs4tadZi,34334,[SPARK-36763][SQL] Pull out complex sorting expressions,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,8,2021-10-20T01:56:29Z,2021-12-09T14:04:09Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?

This pr pull out complex sorting expressions if it is global sorting to reduce the evaluation of complex expressions. For example:
```sql
SELECT id AS a, id AS b FROM range(10) ORDER BY a - b
```
Before this pr:
```
== Physical Plan ==
AdaptiveSparkPlan isFinalPlan=false
+- Sort [(a#0L - b#1L) ASC NULLS FIRST], true, 0
   +- Exchange rangepartitioning((a#0L - b#1L) ASC NULLS FIRST, 5), ENSURE_REQUIREMENTS, [id=#12]
      +- Project [id#2L AS a#0L, id#2L AS b#1L]
         +- Range (0, 10, step=1, splits=2)
```
After this pr:
```
== Physical Plan ==
AdaptiveSparkPlan isFinalPlan=false
+- Project [a#0L, b#1L]
   +- Sort [_sortingexpression#5L ASC NULLS FIRST], true, 0
      +- Exchange rangepartitioning(_sortingexpression#5L ASC NULLS FIRST, 5), ENSURE_REQUIREMENTS, [id=#16]
         +- Project [id#2L AS a#0L, id#2L AS b#1L, (id#2L - id#2L) AS _sortingexpression#5L]
            +- Range (0, 10, step=1, splits=2)
```

### Why are the changes needed?

Improve order performance if the sorting expressions contains complex expressions.

### Does this PR introduce _any_ user-facing change?

No.


### How was this patch tested?

Unit test and benchmark test:
```scala
import org.apache.spark.benchmark.Benchmark
val numRows = 1024 * 1024 * 10
spark.sql(s""CREATE TABLE t1 using parquet AS select id AS a, id AS b FROM range(${numRows}L)"")
val benchmark = new Benchmark(""Benchmark pull out ordering expressions"", numRows, minNumIters = 5)

Seq(false, true).foreach { pullOutEnabled =>
  val name = s""Pull out ordering expressions ${if (pullOutEnabled) ""(Enabled)"" else ""(Disabled)""}""
  benchmark.addCase(name) { _ =>
    withSQLConf(""spark.sql.pullOutOrderingExpressions"" -> s""$pullOutEnabled"") {
      spark.sql(""SELECT t1.* FROM t1 ORDER BY translate(t1.a, '123', 'abc')"").write.format(""noop"").mode(""Overwrite"").save()
    }
  }
}
benchmark.run()
```
```
Java HotSpot(TM) 64-Bit Server VM 1.8.0_251-b08 on Mac OS X 10.15.7
Intel(R) Core(TM) i9-9980HK CPU @ 2.40GHz
Benchmark pull out ordering expressions:  Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)   Relative
------------------------------------------------------------------------------------------------------------------------
Pull out ordering expressions (Disabled)           9232           9753         867          1.1         880.4       1.0X
Pull out ordering expressions (Enabled)            7084           7462         370          1.5         675.5       1.3X
```",https://api.github.com/repos/apache/spark/issues/34334/timeline,,spark,apache,RabbidHY,90840965,MDQ6VXNlcjkwODQwOTY1,https://avatars.githubusercontent.com/u/90840965?v=4,,https://api.github.com/users/RabbidHY,https://github.com/RabbidHY,https://api.github.com/users/RabbidHY/followers,https://api.github.com/users/RabbidHY/following{/other_user},https://api.github.com/users/RabbidHY/gists{/gist_id},https://api.github.com/users/RabbidHY/starred{/owner}{/repo},https://api.github.com/users/RabbidHY/subscriptions,https://api.github.com/users/RabbidHY/orgs,https://api.github.com/users/RabbidHY/repos,https://api.github.com/users/RabbidHY/events{/privacy},https://api.github.com/users/RabbidHY/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34334,https://github.com/apache/spark/pull/34334,https://github.com/apache/spark/pull/34334.diff,https://github.com/apache/spark/pull/34334.patch,,https://api.github.com/repos/apache/spark/issues/34334/reactions,3,0,0,3,0,0,0,0,0,,,,,,,,,,,,,,,,,,
153,https://api.github.com/repos/apache/spark/issues/34326,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34326/labels{/name},https://api.github.com/repos/apache/spark/issues/34326/comments,https://api.github.com/repos/apache/spark/issues/34326/events,https://github.com/apache/spark/pull/34326,1029902759,PR_kwDOAQXtWs4tXXNr,34326,[SPARK-37053][CORE] Add metrics to SparkHistoryServer,"[{'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,51,2021-10-19T06:17:05Z,2021-12-22T09:42:29Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?
Add metrics system to history server.

|  Metrics  | Desc  |
|  ----  | ----  |
| HistoryServer.applications  | The size of all applications loaded in history server  |
| HistoryServer. incompleted  | The size of incompleted applications loaded in history server |
| HistoryServer. under.process  | The size of under process event log applications loaded in history server |
| HistoryServer. check.logs.timer  | The cost time of check event log |
| HistoryServer. clean.logs.timer  | The cost time of clean event log |
| HistoryServer. clean.driver.logs.timer  | The cost time of clean driver log |
| HistoryServer. compact.timer  | The cost time of compact event log |
| HistoryServer. loadStore.timer  | The cost time of load store log |
| ApplicationCache.history.cache.eviction.count | Application eviction count |
| ApplicationCache.history.cache.load.count | Application load cache count|
| ApplicationCache.history.cache.lookup.count | Application cache lookup count |
| ApplicationCache.history.cache.lookup.failure.count | Application look up failed count|
|  ApplicationCache.history.cache.load.timer | Application load time  | 


A demo metrics json:
```
{
  ""version"" : ""4.0.0"",
  ""gauges"" : {
    ""HistoryServer.applications"" : {
      ""value"" : 14
    },
    ""HistoryServer. incompleted"" : {
      ""value"" : 0
    },
    ""HistoryServer.under.process"" : {
      ""value"" : 0
    }
  },
  ""counters"" : {
    ""ApplicationCache.history.cache.eviction.count"" : {
      ""count"" : 0
    },
    ""ApplicationCache.history.cache.load.count"" : {
      ""count"" : 0
    },
    ""ApplicationCache.history.cache.lookup.count"" : {
      ""count"" : 0
    },
    ""ApplicationCache.history.cache.lookup.failure.count"" : {
      ""count"" : 0
    }
  },
  ""meters"" : { },
  ""timers"" : {
    ""ApplicationCache.history.cache.load.timer"" : {
      ""count"" : 0,
      ""max"" : 0.0,
      ""mean"" : 0.0,
      ""min"" : 0.0,
      ""p50"" : 0.0,
      ""p75"" : 0.0,
      ""p95"" : 0.0,
      ""p98"" : 0.0,
      ""p99"" : 0.0,
      ""p999"" : 0.0,
      ""stddev"" : 0.0,
      ""m15_rate"" : 0.0,
      ""m1_rate"" : 0.0,
      ""m5_rate"" : 0.0,
      ""mean_rate"" : 0.0,
      ""duration_units"" : ""milliseconds"",
      ""rate_units"" : ""calls/second""
    },
    ""HistoryServer.check.logs.timer"" : {
      ""count"" : 1,
      ""max"" : 735.628517,
      ""mean"" : 735.628517,
      ""min"" : 735.628517,
      ""p50"" : 735.628517,
      ""p75"" : 735.628517,
      ""p95"" : 735.628517,
      ""p98"" : 735.628517,
      ""p99"" : 735.628517,
      ""p999"" : 735.628517,
      ""stddev"" : 0.0,
      ""m15_rate"" : 0.0,
      ""m1_rate"" : 0.0,
      ""m5_rate"" : 0.0,
      ""mean_rate"" : 0.4121821730552311,
      ""duration_units"" : ""milliseconds"",
      ""rate_units"" : ""calls/second""
    },
    ""HistoryServer.clean.driver.logs.timer"" : {
      ""count"" : 0,
      ""max"" : 0.0,
      ""mean"" : 0.0,
      ""min"" : 0.0,
      ""p50"" : 0.0,
      ""p75"" : 0.0,
      ""p95"" : 0.0,
      ""p98"" : 0.0,
      ""p99"" : 0.0,
      ""p999"" : 0.0,
      ""stddev"" : 0.0,
      ""m15_rate"" : 0.0,
      ""m1_rate"" : 0.0,
      ""m5_rate"" : 0.0,
      ""mean_rate"" : 0.0,
      ""duration_units"" : ""milliseconds"",
      ""rate_units"" : ""calls/second""
    },
    ""HistoryServer.clean.logs.timer"" : {
      ""count"" : 0,
      ""max"" : 0.0,
      ""mean"" : 0.0,
      ""min"" : 0.0,
      ""p50"" : 0.0,
      ""p75"" : 0.0,
      ""p95"" : 0.0,
      ""p98"" : 0.0,
      ""p99"" : 0.0,
      ""p999"" : 0.0,
      ""stddev"" : 0.0,
      ""m15_rate"" : 0.0,
      ""m1_rate"" : 0.0,
      ""m5_rate"" : 0.0,
      ""mean_rate"" : 0.0,
      ""duration_units"" : ""milliseconds"",
      ""rate_units"" : ""calls/second""
    },
    ""HistoryServer.compact.timer"" : {
      ""count"" : 0,
      ""max"" : 0.0,
      ""mean"" : 0.0,
      ""min"" : 0.0,
      ""p50"" : 0.0,
      ""p75"" : 0.0,
      ""p95"" : 0.0,
      ""p98"" : 0.0,
      ""p99"" : 0.0,
      ""p999"" : 0.0,
      ""stddev"" : 0.0,
      ""m15_rate"" : 0.0,
      ""m1_rate"" : 0.0,
      ""m5_rate"" : 0.0,
      ""mean_rate"" : 0.0,
      ""duration_units"" : ""milliseconds"",
      ""rate_units"" : ""calls/second""
    },
    ""HistoryServer.load.store.timer"" : {
      ""count"" : 0,
      ""max"" : 0.0,
      ""mean"" : 0.0,
      ""min"" : 0.0,
      ""p50"" : 0.0,
      ""p75"" : 0.0,
      ""p95"" : 0.0,
      ""p98"" : 0.0,
      ""p99"" : 0.0,
      ""p999"" : 0.0,
      ""stddev"" : 0.0,
      ""m15_rate"" : 0.0,
      ""m1_rate"" : 0.0,
      ""m5_rate"" : 0.0,
      ""mean_rate"" : 0.0,
      ""duration_units"" : ""milliseconds"",
      ""rate_units"" : ""calls/second""
    }
  }
}

```


### Why are the changes needed?
Add metrics to History Server, with this patch we can plugin these metrics to any Dropwizard supported monitoring infra and get insights into the performance of SHS.


### Does this PR introduce _any_ user-facing change?
User can use history server monitor data

### How was this patch tested?
Added UT
",https://api.github.com/repos/apache/spark/issues/34326/timeline,,spark,apache,AngersZhuuuu,46485123,MDQ6VXNlcjQ2NDg1MTIz,https://avatars.githubusercontent.com/u/46485123?v=4,,https://api.github.com/users/AngersZhuuuu,https://github.com/AngersZhuuuu,https://api.github.com/users/AngersZhuuuu/followers,https://api.github.com/users/AngersZhuuuu/following{/other_user},https://api.github.com/users/AngersZhuuuu/gists{/gist_id},https://api.github.com/users/AngersZhuuuu/starred{/owner}{/repo},https://api.github.com/users/AngersZhuuuu/subscriptions,https://api.github.com/users/AngersZhuuuu/orgs,https://api.github.com/users/AngersZhuuuu/repos,https://api.github.com/users/AngersZhuuuu/events{/privacy},https://api.github.com/users/AngersZhuuuu/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34326,https://github.com/apache/spark/pull/34326,https://github.com/apache/spark/pull/34326.diff,https://github.com/apache/spark/pull/34326.patch,,https://api.github.com/repos/apache/spark/issues/34326/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
154,https://api.github.com/repos/apache/spark/issues/34324,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34324/labels{/name},https://api.github.com/repos/apache/spark/issues/34324/comments,https://api.github.com/repos/apache/spark/issues/34324/events,https://github.com/apache/spark/pull/34324,1029888542,PR_kwDOAQXtWs4tXUbT,34324,[SPARK-37015][PYTHON] Inline type hints for python/pyspark/streaming/dstream.py,"[{'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1982260031, 'node_id': 'MDU6TGFiZWwxOTgyMjYwMDMx', 'url': 'https://api.github.com/repos/apache/spark/labels/PYTHON', 'name': 'PYTHON', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1984378631, 'node_id': 'MDU6TGFiZWwxOTg0Mzc4NjMx', 'url': 'https://api.github.com/repos/apache/spark/labels/DSTREAM', 'name': 'DSTREAM', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,19,2021-10-19T05:53:39Z,2021-11-11T12:08:08Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?
Inline type hints for python/pyspark/streaming/dstream.py

### Why are the changes needed?
We can take advantage of static type checking within the functions by inlining the type hints.

### Does this PR introduce _any_ user-facing change?
No

### How was this patch tested?
Existing tests",https://api.github.com/repos/apache/spark/issues/34324/timeline,,spark,apache,dchvn,91461145,MDQ6VXNlcjkxNDYxMTQ1,https://avatars.githubusercontent.com/u/91461145?v=4,,https://api.github.com/users/dchvn,https://github.com/dchvn,https://api.github.com/users/dchvn/followers,https://api.github.com/users/dchvn/following{/other_user},https://api.github.com/users/dchvn/gists{/gist_id},https://api.github.com/users/dchvn/starred{/owner}{/repo},https://api.github.com/users/dchvn/subscriptions,https://api.github.com/users/dchvn/orgs,https://api.github.com/users/dchvn/repos,https://api.github.com/users/dchvn/events{/privacy},https://api.github.com/users/dchvn/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34324,https://github.com/apache/spark/pull/34324,https://github.com/apache/spark/pull/34324.diff,https://github.com/apache/spark/pull/34324.patch,,https://api.github.com/repos/apache/spark/issues/34324/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
155,https://api.github.com/repos/apache/spark/issues/34320,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34320/labels{/name},https://api.github.com/repos/apache/spark/issues/34320/comments,https://api.github.com/repos/apache/spark/issues/34320/events,https://github.com/apache/spark/pull/34320,1029793022,PR_kwDOAQXtWs4tXBxP,34320,[SPARK-18621][PYTHON] make sql type reprs eval-able,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1405801475, 'node_id': 'MDU6TGFiZWwxNDA1ODAxNDc1', 'url': 'https://api.github.com/repos/apache/spark/labels/ML', 'name': 'ML', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1982260031, 'node_id': 'MDU6TGFiZWwxOTgyMjYwMDMx', 'url': 'https://api.github.com/repos/apache/spark/labels/PYTHON', 'name': 'PYTHON', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,24,2021-10-19T02:42:38Z,2021-12-29T18:25:18Z,,NONE,,False,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->
These changes update the `__repr__` methods of type classes in `pyspark.sql.types` to print string representations which are `eval`-able. In other words, any instance of a `DataType` will produce a repr which can be passed to `eval()` to create an identical instance.

Similar changes previously submitted: https://github.com/apache/spark/pull/25495

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->
This [bug](https://issues.apache.org/jira/browse/SPARK-18621) has been around for a while. The current implementation returns a string representation which is valid in scala rather than python. These changes fix the repr to be valid with python.

The [motivation](https://docs.python.org/3/library/functions.html#repr) is ""to return a string that would yield an object with the same value when passed to eval()"".

### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->

Example:

Current implementation:

```python
from pyspark.sql.types import *

struct = StructType([StructField('f1', StringType(), True)])
repr(struct)
# StructType(List(StructField(f1,StringType,true)))
new_struct = eval(repr(struct))
# Traceback (most recent call last):
#   File ""<input>"", line 1, in <module>
#   File ""<string>"", line 1, in <module>
# NameError: name 'List' is not defined

struct_field = StructField('f1', StringType(), True)
repr(struct_field)
# StructField(f1,StringType,true)
new_struct_field = eval(repr(struct_field))
# Traceback (most recent call last):
#   File ""<input>"", line 1, in <module>
#   File ""<string>"", line 1, in <module>
# NameError: name 'f1' is not defined
```

With changes:

```python
from pyspark.sql.types import *

struct = StructType([StructField('f1', StringType(), True)])
repr(struct)
# StructType([StructField('f1', StringType(), True)])
new_struct = eval(repr(struct))
struct == new_struct
# True

struct_field = StructField('f1', StringType(), True)
repr(struct_field)
# StructField('f1', StringType(), True)
new_struct_field = eval(repr(struct_field))
struct_field == new_struct_field
# True
```

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->
The changes include a test which asserts that an instance of each type is equal to the `eval` of its `repr`, as in the above example.
",https://api.github.com/repos/apache/spark/issues/34320/timeline,,spark,apache,crflynn,16403734,MDQ6VXNlcjE2NDAzNzM0,https://avatars.githubusercontent.com/u/16403734?v=4,,https://api.github.com/users/crflynn,https://github.com/crflynn,https://api.github.com/users/crflynn/followers,https://api.github.com/users/crflynn/following{/other_user},https://api.github.com/users/crflynn/gists{/gist_id},https://api.github.com/users/crflynn/starred{/owner}{/repo},https://api.github.com/users/crflynn/subscriptions,https://api.github.com/users/crflynn/orgs,https://api.github.com/users/crflynn/repos,https://api.github.com/users/crflynn/events{/privacy},https://api.github.com/users/crflynn/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34320,https://github.com/apache/spark/pull/34320,https://github.com/apache/spark/pull/34320.diff,https://github.com/apache/spark/pull/34320.patch,,https://api.github.com/repos/apache/spark/issues/34320/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
156,https://api.github.com/repos/apache/spark/issues/34316,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34316/labels{/name},https://api.github.com/repos/apache/spark/issues/34316/comments,https://api.github.com/repos/apache/spark/issues/34316/events,https://github.com/apache/spark/pull/34316,1029034299,PR_kwDOAQXtWs4tUsSZ,34316,[SPARK-37043][SQL] Cancel all running job after AQE plan finished,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,7,2021-10-18T12:13:21Z,2021-11-30T12:08:56Z,,CONTRIBUTOR,,True,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->
Cancel running job after AQE plan finished, so this PR add a `runningStages` in `AdaptiveExecutionContext` to record the running stages.

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->
We see stage was still running after AQE plan finished. This is because the plan which contains a join with one empty side has been converted to `LocalTableScanExec` during `AQEOptimizer`, but the other side of this join is still running (shuffle map stage).

It's no meaning to keep running the stage, so It's better to cancel the running stage after AQE plan finished in case wasting the task resource.

### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->
no

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->
add test.",https://api.github.com/repos/apache/spark/issues/34316/timeline,,spark,apache,ulysses-you,12025282,MDQ6VXNlcjEyMDI1Mjgy,https://avatars.githubusercontent.com/u/12025282?v=4,,https://api.github.com/users/ulysses-you,https://github.com/ulysses-you,https://api.github.com/users/ulysses-you/followers,https://api.github.com/users/ulysses-you/following{/other_user},https://api.github.com/users/ulysses-you/gists{/gist_id},https://api.github.com/users/ulysses-you/starred{/owner}{/repo},https://api.github.com/users/ulysses-you/subscriptions,https://api.github.com/users/ulysses-you/orgs,https://api.github.com/users/ulysses-you/repos,https://api.github.com/users/ulysses-you/events{/privacy},https://api.github.com/users/ulysses-you/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34316,https://github.com/apache/spark/pull/34316,https://github.com/apache/spark/pull/34316.diff,https://github.com/apache/spark/pull/34316.patch,,https://api.github.com/repos/apache/spark/issues/34316/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
157,https://api.github.com/repos/apache/spark/issues/34302,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34302/labels{/name},https://api.github.com/repos/apache/spark/issues/34302/comments,https://api.github.com/repos/apache/spark/issues/34302/events,https://github.com/apache/spark/pull/34302,1028228593,PR_kwDOAQXtWs4tSUGv,34302,[SPARK-37028][UI] Add a 'kill' executor link in the Web UI.,"[{'id': 1406605079, 'node_id': 'MDU6TGFiZWwxNDA2NjA1MDc5', 'url': 'https://api.github.com/repos/apache/spark/labels/WEB%20UI', 'name': 'WEB UI', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,7,2021-10-17T06:11:39Z,2021-11-04T06:32:43Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?
 Add a 'kill' executor link in the Web UI,  so it's easier for users to kill executors in the UI.

### Why are the changes needed?
The executor which is running in a bad node(eg. The system is overloaded or disks are busy) or  has big GC overheads may affect the efficiency of job execution, although there are speculative mechanisms to resolve this problem, but sometimes the speculated task may also run in a bad executor.
We should have a ""kill"" link for each executor, similar to what we have for each stage, so it's easier for users to kill executors in the UI.

### Does this PR introduce _any_ user-facing change?
Yes,  user can  kill executors via the Web UI.
![image](https://user-images.githubusercontent.com/39684231/137618317-752fec71-6db7-4c75-a6f3-7a5644769cec.png)


### How was this patch tested?

Add unittests.",https://api.github.com/repos/apache/spark/issues/34302/timeline,,spark,apache,weixiuli,39684231,MDQ6VXNlcjM5Njg0MjMx,https://avatars.githubusercontent.com/u/39684231?v=4,,https://api.github.com/users/weixiuli,https://github.com/weixiuli,https://api.github.com/users/weixiuli/followers,https://api.github.com/users/weixiuli/following{/other_user},https://api.github.com/users/weixiuli/gists{/gist_id},https://api.github.com/users/weixiuli/starred{/owner}{/repo},https://api.github.com/users/weixiuli/subscriptions,https://api.github.com/users/weixiuli/orgs,https://api.github.com/users/weixiuli/repos,https://api.github.com/users/weixiuli/events{/privacy},https://api.github.com/users/weixiuli/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34302,https://github.com/apache/spark/pull/34302,https://github.com/apache/spark/pull/34302.diff,https://github.com/apache/spark/pull/34302.patch,,https://api.github.com/repos/apache/spark/issues/34302/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
158,https://api.github.com/repos/apache/spark/issues/34293,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34293/labels{/name},https://api.github.com/repos/apache/spark/issues/34293/comments,https://api.github.com/repos/apache/spark/issues/34293/events,https://github.com/apache/spark/pull/34293,1027193162,PR_kwDOAQXtWs4tPWpA,34293,[SPARK-37014][PYTHON] Inline type hints for python/pyspark/streaming/context.py,"[{'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1982260031, 'node_id': 'MDU6TGFiZWwxOTgyMjYwMDMx', 'url': 'https://api.github.com/repos/apache/spark/labels/PYTHON', 'name': 'PYTHON', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1984378631, 'node_id': 'MDU6TGFiZWwxOTg0Mzc4NjMx', 'url': 'https://api.github.com/repos/apache/spark/labels/DSTREAM', 'name': 'DSTREAM', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,15,2021-10-15T08:11:00Z,2021-11-11T12:08:08Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?
Inline type hints for python/pyspark/streaming/context.py from Inline type hints for python/pyspark/streaming/context.pyi.

### Why are the changes needed?
Currently, there is type hint stub files python/pyspark/streaming/context.pyi to show the expected types for functions, but we can also take advantage of static type checking within the functions by inlining the type hints.

### Does this PR introduce _any_ user-facing change?
No

### How was this patch tested?
Existing test.",https://api.github.com/repos/apache/spark/issues/34293/timeline,,spark,apache,dchvn,91461145,MDQ6VXNlcjkxNDYxMTQ1,https://avatars.githubusercontent.com/u/91461145?v=4,,https://api.github.com/users/dchvn,https://github.com/dchvn,https://api.github.com/users/dchvn/followers,https://api.github.com/users/dchvn/following{/other_user},https://api.github.com/users/dchvn/gists{/gist_id},https://api.github.com/users/dchvn/starred{/owner}{/repo},https://api.github.com/users/dchvn/subscriptions,https://api.github.com/users/dchvn/orgs,https://api.github.com/users/dchvn/repos,https://api.github.com/users/dchvn/events{/privacy},https://api.github.com/users/dchvn/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34293,https://github.com/apache/spark/pull/34293,https://github.com/apache/spark/pull/34293.diff,https://github.com/apache/spark/pull/34293.patch,,https://api.github.com/repos/apache/spark/issues/34293/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
159,https://api.github.com/repos/apache/spark/issues/34290,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34290/labels{/name},https://api.github.com/repos/apache/spark/issues/34290/comments,https://api.github.com/repos/apache/spark/issues/34290/events,https://github.com/apache/spark/pull/34290,1027083451,PR_kwDOAQXtWs4tPBd2,34290,[SPARK-37016][SQL] Publicise UpperCaseCharStream,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,2,2021-10-15T05:00:55Z,2021-10-21T02:52:51Z,,CONTRIBUTOR,,False,"What changes were proposed in this pull request?
Publicise `UpperCaseCharStream`

Why are the changes needed?
Many Spark extension projects are copying `UpperCaseCharStream` because it is private beneath `parser` package, such as:
[Delta Lake](https://github.com/delta-io/delta/blob/625de3b305f109441ad04b20dba91dd6c4e1d78e/core/src/main/scala/io/delta/sql/parser/DeltaSqlParser.scala#L290)
[Hudi](https://github.com/apache/hudi/blob/3f8ca1a3552bb866163d3b1648f68d9c4824e21d/hudi-spark-datasource/hudi-spark/src/main/scala/org/apache/spark/sql/parser/HoodieCommonSqlParser.scala#L112)
[Iceberg](https://github.com/apache/iceberg/blob/c3ac4c6ca74a0013b4705d5bd5d17fade8e6f499/spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/parser/extensions/IcebergSparkSqlExtensionsParser.scala#L175)
[Submarine](https://github.com/apache/submarine/blob/2faebb8efd69833853f62d89b4f1fea1b1718148/submarine-security/spark-security/src/main/scala/org/apache/submarine/spark/security/parser/UpperCaseCharStream.scala#L31)
[Kyuubi](https://github.com/apache/incubator-kyuubi/blob/8a5134e3223844714fc58833a6859d4df5b68d57/dev/kyuubi-extension-spark-common/src/main/scala/org/apache/kyuubi/sql/zorder/ZorderSparkSqlExtensionsParserBase.scala#L108)
[Spark-ACID](https://github.com/qubole/spark-acid/blob/19bd6db757677c40f448e85c74d9995ba97d5942/src/main/scala/com/qubole/spark/datasources/hiveacid/sql/catalyst/parser/ParseDriver.scala#L13)
We can publicise `UpperCaseCharStream` to eliminate code duplication.

Does this PR introduce any user-facing change?
no

How was this patch tested?
existing tests",https://api.github.com/repos/apache/spark/issues/34290/timeline,,spark,apache,dohongdayi,1321316,MDQ6VXNlcjEzMjEzMTY=,https://avatars.githubusercontent.com/u/1321316?v=4,,https://api.github.com/users/dohongdayi,https://github.com/dohongdayi,https://api.github.com/users/dohongdayi/followers,https://api.github.com/users/dohongdayi/following{/other_user},https://api.github.com/users/dohongdayi/gists{/gist_id},https://api.github.com/users/dohongdayi/starred{/owner}{/repo},https://api.github.com/users/dohongdayi/subscriptions,https://api.github.com/users/dohongdayi/orgs,https://api.github.com/users/dohongdayi/repos,https://api.github.com/users/dohongdayi/events{/privacy},https://api.github.com/users/dohongdayi/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34290,https://github.com/apache/spark/pull/34290,https://github.com/apache/spark/pull/34290.diff,https://github.com/apache/spark/pull/34290.patch,,https://api.github.com/repos/apache/spark/issues/34290/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
160,https://api.github.com/repos/apache/spark/issues/34272,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34272/labels{/name},https://api.github.com/repos/apache/spark/issues/34272/comments,https://api.github.com/repos/apache/spark/issues/34272/events,https://github.com/apache/spark/pull/34272,1025081600,PR_kwDOAQXtWs4tIu5i,34272,"[WIP][SPARK-36996][SQL] Fixing ""SQL column nullable setting not retained as part of spark.read"" issue","[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,7,2021-10-13T10:39:33Z,2021-10-16T17:44:38Z,,CONTRIBUTOR,,False,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->
Changes included in this PR:
1.  alwaysNullable value as ""False"" in getQueryOutputSchema function of JDBCRDD.scala so all column Nullable value will be fetched from SQL metadata.
2. pass ""nullable"" value as true  if column typeName is ""TIMESTAMP"" ; this changes is included in order to satisfy issue reported in SPARK-19726.

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->
We require this changes in order to fetch exact Nullable value from SQL metadata otherwise all columns be treated as NULLABLE.

### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->
No

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->
1. Tested in Internal Cluster
2. Used available Test Cases",https://api.github.com/repos/apache/spark/issues/34272/timeline,,spark,apache,senthh,9917543,MDQ6VXNlcjk5MTc1NDM=,https://avatars.githubusercontent.com/u/9917543?v=4,,https://api.github.com/users/senthh,https://github.com/senthh,https://api.github.com/users/senthh/followers,https://api.github.com/users/senthh/following{/other_user},https://api.github.com/users/senthh/gists{/gist_id},https://api.github.com/users/senthh/starred{/owner}{/repo},https://api.github.com/users/senthh/subscriptions,https://api.github.com/users/senthh/orgs,https://api.github.com/users/senthh/repos,https://api.github.com/users/senthh/events{/privacy},https://api.github.com/users/senthh/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34272,https://github.com/apache/spark/pull/34272,https://github.com/apache/spark/pull/34272.diff,https://github.com/apache/spark/pull/34272.patch,,https://api.github.com/repos/apache/spark/issues/34272/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
161,https://api.github.com/repos/apache/spark/issues/34264,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34264/labels{/name},https://api.github.com/repos/apache/spark/issues/34264/comments,https://api.github.com/repos/apache/spark/issues/34264/events,https://github.com/apache/spark/pull/34264,1024195363,PR_kwDOAQXtWs4tF2j7,34264,[SPARK-36462][K8S] Add the ability to selectively disable watching or polling,"[{'id': 1406605057, 'node_id': 'MDU6TGFiZWwxNDA2NjA1MDU3', 'url': 'https://api.github.com/repos/apache/spark/labels/KUBERNETES', 'name': 'KUBERNETES', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,11,2021-10-12T19:17:23Z,2021-12-01T07:35:32Z,,CONTRIBUTOR,,False,"
### What changes were proposed in this pull request?

Add the ability to selectively disable watching or polling

### Why are the changes needed?

Watching or polling for pod status on Kubernetes can place additional load on etcd, with a large number of executors and large number of jobs this can have negative impacts and executors register themselves with the driver under normal operations anyways.

### Does this PR introduce _any_ user-facing change?

Two new config flags.


### How was this patch tested?

New unit tests + manually tested a forked version of this on an internal cluster with both watching and polling disabled.",https://api.github.com/repos/apache/spark/issues/34264/timeline,,spark,apache,holdenk,59893,MDQ6VXNlcjU5ODkz,https://avatars.githubusercontent.com/u/59893?v=4,,https://api.github.com/users/holdenk,https://github.com/holdenk,https://api.github.com/users/holdenk/followers,https://api.github.com/users/holdenk/following{/other_user},https://api.github.com/users/holdenk/gists{/gist_id},https://api.github.com/users/holdenk/starred{/owner}{/repo},https://api.github.com/users/holdenk/subscriptions,https://api.github.com/users/holdenk/orgs,https://api.github.com/users/holdenk/repos,https://api.github.com/users/holdenk/events{/privacy},https://api.github.com/users/holdenk/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34264,https://github.com/apache/spark/pull/34264,https://github.com/apache/spark/pull/34264.diff,https://github.com/apache/spark/pull/34264.patch,,https://api.github.com/repos/apache/spark/issues/34264/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
162,https://api.github.com/repos/apache/spark/issues/34234,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34234/labels{/name},https://api.github.com/repos/apache/spark/issues/34234/comments,https://api.github.com/repos/apache/spark/issues/34234/events,https://github.com/apache/spark/pull/34234,1021860139,PR_kwDOAQXtWs4s-pXK,34234,[SPARK-36967][CORE] Report accurate shuffle block size if its skewed,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,23,2021-10-10T02:28:30Z,2021-12-21T19:23:18Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?

A shuffle block is considered as skewed and will be accurately recorded in HighlyCompressedMapStatus if its size if larger than this factor multiplying  the median shuffle block size.

Before this change 

![map_status_before](https://user-images.githubusercontent.com/3626747/137251903-08a3544c-dc77-4b78-8ae5-93b42a54bd03.png)

After this change

![map_status_after](https://user-images.githubusercontent.com/3626747/137251871-355db24d-d66b-4702-8766-216db30a39e0.jpg)

### Why are the changes needed?

Now map task will report accurate shuffle block size if the block size is greater than ""spark.shuffle.accurateBlockThreshold""( 100M by default ). But if there are a large number of map tasks and the shuffle block sizes of these tasks are smaller than ""spark.shuffle.accurateBlockThreshold"", there may be unrecognized data skew.

For example, there are 10000 map task and 10000 reduce task, and each map task create 50M shuffle blocks for reduce 0, and 10K shuffle blocks for the left reduce tasks, reduce 0 is data skew, but the stat of this plan do not have this information.

### Does this PR introduce _any_ user-facing change?
No

### How was this patch tested?

Update exists UTs
",https://api.github.com/repos/apache/spark/issues/34234/timeline,,spark,apache,wankunde,3626747,MDQ6VXNlcjM2MjY3NDc=,https://avatars.githubusercontent.com/u/3626747?v=4,,https://api.github.com/users/wankunde,https://github.com/wankunde,https://api.github.com/users/wankunde/followers,https://api.github.com/users/wankunde/following{/other_user},https://api.github.com/users/wankunde/gists{/gist_id},https://api.github.com/users/wankunde/starred{/owner}{/repo},https://api.github.com/users/wankunde/subscriptions,https://api.github.com/users/wankunde/orgs,https://api.github.com/users/wankunde/repos,https://api.github.com/users/wankunde/events{/privacy},https://api.github.com/users/wankunde/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34234,https://github.com/apache/spark/pull/34234,https://github.com/apache/spark/pull/34234.diff,https://github.com/apache/spark/pull/34234.patch,,https://api.github.com/repos/apache/spark/issues/34234/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
163,https://api.github.com/repos/apache/spark/issues/34231,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34231/labels{/name},https://api.github.com/repos/apache/spark/issues/34231/comments,https://api.github.com/repos/apache/spark/issues/34231/events,https://github.com/apache/spark/pull/34231,1021635960,PR_kwDOAQXtWs4s-Dvi,34231,[SPARK-36964][CORE][YARN] Share cached dnsToSwitchMapping for yarn locality container requests,"[{'id': 1427970157, 'node_id': 'MDU6TGFiZWwxNDI3OTcwMTU3', 'url': 'https://api.github.com/repos/apache/spark/labels/YARN', 'name': 'YARN', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,7,2021-10-09T07:41:22Z,2021-12-03T13:43:56Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?

since SPARK-13704, spark re-implemented RackResolver and created a separate dnsToSwitchMapping instance. Here change to use RackResolver's internal dnsToSwitchMapping to reduce the time taken by YarnAllocator to add container requests.

### Why are the changes needed?

If submits a stage with abundant tasks, rack resolving takes a long time when YarnAllocator add requests with locality preference, which is caused by a large number of loops to execute the rack parsing script, eventually causing ExecutorAllocationManager request total Executors rpc  timeout.

### How was this patch tested?
UT +  manually testing on a 5w+ node cluster.",https://api.github.com/repos/apache/spark/issues/34231/timeline,,spark,apache,gaoyajun02,81629032,MDQ6VXNlcjgxNjI5MDMy,https://avatars.githubusercontent.com/u/81629032?v=4,,https://api.github.com/users/gaoyajun02,https://github.com/gaoyajun02,https://api.github.com/users/gaoyajun02/followers,https://api.github.com/users/gaoyajun02/following{/other_user},https://api.github.com/users/gaoyajun02/gists{/gist_id},https://api.github.com/users/gaoyajun02/starred{/owner}{/repo},https://api.github.com/users/gaoyajun02/subscriptions,https://api.github.com/users/gaoyajun02/orgs,https://api.github.com/users/gaoyajun02/repos,https://api.github.com/users/gaoyajun02/events{/privacy},https://api.github.com/users/gaoyajun02/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34231,https://github.com/apache/spark/pull/34231,https://github.com/apache/spark/pull/34231.diff,https://github.com/apache/spark/pull/34231.patch,,https://api.github.com/repos/apache/spark/issues/34231/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
164,https://api.github.com/repos/apache/spark/issues/34223,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34223/labels{/name},https://api.github.com/repos/apache/spark/issues/34223/comments,https://api.github.com/repos/apache/spark/issues/34223/events,https://github.com/apache/spark/pull/34223,1020990040,PR_kwDOAQXtWs4s8GTn,34223,[SPARK-36957][SQL] Add Aggregate function Product to SQL function,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,14,2021-10-08T11:25:41Z,2021-10-11T08:42:35Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?
Add Aggregate function Product to SQL function


### Why are the changes needed?
We have support Aggregate function in DSL and  DataFrame API, we need to support it in SQL query too.


### Does this PR introduce _any_ user-facing change?
User can use aggregate function `product` in SQL


### How was this patch tested?
UT
",https://api.github.com/repos/apache/spark/issues/34223/timeline,,spark,apache,AngersZhuuuu,46485123,MDQ6VXNlcjQ2NDg1MTIz,https://avatars.githubusercontent.com/u/46485123?v=4,,https://api.github.com/users/AngersZhuuuu,https://github.com/AngersZhuuuu,https://api.github.com/users/AngersZhuuuu/followers,https://api.github.com/users/AngersZhuuuu/following{/other_user},https://api.github.com/users/AngersZhuuuu/gists{/gist_id},https://api.github.com/users/AngersZhuuuu/starred{/owner}{/repo},https://api.github.com/users/AngersZhuuuu/subscriptions,https://api.github.com/users/AngersZhuuuu/orgs,https://api.github.com/users/AngersZhuuuu/repos,https://api.github.com/users/AngersZhuuuu/events{/privacy},https://api.github.com/users/AngersZhuuuu/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34223,https://github.com/apache/spark/pull/34223,https://github.com/apache/spark/pull/34223.diff,https://github.com/apache/spark/pull/34223.patch,,https://api.github.com/repos/apache/spark/issues/34223/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
165,https://api.github.com/repos/apache/spark/issues/34212,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34212/labels{/name},https://api.github.com/repos/apache/spark/issues/34212/comments,https://api.github.com/repos/apache/spark/issues/34212/events,https://github.com/apache/spark/pull/34212,1019923504,PR_kwDOAQXtWs4s4xo1,34212,[SPARK-36402][PYTHON] Implement Series.combine,"[{'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1982260031, 'node_id': 'MDU6TGFiZWwxOTgyMjYwMDMx', 'url': 'https://api.github.com/repos/apache/spark/labels/PYTHON', 'name': 'PYTHON', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,17,2021-10-07T11:17:52Z,2021-12-17T02:29:29Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?

Implement Series.combine

### Why are the changes needed?

Increase pandas API coverage in PySpark
### Does this PR introduce _any_ user-facing change?

User can use

```python
>>> s1 = ps.Series({'falcon': 330.0, 'eagle': 160.0})
>>> s1
falcon    330.0
eagle     160.0
dtype: float64

>>> s2 = ps.Series({'falcon': 345.0, 'eagle': 200.0, 'duck': 30.0})
>>> s2
falcon    345.0
eagle     200.0
duck       30.0
dtype: float64

>>> s1.combine(s2, max)
duck        NaN
eagle     200.0
falcon    345.0
dtype: float64
```
### How was this patch tested?

unit tests and docstest",https://api.github.com/repos/apache/spark/issues/34212/timeline,,spark,apache,dchvn,91461145,MDQ6VXNlcjkxNDYxMTQ1,https://avatars.githubusercontent.com/u/91461145?v=4,,https://api.github.com/users/dchvn,https://github.com/dchvn,https://api.github.com/users/dchvn/followers,https://api.github.com/users/dchvn/following{/other_user},https://api.github.com/users/dchvn/gists{/gist_id},https://api.github.com/users/dchvn/starred{/owner}{/repo},https://api.github.com/users/dchvn/subscriptions,https://api.github.com/users/dchvn/orgs,https://api.github.com/users/dchvn/repos,https://api.github.com/users/dchvn/events{/privacy},https://api.github.com/users/dchvn/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34212,https://github.com/apache/spark/pull/34212,https://github.com/apache/spark/pull/34212.diff,https://github.com/apache/spark/pull/34212.patch,,https://api.github.com/repos/apache/spark/issues/34212/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
166,https://api.github.com/repos/apache/spark/issues/34208,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34208/labels{/name},https://api.github.com/repos/apache/spark/issues/34208/comments,https://api.github.com/repos/apache/spark/issues/34208/events,https://github.com/apache/spark/pull/34208,1019608364,PR_kwDOAQXtWs4s3yoQ,34208,[SPARK-36304][SQL] Refactor fifteenth set of 20 query execution errors to use error classes,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1406587328, 'node_id': 'MDU6TGFiZWwxNDA2NTg3MzI4', 'url': 'https://api.github.com/repos/apache/spark/labels/STRUCTURED%20STREAMING', 'name': 'STRUCTURED STREAMING', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,7,2021-10-07T04:43:04Z,2021-10-15T17:35:06Z,,CONTRIBUTOR,,False,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->Adds error classes to some of the exceptions in QueryExecutionErrors.


### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->Improves auditing for developers and adds useful fields for users (error class and SQLSTATE).


### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->Fills in missing error class and SQLSTATE fields.


### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->Existing tests
",https://api.github.com/repos/apache/spark/issues/34208/timeline,,spark,apache,dchvn,91461145,MDQ6VXNlcjkxNDYxMTQ1,https://avatars.githubusercontent.com/u/91461145?v=4,,https://api.github.com/users/dchvn,https://github.com/dchvn,https://api.github.com/users/dchvn/followers,https://api.github.com/users/dchvn/following{/other_user},https://api.github.com/users/dchvn/gists{/gist_id},https://api.github.com/users/dchvn/starred{/owner}{/repo},https://api.github.com/users/dchvn/subscriptions,https://api.github.com/users/dchvn/orgs,https://api.github.com/users/dchvn/repos,https://api.github.com/users/dchvn/events{/privacy},https://api.github.com/users/dchvn/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34208,https://github.com/apache/spark/pull/34208,https://github.com/apache/spark/pull/34208.diff,https://github.com/apache/spark/pull/34208.patch,,https://api.github.com/repos/apache/spark/issues/34208/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
167,https://api.github.com/repos/apache/spark/issues/34198,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34198/labels{/name},https://api.github.com/repos/apache/spark/issues/34198/comments,https://api.github.com/repos/apache/spark/issues/34198/events,https://github.com/apache/spark/pull/34198,1018861192,PR_kwDOAQXtWs4s1Ntj,34198,[SPARK-36300][SQL] Refactor eleventh set of 20 in QueryExecutionErrors to use error classes,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981471081, 'node_id': 'MDU6TGFiZWwxOTgxNDcxMDgx', 'url': 'https://api.github.com/repos/apache/spark/labels/DOCS', 'name': 'DOCS', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,1,2021-10-06T17:50:02Z,2021-10-10T08:18:42Z,,NONE,,False,"### What changes were proposed in this pull request?
Refactor some exceptions in QueryExecutionErrors to use error classes.

```
expressionDecodingError
expressionEncodingError
classHasUnexpectedSerializerError
cannotGetOuterPointerForInnerClassError
userDefinedTypeNotAnnotatedAndRegisteredError
invalidInputSyntaxForBooleanError
unsupportedOperandTypeForSizeFunctionError
unexpectedValueForStartInFunctionError
unexpectedValueForLengthInFunctionError
sqlArrayIndexNotStartAtOneError
concatArraysWithElementsExceedLimitError
flattenArraysWithElementsExceedLimitError
createArrayWithElementsExceedLimitError
unionArrayWithElementsExceedLimitError
initialTypeNotTargetDataTypeError
initialTypeNotTargetDataTypesError
cannotConvertColumnToJSONError
malformedRecordsDetectedInSchemaInferenceError
malformedJSONError
malformedRecordsDetectedInSchemaInferenceError
```

### Why are the changes needed?
There are currently ~350 exceptions in this file; so this PR only focuses on the eleventh set of 20.


### Does this PR introduce _any_ user-facing change?
No

### How was this patch tested?
Existed UT
",https://api.github.com/repos/apache/spark/issues/34198/timeline,,spark,apache,changvvb,13730772,MDQ6VXNlcjEzNzMwNzcy,https://avatars.githubusercontent.com/u/13730772?v=4,,https://api.github.com/users/changvvb,https://github.com/changvvb,https://api.github.com/users/changvvb/followers,https://api.github.com/users/changvvb/following{/other_user},https://api.github.com/users/changvvb/gists{/gist_id},https://api.github.com/users/changvvb/starred{/owner}{/repo},https://api.github.com/users/changvvb/subscriptions,https://api.github.com/users/changvvb/orgs,https://api.github.com/users/changvvb/repos,https://api.github.com/users/changvvb/events{/privacy},https://api.github.com/users/changvvb/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34198,https://github.com/apache/spark/pull/34198,https://github.com/apache/spark/pull/34198.diff,https://github.com/apache/spark/pull/34198.patch,,https://api.github.com/repos/apache/spark/issues/34198/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
168,https://api.github.com/repos/apache/spark/issues/34192,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34192/labels{/name},https://api.github.com/repos/apache/spark/issues/34192/comments,https://api.github.com/repos/apache/spark/issues/34192/events,https://github.com/apache/spark/pull/34192,1017316164,PR_kwDOAQXtWs4sv20B,34192,[SPARK-36096][CORE] Grouping exception in core/resource,"[{'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,1,2021-10-06T03:02:04Z,2021-10-06T03:02:19Z,,NONE,,False,"### What changes were proposed in this pull request?
This PR group exception messages in core/src/main/scala/org/apache/spark/resource

### Why are the changes needed?
It will largely help with standardization of error messages and its maintenance.

### Does this PR introduce _any_ user-facing change?
No. Error messages remain unchanged.

### How was this patch tested?
No new tests - pass all original tests to make sure it doesn't break any existing behavior.",https://api.github.com/repos/apache/spark/issues/34192/timeline,,spark,apache,phamhuyhoang97,31922679,MDQ6VXNlcjMxOTIyNjc5,https://avatars.githubusercontent.com/u/31922679?v=4,,https://api.github.com/users/phamhuyhoang97,https://github.com/phamhuyhoang97,https://api.github.com/users/phamhuyhoang97/followers,https://api.github.com/users/phamhuyhoang97/following{/other_user},https://api.github.com/users/phamhuyhoang97/gists{/gist_id},https://api.github.com/users/phamhuyhoang97/starred{/owner}{/repo},https://api.github.com/users/phamhuyhoang97/subscriptions,https://api.github.com/users/phamhuyhoang97/orgs,https://api.github.com/users/phamhuyhoang97/repos,https://api.github.com/users/phamhuyhoang97/events{/privacy},https://api.github.com/users/phamhuyhoang97/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34192,https://github.com/apache/spark/pull/34192,https://github.com/apache/spark/pull/34192.diff,https://github.com/apache/spark/pull/34192.patch,,https://api.github.com/repos/apache/spark/issues/34192/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
169,https://api.github.com/repos/apache/spark/issues/34191,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34191/labels{/name},https://api.github.com/repos/apache/spark/issues/34191/comments,https://api.github.com/repos/apache/spark/issues/34191/events,https://github.com/apache/spark/pull/34191,1017282733,PR_kwDOAQXtWs4svvbk,34191,[SPARK-36100][CORE] Grouping exception in core/status,"[{'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,1,2021-10-06T02:36:03Z,2021-10-06T04:52:38Z,,NONE,,False,"### What changes were proposed in this pull request?
This PR group exception messages in core/src/main/scala/org/apache/spark/status

### Why are the changes needed?
It will largely help with standardization of error messages and its maintenance.

### Does this PR introduce _any_ user-facing change?
No. Error messages remain unchanged.

### How was this patch tested?
No new tests - pass all original tests to make sure it doesn't break any existing behavior.",https://api.github.com/repos/apache/spark/issues/34191/timeline,,spark,apache,phamhuyhoang97,31922679,MDQ6VXNlcjMxOTIyNjc5,https://avatars.githubusercontent.com/u/31922679?v=4,,https://api.github.com/users/phamhuyhoang97,https://github.com/phamhuyhoang97,https://api.github.com/users/phamhuyhoang97/followers,https://api.github.com/users/phamhuyhoang97/following{/other_user},https://api.github.com/users/phamhuyhoang97/gists{/gist_id},https://api.github.com/users/phamhuyhoang97/starred{/owner}{/repo},https://api.github.com/users/phamhuyhoang97/subscriptions,https://api.github.com/users/phamhuyhoang97/orgs,https://api.github.com/users/phamhuyhoang97/repos,https://api.github.com/users/phamhuyhoang97/events{/privacy},https://api.github.com/users/phamhuyhoang97/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34191,https://github.com/apache/spark/pull/34191,https://github.com/apache/spark/pull/34191.diff,https://github.com/apache/spark/pull/34191.patch,,https://api.github.com/repos/apache/spark/issues/34191/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
170,https://api.github.com/repos/apache/spark/issues/34190,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34190/labels{/name},https://api.github.com/repos/apache/spark/issues/34190/comments,https://api.github.com/repos/apache/spark/issues/34190/events,https://github.com/apache/spark/pull/34190,1017265042,PR_kwDOAQXtWs4svrdg,34190,[SPARK-36293][SQL] Refactor fourth set of 20 query execution errors to use error classes,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,9,2021-10-06T02:21:55Z,2021-10-13T22:28:41Z,,CONTRIBUTOR,,False,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->
Refactor some exceptions in QueryExecutionErrors to use error classes.
There are currently ~350 exceptions in this file; so this PR only focuses on the fourth set of 20.

> unableToCreateDatabaseAsFailedToCreateDirectoryError
> unableToDropDatabaseAsFailedToDeleteDirectoryError
> unableToCreateTableAsFailedToCreateDirectoryError
> unableToDeletePartitionPathError
> unableToDropTableAsFailedToDeleteDirectoryError
> unableToRenameTableAsFailedToRenameDirectoryError
> unableToCreatePartitionPathError
> unableToRenamePartitionPathError
> methodNotImplementedError
> tableStatsNotSpecifiedError
> unaryMinusCauseOverflowError
> binaryArithmeticCauseOverflowError
> failedSplitSubExpressionMsg
> failedSplitSubExpressionError
> failedToCompileMsg
> internalCompilerError
> compilerError
> unsupportedTableChangeError
> notADatasourceRDDPartitionError
> dataPathNotSpecifiedError

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->
https://issues.apache.org/jira/browse/SPARK-36094

### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->
No

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->
Pass all current tests",https://api.github.com/repos/apache/spark/issues/34190/timeline,,spark,apache,dchvn,91461145,MDQ6VXNlcjkxNDYxMTQ1,https://avatars.githubusercontent.com/u/91461145?v=4,,https://api.github.com/users/dchvn,https://github.com/dchvn,https://api.github.com/users/dchvn/followers,https://api.github.com/users/dchvn/following{/other_user},https://api.github.com/users/dchvn/gists{/gist_id},https://api.github.com/users/dchvn/starred{/owner}{/repo},https://api.github.com/users/dchvn/subscriptions,https://api.github.com/users/dchvn/orgs,https://api.github.com/users/dchvn/repos,https://api.github.com/users/dchvn/events{/privacy},https://api.github.com/users/dchvn/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34190,https://github.com/apache/spark/pull/34190,https://github.com/apache/spark/pull/34190.diff,https://github.com/apache/spark/pull/34190.patch,,https://api.github.com/repos/apache/spark/issues/34190/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
171,https://api.github.com/repos/apache/spark/issues/34189,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34189/labels{/name},https://api.github.com/repos/apache/spark/issues/34189/comments,https://api.github.com/repos/apache/spark/issues/34189/events,https://github.com/apache/spark/pull/34189,1017219843,PR_kwDOAQXtWs4svhWR,34189,[SPARK-36102][CORE] Grouping exception in core/deploy,"[{'id': 1406605079, 'node_id': 'MDU6TGFiZWwxNDA2NjA1MDc5', 'url': 'https://api.github.com/repos/apache/spark/labels/WEB%20UI', 'name': 'WEB UI', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1982260031, 'node_id': 'MDU6TGFiZWwxOTgyMjYwMDMx', 'url': 'https://api.github.com/repos/apache/spark/labels/PYTHON', 'name': 'PYTHON', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,1,2021-10-06T01:45:45Z,2021-10-06T01:46:30Z,,NONE,,False,"### What changes were proposed in this pull request?
This PR group exception messages in core/src/main/scala/org/apache/spark/deploy

### Why are the changes needed?
It will largely help with standardization of error messages and its maintenance.

### Does this PR introduce _any_ user-facing change?
No. Error messages remain unchanged.

### How was this patch tested?
No new tests - pass all original tests to make sure it doesn't break any existing behavior.",https://api.github.com/repos/apache/spark/issues/34189/timeline,,spark,apache,thangnd197,28508170,MDQ6VXNlcjI4NTA4MTcw,https://avatars.githubusercontent.com/u/28508170?v=4,,https://api.github.com/users/thangnd197,https://github.com/thangnd197,https://api.github.com/users/thangnd197/followers,https://api.github.com/users/thangnd197/following{/other_user},https://api.github.com/users/thangnd197/gists{/gist_id},https://api.github.com/users/thangnd197/starred{/owner}{/repo},https://api.github.com/users/thangnd197/subscriptions,https://api.github.com/users/thangnd197/orgs,https://api.github.com/users/thangnd197/repos,https://api.github.com/users/thangnd197/events{/privacy},https://api.github.com/users/thangnd197/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34189,https://github.com/apache/spark/pull/34189,https://github.com/apache/spark/pull/34189.diff,https://github.com/apache/spark/pull/34189.patch,,https://api.github.com/repos/apache/spark/issues/34189/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
172,https://api.github.com/repos/apache/spark/issues/34188,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34188/labels{/name},https://api.github.com/repos/apache/spark/issues/34188/comments,https://api.github.com/repos/apache/spark/issues/34188/events,https://github.com/apache/spark/pull/34188,1017213880,PR_kwDOAQXtWs4svgAi,34188,[SPARK-36099][CORE] Grouping exception in core/util,"[{'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,1,2021-10-06T01:40:54Z,2021-10-06T01:44:37Z,,NONE,,False,"### What changes were proposed in this pull request?
This PR group exception messages in core/src/main/scala/org/apache/spark/util

### Why are the changes needed?
It will largely help with standardization of error messages and its maintenance.

### Does this PR introduce _any_ user-facing change?
No. Error messages remain unchanged.

### How was this patch tested?
No new tests - pass all original tests to make sure it doesn't break any existing behavior.",https://api.github.com/repos/apache/spark/issues/34188/timeline,,spark,apache,thangnd197,28508170,MDQ6VXNlcjI4NTA4MTcw,https://avatars.githubusercontent.com/u/28508170?v=4,,https://api.github.com/users/thangnd197,https://github.com/thangnd197,https://api.github.com/users/thangnd197/followers,https://api.github.com/users/thangnd197/following{/other_user},https://api.github.com/users/thangnd197/gists{/gist_id},https://api.github.com/users/thangnd197/starred{/owner}{/repo},https://api.github.com/users/thangnd197/subscriptions,https://api.github.com/users/thangnd197/orgs,https://api.github.com/users/thangnd197/repos,https://api.github.com/users/thangnd197/events{/privacy},https://api.github.com/users/thangnd197/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34188,https://github.com/apache/spark/pull/34188,https://github.com/apache/spark/pull/34188.diff,https://github.com/apache/spark/pull/34188.patch,,https://api.github.com/repos/apache/spark/issues/34188/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
173,https://api.github.com/repos/apache/spark/issues/34178,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34178/labels{/name},https://api.github.com/repos/apache/spark/issues/34178/comments,https://api.github.com/repos/apache/spark/issues/34178/events,https://github.com/apache/spark/pull/34178,1015848269,PR_kwDOAQXtWs4srCYT,34178,[SPARK-36296][SQL] Refactor seventh set of 20 in QueryExecutionErrors to use error classes,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981471081, 'node_id': 'MDU6TGFiZWwxOTgxNDcxMDgx', 'url': 'https://api.github.com/repos/apache/spark/labels/DOCS', 'name': 'DOCS', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,6,2021-10-05T03:38:06Z,2021-10-18T07:32:09Z,,CONTRIBUTOR,,False,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
Refactor some exceptions in QueryExecutionErrors to use error classes.
```
missingJdbcTableNameAndQueryError
emptyOptionError
invalidJdbcTxnIsolationLevelError
cannotGetJdbcTypeError
unrecognizedSqlTypeError
unsupportedJdbcTypeError
unsupportedArrayElementTypeBasedOnBinaryError
nestedArraysUnsupportedError
cannotTranslateNonNullValueForFieldError
invalidJdbcNumPartitionsError
transactionUnsupportedByJdbcServerError
dataTypeUnsupportedYetError
unsupportedOperationForDataTypeError
inputFilterNotFullyConvertibleError
cannotReadFooterForFileError
cannotReadFooterForFileError
foundDuplicateFieldInCaseInsensitiveModeError
failedToMergeIncompatibleSchemasError
ddlUnsupportedTemporarilyError
operatingOnCanonicalizationPlanError
```


### Why are the changes needed?
There are currently ~350 exceptions in this file; so this PR only focuses on the seventh set of 20.


### Does this PR introduce _any_ user-facing change?
No


### How was this patch tested?
Existing UT Testcase
",https://api.github.com/repos/apache/spark/issues/34178/timeline,,spark,apache,dchvn,91461145,MDQ6VXNlcjkxNDYxMTQ1,https://avatars.githubusercontent.com/u/91461145?v=4,,https://api.github.com/users/dchvn,https://github.com/dchvn,https://api.github.com/users/dchvn/followers,https://api.github.com/users/dchvn/following{/other_user},https://api.github.com/users/dchvn/gists{/gist_id},https://api.github.com/users/dchvn/starred{/owner}{/repo},https://api.github.com/users/dchvn/subscriptions,https://api.github.com/users/dchvn/orgs,https://api.github.com/users/dchvn/repos,https://api.github.com/users/dchvn/events{/privacy},https://api.github.com/users/dchvn/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34178,https://github.com/apache/spark/pull/34178,https://github.com/apache/spark/pull/34178.diff,https://github.com/apache/spark/pull/34178.patch,,https://api.github.com/repos/apache/spark/issues/34178/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
174,https://api.github.com/repos/apache/spark/issues/34177,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34177/labels{/name},https://api.github.com/repos/apache/spark/issues/34177/comments,https://api.github.com/repos/apache/spark/issues/34177/events,https://github.com/apache/spark/pull/34177,1015822698,PR_kwDOAQXtWs4sq9dW,34177,[SPARK-36292][SQL] Refactor third set of 20 in QueryExecutionErrors to use error classes,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,4,2021-10-05T02:43:40Z,2021-11-04T03:38:07Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?
Refactor some exceptions in QueryExecutionErrors to use error classes.
```
cannotGenerateCodeForUnsupportedTypeError
cannotInterpolateClassIntoCodeBlockError
customCollectionClsNotResolvedError
classUnsupportedByMapObjectsError
nullAsMapKeyNotAllowedError
methodNotDeclaredError
constructorNotFoundError
primaryConstructorNotFoundError
unsupportedNaturalJoinTypeError
notExpectedUnresolvedEncoderError
unsupportedEncoderError
notOverrideExpectedMethodsError
failToConvertValueToJsonError
unexpectedOperatorInCorrelatedSubquery
unreachableError
unsupportedRoundingMode
resolveCannotHandleNestedSchema
inputExternalRowCannotBeNullError
fieldCannotBeNullMsg
fieldCannotBeNullError
```


### Why are the changes needed?
There are currently ~350 exceptions in this file; so this PR only focuses on the third set of 20.


### Does this PR introduce _any_ user-facing change?
No


### How was this patch tested?
Existed UT Testcase
",https://api.github.com/repos/apache/spark/issues/34177/timeline,,spark,apache,dchvn,91461145,MDQ6VXNlcjkxNDYxMTQ1,https://avatars.githubusercontent.com/u/91461145?v=4,,https://api.github.com/users/dchvn,https://github.com/dchvn,https://api.github.com/users/dchvn/followers,https://api.github.com/users/dchvn/following{/other_user},https://api.github.com/users/dchvn/gists{/gist_id},https://api.github.com/users/dchvn/starred{/owner}{/repo},https://api.github.com/users/dchvn/subscriptions,https://api.github.com/users/dchvn/orgs,https://api.github.com/users/dchvn/repos,https://api.github.com/users/dchvn/events{/privacy},https://api.github.com/users/dchvn/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34177,https://github.com/apache/spark/pull/34177,https://github.com/apache/spark/pull/34177.diff,https://github.com/apache/spark/pull/34177.patch,,https://api.github.com/repos/apache/spark/issues/34177/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
175,https://api.github.com/repos/apache/spark/issues/34170,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34170/labels{/name},https://api.github.com/repos/apache/spark/issues/34170/comments,https://api.github.com/repos/apache/spark/issues/34170/events,https://github.com/apache/spark/pull/34170,1015370551,PR_kwDOAQXtWs4spkZ1,34170,[SPARK-36911][SQL] - Add SQLMetric for AQE Overhead,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,1,2021-10-04T16:12:58Z,2021-10-04T16:38:13Z,,NONE,,False,"…reOptimization, and generating explainString in AQE and then add it to metrics + log times.

<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->
Add metrics for durations of ""reOptimize"", ""generate explainString"" and ""createQueryStages"" to AdaptiveSparkPlanExec metrics to make it easier to see overhead of AQE for a query.

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->
The changes make it easier to get a sense of the overhead of AQE on a given query.

### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->
New SQLMetrics are added

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->
Added tests",https://api.github.com/repos/apache/spark/issues/34170/timeline,,spark,apache,ChenMichael,9058607,MDQ6VXNlcjkwNTg2MDc=,https://avatars.githubusercontent.com/u/9058607?v=4,,https://api.github.com/users/ChenMichael,https://github.com/ChenMichael,https://api.github.com/users/ChenMichael/followers,https://api.github.com/users/ChenMichael/following{/other_user},https://api.github.com/users/ChenMichael/gists{/gist_id},https://api.github.com/users/ChenMichael/starred{/owner}{/repo},https://api.github.com/users/ChenMichael/subscriptions,https://api.github.com/users/ChenMichael/orgs,https://api.github.com/users/ChenMichael/repos,https://api.github.com/users/ChenMichael/events{/privacy},https://api.github.com/users/ChenMichael/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34170,https://github.com/apache/spark/pull/34170,https://github.com/apache/spark/pull/34170.diff,https://github.com/apache/spark/pull/34170.patch,,https://api.github.com/repos/apache/spark/issues/34170/reactions,1,1,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
176,https://api.github.com/repos/apache/spark/issues/34168,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34168/labels{/name},https://api.github.com/repos/apache/spark/issues/34168/comments,https://api.github.com/repos/apache/spark/issues/34168/events,https://github.com/apache/spark/pull/34168,1014922325,PR_kwDOAQXtWs4soJ6k,34168,[SPARK-36302][SQL] Refactor thirteenth set of 20 query execution errors to use error classes,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,31,2021-10-04T09:08:17Z,2021-11-04T10:19:50Z,,CONTRIBUTOR,,False,"

<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->Adds error classes to some of the exceptions in QueryExecutionErrors.


### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->Improves auditing for developers and adds useful fields for users (error class and SQLSTATE).


### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->Fills in missing error class and SQLSTATE fields.


### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->Existing tests
",https://api.github.com/repos/apache/spark/issues/34168/timeline,,spark,apache,dchvn,91461145,MDQ6VXNlcjkxNDYxMTQ1,https://avatars.githubusercontent.com/u/91461145?v=4,,https://api.github.com/users/dchvn,https://github.com/dchvn,https://api.github.com/users/dchvn/followers,https://api.github.com/users/dchvn/following{/other_user},https://api.github.com/users/dchvn/gists{/gist_id},https://api.github.com/users/dchvn/starred{/owner}{/repo},https://api.github.com/users/dchvn/subscriptions,https://api.github.com/users/dchvn/orgs,https://api.github.com/users/dchvn/repos,https://api.github.com/users/dchvn/events{/privacy},https://api.github.com/users/dchvn/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34168,https://github.com/apache/spark/pull/34168,https://github.com/apache/spark/pull/34168.diff,https://github.com/apache/spark/pull/34168.patch,,https://api.github.com/repos/apache/spark/issues/34168/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
177,https://api.github.com/repos/apache/spark/issues/34165,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34165/labels{/name},https://api.github.com/repos/apache/spark/issues/34165/comments,https://api.github.com/repos/apache/spark/issues/34165/events,https://github.com/apache/spark/pull/34165,1014468489,PR_kwDOAQXtWs4smy8p,34165,[SPARK-36916][INFRA] Enable Dependabot for improving security posture of the dependencies,"[{'id': 1982110661, 'node_id': 'MDU6TGFiZWwxOTgyMTEwNjYx', 'url': 'https://api.github.com/repos/apache/spark/labels/INFRA', 'name': 'INFRA', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,28,2021-10-03T17:29:19Z,2021-10-12T16:00:34Z,,CONTRIBUTOR,,False,"
### What changes were proposed in this pull request?
Enable dependabot to get security updates and if needed version updates on dependencies 

### Why are the changes needed?

https://docs.github.com/en/code-security/supply-chain-security/keeping-your-dependencies-updated-automatically

Having knowledge about vulnerabilities of the dependencies helps the project owners decide on their dependencies security posture to make decisions.

If the project decides to get updates only on security updates and not on any version updates then setting these options would not open any PR 's `open-pull-requests-limit: 0`


### Does this PR introduce _any_ user-facing change?
NO
This option has to be enabled in the security section of the project.
https://docs.github.com/en/code-security/supply-chain-security/managing-vulnerabilities-in-your-projects-dependencies/configuring-dependabot-security-updates#managing-dependabot-security-updates-for-your-repositories
### How was this patch tested?
N/A
",https://api.github.com/repos/apache/spark/issues/34165/timeline,,spark,apache,naveensrinivasan,172697,MDQ6VXNlcjE3MjY5Nw==,https://avatars.githubusercontent.com/u/172697?v=4,,https://api.github.com/users/naveensrinivasan,https://github.com/naveensrinivasan,https://api.github.com/users/naveensrinivasan/followers,https://api.github.com/users/naveensrinivasan/following{/other_user},https://api.github.com/users/naveensrinivasan/gists{/gist_id},https://api.github.com/users/naveensrinivasan/starred{/owner}{/repo},https://api.github.com/users/naveensrinivasan/subscriptions,https://api.github.com/users/naveensrinivasan/orgs,https://api.github.com/users/naveensrinivasan/repos,https://api.github.com/users/naveensrinivasan/events{/privacy},https://api.github.com/users/naveensrinivasan/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34165,https://github.com/apache/spark/pull/34165,https://github.com/apache/spark/pull/34165.diff,https://github.com/apache/spark/pull/34165.patch,,https://api.github.com/repos/apache/spark/issues/34165/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
178,https://api.github.com/repos/apache/spark/issues/34141,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34141/labels{/name},https://api.github.com/repos/apache/spark/issues/34141/comments,https://api.github.com/repos/apache/spark/issues/34141/events,https://github.com/apache/spark/pull/34141,1010604025,PR_kwDOAQXtWs4sbt0p,34141,[SPARK-33887][SQL] Allow insert overwrite same table with static partition if using dynamic partition overwrite mode,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,2,2021-09-29T07:38:14Z,2021-09-29T08:21:48Z,,NONE,,False,"
<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
For dynamic partition overwrite, we do not delete partition directories ahead, We write to staging directories and move to final partition directories after writing job is done. So it is ok to have outputPath try to overwrite inputpath. In view of this, i think inserting static partition in dynamic partition overwrite mode is also supported, i adjust the judging conditions of `dynamicPartitionOverwrite` to allow this.
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->


### Why are the changes needed?
In some cases, we need overwrite static partition using itself.
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->


### Does this PR introduce _any_ user-facing change?
No
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->


### How was this patch tested?
UT
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->
",https://api.github.com/repos/apache/spark/issues/34141/timeline,,spark,apache,PengleiShi,23723660,MDQ6VXNlcjIzNzIzNjYw,https://avatars.githubusercontent.com/u/23723660?v=4,,https://api.github.com/users/PengleiShi,https://github.com/PengleiShi,https://api.github.com/users/PengleiShi/followers,https://api.github.com/users/PengleiShi/following{/other_user},https://api.github.com/users/PengleiShi/gists{/gist_id},https://api.github.com/users/PengleiShi/starred{/owner}{/repo},https://api.github.com/users/PengleiShi/subscriptions,https://api.github.com/users/PengleiShi/orgs,https://api.github.com/users/PengleiShi/repos,https://api.github.com/users/PengleiShi/events{/privacy},https://api.github.com/users/PengleiShi/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34141,https://github.com/apache/spark/pull/34141,https://github.com/apache/spark/pull/34141.diff,https://github.com/apache/spark/pull/34141.patch,,https://api.github.com/repos/apache/spark/issues/34141/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
179,https://api.github.com/repos/apache/spark/issues/34122,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34122/labels{/name},https://api.github.com/repos/apache/spark/issues/34122/comments,https://api.github.com/repos/apache/spark/issues/34122/events,https://github.com/apache/spark/pull/34122,1008746689,PR_kwDOAQXtWs4sV-pu,34122,[WIP][SPARK-34826][SHUFFLE] Adaptively fetch shuffle mergers for push based shuffle,"[{'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,3,2021-09-27T22:46:32Z,2021-09-28T01:34:08Z,,CONTRIBUTOR,,True,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->
Currently shuffle mergers are fetched before the start of the ShuffleMapStage. But for initial stages this can be problematic as shuffle mergers are nothing but unique hosts with shuffle services running which could be very few based on executors and this can cause merge ratio to be low. 

With this approach, `ShuffleMapTask` query for merger locations if not available and if available and start using this for pushing the blocks. Since partitions are mapped uniquely to a merger location, it should be fine to not push for the earlier set of tasks. This should improve the merge ratio for even initial stages.

Note: Currently this is in WIP because the changes are on top of SPARK-33701, once that gets merged will remove those changes updated it and remove WIP tag.

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->
Performance improvement. No new APIs change.

### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->
No

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->
Added unit tests and also has been working in our internal production environment for a while now.",https://api.github.com/repos/apache/spark/issues/34122/timeline,,spark,apache,venkata91,8871522,MDQ6VXNlcjg4NzE1MjI=,https://avatars.githubusercontent.com/u/8871522?v=4,,https://api.github.com/users/venkata91,https://github.com/venkata91,https://api.github.com/users/venkata91/followers,https://api.github.com/users/venkata91/following{/other_user},https://api.github.com/users/venkata91/gists{/gist_id},https://api.github.com/users/venkata91/starred{/owner}{/repo},https://api.github.com/users/venkata91/subscriptions,https://api.github.com/users/venkata91/orgs,https://api.github.com/users/venkata91/repos,https://api.github.com/users/venkata91/events{/privacy},https://api.github.com/users/venkata91/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34122,https://github.com/apache/spark/pull/34122,https://github.com/apache/spark/pull/34122.diff,https://github.com/apache/spark/pull/34122.patch,,https://api.github.com/repos/apache/spark/issues/34122/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
180,https://api.github.com/repos/apache/spark/issues/34108,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34108/labels{/name},https://api.github.com/repos/apache/spark/issues/34108/comments,https://api.github.com/repos/apache/spark/issues/34108/events,https://github.com/apache/spark/pull/34108,1007293416,PR_kwDOAQXtWs4sRnM9,34108,[SPARK-36638][SQL][TEST] Generalize OptimizeSkewedJoin - correctness,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,4,2021-09-26T06:25:07Z,2021-09-28T01:37:11Z,,CONTRIBUTOR,,True,this draft is only used to check correctness of the algorithm in https://github.com/apache/spark/pull/33893,https://api.github.com/repos/apache/spark/issues/34108/timeline,,spark,apache,zhengruifeng,7322292,MDQ6VXNlcjczMjIyOTI=,https://avatars.githubusercontent.com/u/7322292?v=4,,https://api.github.com/users/zhengruifeng,https://github.com/zhengruifeng,https://api.github.com/users/zhengruifeng/followers,https://api.github.com/users/zhengruifeng/following{/other_user},https://api.github.com/users/zhengruifeng/gists{/gist_id},https://api.github.com/users/zhengruifeng/starred{/owner}{/repo},https://api.github.com/users/zhengruifeng/subscriptions,https://api.github.com/users/zhengruifeng/orgs,https://api.github.com/users/zhengruifeng/repos,https://api.github.com/users/zhengruifeng/events{/privacy},https://api.github.com/users/zhengruifeng/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34108,https://github.com/apache/spark/pull/34108,https://github.com/apache/spark/pull/34108.diff,https://github.com/apache/spark/pull/34108.patch,,https://api.github.com/repos/apache/spark/issues/34108/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
181,https://api.github.com/repos/apache/spark/issues/34098,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34098/labels{/name},https://api.github.com/repos/apache/spark/issues/34098/comments,https://api.github.com/repos/apache/spark/issues/34098/events,https://github.com/apache/spark/pull/34098,1006345272,PR_kwDOAQXtWs4sO97D,34098,[SPARK-36842][Core] TaskSchedulerImpl - stop TaskResultGetter properly,"[{'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,12,2021-09-24T11:02:51Z,2021-10-31T13:58:10Z,,CONTRIBUTOR,,False,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
Catch exception during TaskSchedulerImpl.stop() so that all components can be stopped properly

### Why are the changes needed?
Otherwise some threads won't be stopped during spark session restart

### Does this PR introduce _any_ user-facing change?
NO

### How was this patch tested?
It's tested by 
1. create a new spark session in yarn-client mode
2. kill the spark application on yarn
3. check that the spark context is stopped and create a new spark session
4. do the above steps multiple times and verify that no task-result-getter threads number doesn't increase
",https://api.github.com/repos/apache/spark/issues/34098/timeline,,spark,apache,lxian,3442641,MDQ6VXNlcjM0NDI2NDE=,https://avatars.githubusercontent.com/u/3442641?v=4,,https://api.github.com/users/lxian,https://github.com/lxian,https://api.github.com/users/lxian/followers,https://api.github.com/users/lxian/following{/other_user},https://api.github.com/users/lxian/gists{/gist_id},https://api.github.com/users/lxian/starred{/owner}{/repo},https://api.github.com/users/lxian/subscriptions,https://api.github.com/users/lxian/orgs,https://api.github.com/users/lxian/repos,https://api.github.com/users/lxian/events{/privacy},https://api.github.com/users/lxian/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34098,https://github.com/apache/spark/pull/34098,https://github.com/apache/spark/pull/34098.diff,https://github.com/apache/spark/pull/34098.patch,,https://api.github.com/repos/apache/spark/issues/34098/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
182,https://api.github.com/repos/apache/spark/issues/34093,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34093/labels{/name},https://api.github.com/repos/apache/spark/issues/34093/comments,https://api.github.com/repos/apache/spark/issues/34093/events,https://github.com/apache/spark/pull/34093,1006160507,PR_kwDOAQXtWs4sOZ5B,34093,[SPARK-36294][SQL] Refactor fifth set of 20 query execution errors to use error classes,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1984361638, 'node_id': 'MDU6TGFiZWwxOTg0MzYxNjM4', 'url': 'https://api.github.com/repos/apache/spark/labels/R', 'name': 'R', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,18,2021-09-24T07:18:22Z,2021-10-19T07:36:09Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?
Refactor fifth set of 20 query execution errors to use error classes. as follows:
```
createStreamingSourceNotSpecifySchemaError
streamedOperatorUnsupportedByDataSourceError
multiplePathsSpecifiedError
failedToFindDataSourceError
removedClassInSpark2Error
incompatibleDataSourceRegisterError
unrecognizedFileFormatError
sparkUpgradeInReadingDatesError
sparkUpgradeInWritingDatesError
buildReaderUnsupportedForFileFormatError
jobAbortedError
taskFailedWhileWritingRowsError
readCurrentFileNotFoundError
unsupportedSaveModeError
cannotClearOutputDirectoryError
cannotClearPartitionDirectoryError
failedToCastValueToDataTypeForPartitionColumnError
endOfStreamError
fallbackV1RelationReportsInconsistentSchemaError
cannotDropNonemptyNamespaceError
```


### Why are the changes needed?
[SPARK-36294](https://issues.apache.org/jira/browse/SPARK-36294)


### Does this PR introduce _any_ user-facing change?
No

### How was this patch tested?
Existed UT Testcase
",https://api.github.com/repos/apache/spark/issues/34093/timeline,,spark,apache,Peng-Lei,41178002,MDQ6VXNlcjQxMTc4MDAy,https://avatars.githubusercontent.com/u/41178002?v=4,,https://api.github.com/users/Peng-Lei,https://github.com/Peng-Lei,https://api.github.com/users/Peng-Lei/followers,https://api.github.com/users/Peng-Lei/following{/other_user},https://api.github.com/users/Peng-Lei/gists{/gist_id},https://api.github.com/users/Peng-Lei/starred{/owner}{/repo},https://api.github.com/users/Peng-Lei/subscriptions,https://api.github.com/users/Peng-Lei/orgs,https://api.github.com/users/Peng-Lei/repos,https://api.github.com/users/Peng-Lei/events{/privacy},https://api.github.com/users/Peng-Lei/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34093,https://github.com/apache/spark/pull/34093,https://github.com/apache/spark/pull/34093.diff,https://github.com/apache/spark/pull/34093.patch,,https://api.github.com/repos/apache/spark/issues/34093/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
183,https://api.github.com/repos/apache/spark/issues/34089,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34089/labels{/name},https://api.github.com/repos/apache/spark/issues/34089/comments,https://api.github.com/repos/apache/spark/issues/34089/events,https://github.com/apache/spark/pull/34089,1006016259,PR_kwDOAQXtWs4sN_AE,34089,[SPARK-36837][BUILD] Upgrade Kafka to 3.0.1,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1406587328, 'node_id': 'MDU6TGFiZWwxNDA2NTg3MzI4', 'url': 'https://api.github.com/repos/apache/spark/labels/STRUCTURED%20STREAMING', 'name': 'STRUCTURED STREAMING', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1406627200, 'node_id': 'MDU6TGFiZWwxNDA2NjI3MjAw', 'url': 'https://api.github.com/repos/apache/spark/labels/BUILD', 'name': 'BUILD', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,11,2021-09-24T02:34:04Z,2021-12-28T22:07:05Z,,MEMBER,,True,"### What changes were proposed in this pull request?

This PR aims to upgrade Kafka client library from 2.8.1 to 3.0.0.

### Why are the changes needed?

Kafka 3.0.0 has the following improvements and bug fixes including client side.
- https://downloads.apache.org/kafka/3.0.0/RELEASE_NOTES.html

### Does this PR introduce _any_ user-facing change?

No.

### How was this patch tested?

Pass the CIs.",https://api.github.com/repos/apache/spark/issues/34089/timeline,,spark,apache,dongjoon-hyun,9700541,MDQ6VXNlcjk3MDA1NDE=,https://avatars.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34089,https://github.com/apache/spark/pull/34089,https://github.com/apache/spark/pull/34089.diff,https://github.com/apache/spark/pull/34089.patch,,https://api.github.com/repos/apache/spark/issues/34089/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
184,https://api.github.com/repos/apache/spark/issues/34083,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34083/labels{/name},https://api.github.com/repos/apache/spark/issues/34083/comments,https://api.github.com/repos/apache/spark/issues/34083/events,https://github.com/apache/spark/pull/34083,1005856593,PR_kwDOAQXtWs4sNgPR,34083,Add docs about using Shiv for packaging (similar to PEX),"[{'id': 1982260031, 'node_id': 'MDU6TGFiZWwxOTgyMjYwMDMx', 'url': 'https://api.github.com/repos/apache/spark/labels/PYTHON', 'name': 'PYTHON', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,16,2021-09-23T21:20:34Z,2021-09-28T01:28:05Z,,NONE,,False,"### What changes were proposed in this pull request?

The Shiv packaging tool works similarly to PEX and can be used to distribute Python with its dependencies in an executable. These changes mention Shiv and demonstrate it's use similar to the PEX project.

### Why are the changes needed?

Shiv is a widely used packaging tool similar to PEX. These changes mention Shiv as an alternative tool.

### Does this PR introduce _any_ user-facing change?

No.

### How was this patch tested?

No code changes.",https://api.github.com/repos/apache/spark/issues/34083/timeline,,spark,apache,grantjenks,118304,MDQ6VXNlcjExODMwNA==,https://avatars.githubusercontent.com/u/118304?v=4,,https://api.github.com/users/grantjenks,https://github.com/grantjenks,https://api.github.com/users/grantjenks/followers,https://api.github.com/users/grantjenks/following{/other_user},https://api.github.com/users/grantjenks/gists{/gist_id},https://api.github.com/users/grantjenks/starred{/owner}{/repo},https://api.github.com/users/grantjenks/subscriptions,https://api.github.com/users/grantjenks/orgs,https://api.github.com/users/grantjenks/repos,https://api.github.com/users/grantjenks/events{/privacy},https://api.github.com/users/grantjenks/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34083,https://github.com/apache/spark/pull/34083,https://github.com/apache/spark/pull/34083.diff,https://github.com/apache/spark/pull/34083.patch,,https://api.github.com/repos/apache/spark/issues/34083/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
185,https://api.github.com/repos/apache/spark/issues/34074,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34074/labels{/name},https://api.github.com/repos/apache/spark/issues/34074/comments,https://api.github.com/repos/apache/spark/issues/34074/events,https://github.com/apache/spark/pull/34074,1004856465,PR_kwDOAQXtWs4sKbtm,34074,[SPARK-33573][SHUFFLE][YARN] Shuffle server side metrics for Push-based shuffle,"[{'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,6,2021-09-22T23:19:50Z,2021-11-01T09:28:12Z,,CONTRIBUTOR,,False,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->
This is one of the patches for SPARK-33235: Push-based Shuffle Improvement Tasks.
Added a class `PushMergeMetrics`, to collect below metrics from shuffle server side for Push-based shuffle:
- no opportunity responses
- too late responses
- pushed bytes written
- cached block bytes

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->
This helps to understand the push based shuffle metrics from shuffle server side.

### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->
No.

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->
Added a method `verifyMetrics` to verify those metrics in existing unit tests.

Lead-authored by: Chandni Singh chsingh@linkedin.com
Co-authored by: Min Shen mshen@linkedin.com
Co-authored by: Minchu Yang minyang@linkedin.com",https://api.github.com/repos/apache/spark/issues/34074/timeline,,spark,apache,rmcyang,31781684,MDQ6VXNlcjMxNzgxNjg0,https://avatars.githubusercontent.com/u/31781684?v=4,,https://api.github.com/users/rmcyang,https://github.com/rmcyang,https://api.github.com/users/rmcyang/followers,https://api.github.com/users/rmcyang/following{/other_user},https://api.github.com/users/rmcyang/gists{/gist_id},https://api.github.com/users/rmcyang/starred{/owner}{/repo},https://api.github.com/users/rmcyang/subscriptions,https://api.github.com/users/rmcyang/orgs,https://api.github.com/users/rmcyang/repos,https://api.github.com/users/rmcyang/events{/privacy},https://api.github.com/users/rmcyang/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34074,https://github.com/apache/spark/pull/34074,https://github.com/apache/spark/pull/34074.diff,https://github.com/apache/spark/pull/34074.patch,,https://api.github.com/repos/apache/spark/issues/34074/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
186,https://api.github.com/repos/apache/spark/issues/34070,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34070/labels{/name},https://api.github.com/repos/apache/spark/issues/34070/comments,https://api.github.com/repos/apache/spark/issues/34070/events,https://github.com/apache/spark/pull/34070,1003981877,PR_kwDOAQXtWs4sHqww,34070,[SPARK-36840][SQL] Support DPP if there is no selective predicate on the filtering side,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,23,2021-09-22T08:11:17Z,2021-11-23T13:56:09Z,,MEMBER,,False,"### What changes were proposed in this pull request?

This pr makes it insert a DPP if there is no selective predicate on the filtering side and it still has benefits even makes the filter ratio much smaller.


### Why are the changes needed?

In some cases, it may pruning a lot of data even if there is no selective predicate on the filtering side.






Before this PR | After this PR
--- | ---
![image](https://user-images.githubusercontent.com/5399861/134630846-fbec8def-a12d-4c77-bd82-084a04ab89c0.png)  | ![image](https://user-images.githubusercontent.com/5399861/134631736-d170c194-22e0-4ae0-a592-ef9e635866f2.png)


### Does this PR introduce _any_ user-facing change?

No.

### How was this patch tested?

Unit test.",https://api.github.com/repos/apache/spark/issues/34070/timeline,,spark,apache,wangyum,5399861,MDQ6VXNlcjUzOTk4NjE=,https://avatars.githubusercontent.com/u/5399861?v=4,,https://api.github.com/users/wangyum,https://github.com/wangyum,https://api.github.com/users/wangyum/followers,https://api.github.com/users/wangyum/following{/other_user},https://api.github.com/users/wangyum/gists{/gist_id},https://api.github.com/users/wangyum/starred{/owner}{/repo},https://api.github.com/users/wangyum/subscriptions,https://api.github.com/users/wangyum/orgs,https://api.github.com/users/wangyum/repos,https://api.github.com/users/wangyum/events{/privacy},https://api.github.com/users/wangyum/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34070,https://github.com/apache/spark/pull/34070,https://github.com/apache/spark/pull/34070.diff,https://github.com/apache/spark/pull/34070.patch,,https://api.github.com/repos/apache/spark/issues/34070/reactions,1,1,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
187,https://api.github.com/repos/apache/spark/issues/34069,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34069/labels{/name},https://api.github.com/repos/apache/spark/issues/34069/comments,https://api.github.com/repos/apache/spark/issues/34069/events,https://github.com/apache/spark/pull/34069,1003887215,PR_kwDOAQXtWs4sHXUM,34069,[SPARK-36823][SQL] Support broadcast nested loop join hint for equi-join,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981471081, 'node_id': 'MDU6TGFiZWwxOTgxNDcxMDgx', 'url': 'https://api.github.com/repos/apache/spark/labels/DOCS', 'name': 'DOCS', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,25,2021-09-22T06:42:15Z,2021-10-27T17:11:09Z,,CONTRIBUTOR,,False,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->
Add a new hint `BROADCAST_NL`

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->
For the join if one side is small and other side is large, the shuffle overhead is also very big. Due to the
bhj limitation, we can only broadcast right side for left join and left side for right join. So for the other case, we can try to use `BroadcastNestedLoopJoin` as the join strategy.

### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->
yes, new hint

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->
Add new test in `JoinHintSuite`",https://api.github.com/repos/apache/spark/issues/34069/timeline,,spark,apache,ulysses-you,12025282,MDQ6VXNlcjEyMDI1Mjgy,https://avatars.githubusercontent.com/u/12025282?v=4,,https://api.github.com/users/ulysses-you,https://github.com/ulysses-you,https://api.github.com/users/ulysses-you/followers,https://api.github.com/users/ulysses-you/following{/other_user},https://api.github.com/users/ulysses-you/gists{/gist_id},https://api.github.com/users/ulysses-you/starred{/owner}{/repo},https://api.github.com/users/ulysses-you/subscriptions,https://api.github.com/users/ulysses-you/orgs,https://api.github.com/users/ulysses-you/repos,https://api.github.com/users/ulysses-you/events{/privacy},https://api.github.com/users/ulysses-you/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34069,https://github.com/apache/spark/pull/34069,https://github.com/apache/spark/pull/34069.diff,https://github.com/apache/spark/pull/34069.patch,,https://api.github.com/repos/apache/spark/issues/34069/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
188,https://api.github.com/repos/apache/spark/issues/34062,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34062/labels{/name},https://api.github.com/repos/apache/spark/issues/34062/comments,https://api.github.com/repos/apache/spark/issues/34062/events,https://github.com/apache/spark/pull/34062,1003178471,PR_kwDOAQXtWs4sE9i-,34062,[SPARK-36819][SQL] Don't insert redundant filters in case static partition pruning can be done,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,25,2021-09-21T19:58:51Z,2021-09-27T09:25:09Z,,CONTRIBUTOR,,False,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
Don't insert dynamic partition pruning filters in case the filters already added statically. In case the filtering predicate on dimension table is on joinKey, no need to insert DPP filter in that case.

Sample query:
```
SELECT f.date_id, f.pid, f.sid FROM
(select date_id, product_id as pid, store_id as sid from fact_stats) as f
JOIN dim_stats s
ON f.sid = s.store_id WHERE s.store_id = 3
```

Without this PR DPP filter is inserted for above query despite of `store_id#4551 = 3` on fact_stats
```
  == Physical Plan ==
  *(2) Project [date_id#4548, pid#4631, sid#4632]
  +- *(2) BroadcastHashJoin [sid#4632], [store_id#4552], Inner, BuildRight, false
     :- *(2) Project [date_id#4548, product_id#4549 AS pid#4631, store_id#4551 AS sid#4632]
     :  +- *(2) ColumnarToRow
     :     +- FileScan parquet default.fact_stats[date_id#4548,product_id#4549,store_id#4551] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/D:/workspace/spark/sql/core/spark-warehouse/org.apache.spark.sql..., PartitionFilters: [(store_id#4551 = 3), isnotnull(store_id#4551), **dynamicpruningexpression(store_id#4551 IN dynamic...,** PushedFilters: [], ReadSchema: struct<date_id:int,product_id:int>
     :           +- SubqueryBroadcast dynamicpruning#4636, 0, [store_id#4552], [id=#2461]
     :              +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [id=#2460]
     :                 +- *(1) Filter (isnotnull(store_id#4552) AND (store_id#4552 = 3))
     :                    +- *(1) ColumnarToRow
     :                       +- FileScan parquet default.dim_stats[store_id#4552] Batched: true, DataFilters: [isnotnull(store_id#4552), (store_id#4552 = 3)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/D:/workspace/spark/sql/core/spark-warehouse/org.apache.spark.sql..., PartitionFilters: [], PushedFilters: [IsNotNull(store_id), EqualTo(store_id,3)], ReadSchema: struct<store_id:int>
     +- ReusedExchange [store_id#4552], BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [id=#2460]
```



### Why are the changes needed?
Having redundant dynamic filters can have  unnecessary overheads: extra filtering overhead + in case of DPP subquery case, subquery execution overhead.
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->


### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->
No

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->
Existing UTs pass. added new test case.",https://api.github.com/repos/apache/spark/issues/34062/timeline,,spark,apache,Swinky,5418286,MDQ6VXNlcjU0MTgyODY=,https://avatars.githubusercontent.com/u/5418286?v=4,,https://api.github.com/users/Swinky,https://github.com/Swinky,https://api.github.com/users/Swinky/followers,https://api.github.com/users/Swinky/following{/other_user},https://api.github.com/users/Swinky/gists{/gist_id},https://api.github.com/users/Swinky/starred{/owner}{/repo},https://api.github.com/users/Swinky/subscriptions,https://api.github.com/users/Swinky/orgs,https://api.github.com/users/Swinky/repos,https://api.github.com/users/Swinky/events{/privacy},https://api.github.com/users/Swinky/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34062,https://github.com/apache/spark/pull/34062,https://github.com/apache/spark/pull/34062.diff,https://github.com/apache/spark/pull/34062.patch,,https://api.github.com/repos/apache/spark/issues/34062/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
189,https://api.github.com/repos/apache/spark/issues/34056,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34056/labels{/name},https://api.github.com/repos/apache/spark/issues/34056/comments,https://api.github.com/repos/apache/spark/issues/34056/events,https://github.com/apache/spark/pull/34056,1001956323,PR_kwDOAQXtWs4sA024,34056,"[SPARK-36811][SQL] Add SQL functions for the BINARY data type for AND, OR, XOR, and NOT","[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,44,2021-09-21T06:52:48Z,2021-11-18T12:50:36Z,,CONTRIBUTOR,,False,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?

This PR introduces four new SQL functions operating on the BINARY data type.
  1. `bitand`: Takes as input two binary strings and optionally a string argument, and returns their bitwise AND.
  2. `bitor`: Takes as input two binary strings and optionally a string argument, and returns their bitwise OR.
  3. `bitxor`: Takes as input two binary strings and optionally a string argument, and returns their bitwise XOR.
  4. `bitnot`: Takes as input a binary string and returns its bitwise NOT.

In more detail, the first three functions behave as follows:
* If the input is only two binary sequences, they are expected to be of the same byte length, otherwise an exception is thrown. In this case the result is the bitwise operation that the function supports, and the byte length of the resulting byte sequence is equal to the common byte length of the two inputs.
* In the three-argument case, the third argument determines how the operation is performed in the case of byte sequences with unequal lengths. The third argument must be equal to either `lpad` or `rpad` (equality here is case insensitive), otherwise an exception is thrown. More precisely, if the third argument is equal to `lpad`, the shorter byte sequence is conceptually left-padded with zeros to match the length of the longer byte sequence, whereas if the third argument is `rpad`, the shorter byte sequence is conceptually right-padded with zeros to match the length of the longer sequence. Obviously, if the two input byte sequences are of the same length the result is the same for both values of the third argument, and equal to what the two-argument version of the function would have produced. In all cases, the byte length of the result is equal to the maximum of the byte lengths of the two input byte sequences.

#### Examples
* Equal length inputs, two argument overload, `bitand` function:
```scala
scala> sql(""select bitand(unhex('aabb'), unhex('03e3'))"").show
+--------------------------------------+
|bitand(unhex(aabb), unhex(03e3), lpad)|
+--------------------------------------+
|                               [02 A3]|
+--------------------------------------+
```
* Equal length inputs, three argument overload, `bitand` function:
```scala
scala> sql(""select bitand(unhex('aabb'), unhex('03e3'), 'lpad')"").show
+--------------------------------------+
|bitand(unhex(aabb), unhex(03e3), lpad)|
+--------------------------------------+
|                               [02 A3]|
+--------------------------------------+

scala> sql(""select bitand(unhex('aabb'), unhex('03e3'), 'rpad')"").show
+--------------------------------------+
|bitand(unhex(aabb), unhex(03e3), rpad)|
+--------------------------------------+
|                               [02 A3]|
+--------------------------------------+
```
* Unequal length inputs, three argument overload, `bitand` function:
```scala
scala> sql(""select bitand(unhex('aabb'), unhex('13e3ff'), 'lpad')"").show
+----------------------------------------+
|bitand(unhex(aabb), unhex(13e3ff), lpad)|
+----------------------------------------+
|                              [00 A2 BB]|
+----------------------------------------+

scala> sql(""select bitand(unhex('aabb'), unhex('13e3ff'), 'rpad')"").show
+----------------------------------------+
|bitand(unhex(aabb), unhex(13e3ff), rpad)|
+----------------------------------------+
|                              [02 A3 00]|
+----------------------------------------+
```
* Unequal length inputs, three argument overload, `bitor` function:
```scala
scala> sql(""select bitor(unhex('aabb'), unhex('13e3ff'), 'lpad')"").show
+---------------------------------------+
|bitor(unhex(aabb), unhex(13e3ff), lpad)|
+---------------------------------------+
|                             [13 EB FF]|
+---------------------------------------+

scala> sql(""select bitor(unhex('aabb'), unhex('13e3ff'), 'rpad')"").show
+---------------------------------------+
|bitor(unhex(aabb), unhex(13e3ff), rpad)|
+---------------------------------------+
|                             [BB FB FF]|
+---------------------------------------+
```
* Unequal length inputs, three argument overload, `bitxor` function:
```scala
scala> sql(""select bitxor(unhex('aabb'), unhex('13e3ff'), 'lpad')"").show
+----------------------------------------+
|bitxor(unhex(aabb), unhex(13e3ff), lpad)|
+----------------------------------------+
|                              [13 49 44]|
+----------------------------------------+


scala> sql(""select bitxor(unhex('aabb'), unhex('13e3ff'), 'rpad')"").show
+----------------------------------------+
|bitxor(unhex(aabb), unhex(13e3ff), rpad)|
+----------------------------------------+
|                              [B9 58 FF]|
+----------------------------------------+
```

### Why are the changes needed?

These functions are useful for performing bitwise operations on `BINARY` values, seen as bit sets.

Other databases offer similar or the same functionality. In more detail:
* Teradata supports them as [functions](https://docs.teradata.com/r/kmuOwjp1zEYg98JsB8fu_A/zNvtiufFYVtAA~wqxccYKA).
* Postgres supports them as [operators](https://www.postgresql.org/docs/9.4/functions-bitstring.html) over the `BIT` data type with the restriction that the inputs must be of equal length.
* MySQL supports them as [operators](https://dev.mysql.com/doc/refman/8.0/en/bit-functions.html). Arguments must have the same length.
* SQL Server supports them as [operators](https://docs.microsoft.com/en-us/sql/t-sql/language-elements/bitwise-operators-transact-sql?view=sql-server-ver15), but only one of the two arguments can be a binary string.

Oracle, Snowflake, Hive SQL, Big Query do not support the proposed functions for binary strings, but rather only for integral types.

### Does this PR introduce _any_ user-facing change?

Yes. Four new SQL functions.

### How was this patch tested?

Unit tests.
",https://api.github.com/repos/apache/spark/issues/34056/timeline,,spark,apache,mkaravel,6397014,MDQ6VXNlcjYzOTcwMTQ=,https://avatars.githubusercontent.com/u/6397014?v=4,,https://api.github.com/users/mkaravel,https://github.com/mkaravel,https://api.github.com/users/mkaravel/followers,https://api.github.com/users/mkaravel/following{/other_user},https://api.github.com/users/mkaravel/gists{/gist_id},https://api.github.com/users/mkaravel/starred{/owner}{/repo},https://api.github.com/users/mkaravel/subscriptions,https://api.github.com/users/mkaravel/orgs,https://api.github.com/users/mkaravel/repos,https://api.github.com/users/mkaravel/events{/privacy},https://api.github.com/users/mkaravel/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34056,https://github.com/apache/spark/pull/34056,https://github.com/apache/spark/pull/34056.diff,https://github.com/apache/spark/pull/34056.patch,,https://api.github.com/repos/apache/spark/issues/34056/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
190,https://api.github.com/repos/apache/spark/issues/34035,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34035/labels{/name},https://api.github.com/repos/apache/spark/issues/34035/comments,https://api.github.com/repos/apache/spark/issues/34035/events,https://github.com/apache/spark/pull/34035,999661296,PR_kwDOAQXtWs4r6Ah1,34035,[SPARK-36793][K8S] Support write container stdout/stderr to file,"[{'id': 1406605057, 'node_id': 'MDU6TGFiZWwxNDA2NjA1MDU3', 'url': 'https://api.github.com/repos/apache/spark/labels/KUBERNETES', 'name': 'KUBERNETES', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981471081, 'node_id': 'MDU6TGFiZWwxOTgxNDcxMDgx', 'url': 'https://api.github.com/repos/apache/spark/labels/DOCS', 'name': 'DOCS', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,1,2021-09-17T19:33:38Z,2021-09-23T22:12:11Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?
Support write container stdout/stderr to file

### Why are the changes needed?
If users want to sidecar logging agent to send stdout/stderr to external log storage,  only way is to change entrypoint.sh, which might break compatibility with community version.

### Does this PR introduce _any_ user-facing change?
Yes. User can enable this feature by spark config.

### How was this patch tested?
Added UT in BasicDriverFeatureStepSuite and BasicExecutorFeatureStepSuite
",https://api.github.com/repos/apache/spark/issues/34035/timeline,,spark,apache,warrenzhu25,1633312,MDQ6VXNlcjE2MzMzMTI=,https://avatars.githubusercontent.com/u/1633312?v=4,,https://api.github.com/users/warrenzhu25,https://github.com/warrenzhu25,https://api.github.com/users/warrenzhu25/followers,https://api.github.com/users/warrenzhu25/following{/other_user},https://api.github.com/users/warrenzhu25/gists{/gist_id},https://api.github.com/users/warrenzhu25/starred{/owner}{/repo},https://api.github.com/users/warrenzhu25/subscriptions,https://api.github.com/users/warrenzhu25/orgs,https://api.github.com/users/warrenzhu25/repos,https://api.github.com/users/warrenzhu25/events{/privacy},https://api.github.com/users/warrenzhu25/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34035,https://github.com/apache/spark/pull/34035,https://github.com/apache/spark/pull/34035.diff,https://github.com/apache/spark/pull/34035.patch,,https://api.github.com/repos/apache/spark/issues/34035/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
191,https://api.github.com/repos/apache/spark/issues/34030,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34030/labels{/name},https://api.github.com/repos/apache/spark/issues/34030/comments,https://api.github.com/repos/apache/spark/issues/34030/events,https://github.com/apache/spark/pull/34030,999006222,PR_kwDOAQXtWs4r4CLl,34030,[SPARK-36790][SQL] Update user-facing catalog to adapt CatalogPlugin,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1984361638, 'node_id': 'MDU6TGFiZWwxOTg0MzYxNjM4', 'url': 'https://api.github.com/repos/apache/spark/labels/R', 'name': 'R', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,10,2021-09-17T06:49:06Z,2021-09-23T15:47:03Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?
Change the CatalogImpl for user-facing catalog API.
before:
`private def sessionCatalog: SessionCatalog = sparkSession.sessionState.catalog`
So all the operations we did were based on SessionCatalog any time.

after:
`private def currentCatalog: CatalogPlugin = sparkSession.sessionState.catalogManager.currentCatalog`
When current catalog is `spark-catalog`, we do the all operations based on SessionCatalog.
Others CatalogPlugin did not support operations  at now. it will throw exception temporarily.
eg: `databaseExists` ,`currentDatabase` and so on.

### Why are the changes needed?
[#SPARK-36790](https://issues.apache.org/jira/browse/SPARK-36790)
User-facting catalog should be access the current catalog.
eg:
`
spark.sql(""use testcat"")
`
so: current catalog != spark.catalog

`
spark.sql(""use testcat.ns1"")
`
so: current database != spark.catalog.currentDatabase

`spark.sql(""show tables"")
`
so: result != spark.catalog.listTables

I think the SparkSession.catalog api should be keep exist instead of SparkSession.sessionState.catalogManager. So update user-facing catalog to adapt CatalogPlugin. At now just throw exception for unsupport the CatalogPlugin temporary.

### Does this PR introduce _any_ user-facing change? 
No


### How was this patch tested?
add ut test",https://api.github.com/repos/apache/spark/issues/34030/timeline,,spark,apache,Peng-Lei,41178002,MDQ6VXNlcjQxMTc4MDAy,https://avatars.githubusercontent.com/u/41178002?v=4,,https://api.github.com/users/Peng-Lei,https://github.com/Peng-Lei,https://api.github.com/users/Peng-Lei/followers,https://api.github.com/users/Peng-Lei/following{/other_user},https://api.github.com/users/Peng-Lei/gists{/gist_id},https://api.github.com/users/Peng-Lei/starred{/owner}{/repo},https://api.github.com/users/Peng-Lei/subscriptions,https://api.github.com/users/Peng-Lei/orgs,https://api.github.com/users/Peng-Lei/repos,https://api.github.com/users/Peng-Lei/events{/privacy},https://api.github.com/users/Peng-Lei/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34030,https://github.com/apache/spark/pull/34030,https://github.com/apache/spark/pull/34030.diff,https://github.com/apache/spark/pull/34030.patch,,https://api.github.com/repos/apache/spark/issues/34030/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
192,https://api.github.com/repos/apache/spark/issues/34024,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34024/labels{/name},https://api.github.com/repos/apache/spark/issues/34024/comments,https://api.github.com/repos/apache/spark/issues/34024/events,https://github.com/apache/spark/pull/34024,998527855,PR_kwDOAQXtWs4r2lZh,34024,[SPARK-36784][SHUFFLE][WIP] Handle DNS issues on executor to prevent shuffle nodes from getting added to exclude list,"[{'id': 1981471081, 'node_id': 'MDU6TGFiZWwxOTgxNDcxMDgx', 'url': 'https://api.github.com/repos/apache/spark/labels/DOCS', 'name': 'DOCS', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,6,2021-09-16T18:53:36Z,2021-09-29T17:37:18Z,,CONTRIBUTOR,,False," ### What changes were proposed in this pull request?

 When a DNS issue happens on the executor node, shuffle nodes would get added to the exclude list due to FetchFailed exception. The change here is to have a configuration host value to test DNS resolution against before marking it as a FetchFailed Exception.

 ### Why are the changes needed?

 This would prevent shuffle nodes from getting added to the exclude list due to DNS issues

 ### Does this PR introduce _any_ user-facing change?

 No

 ### How was this patch tested?

 Added Unit Tests
",https://api.github.com/repos/apache/spark/issues/34024/timeline,,spark,apache,thejdeep,1708757,MDQ6VXNlcjE3MDg3NTc=,https://avatars.githubusercontent.com/u/1708757?v=4,,https://api.github.com/users/thejdeep,https://github.com/thejdeep,https://api.github.com/users/thejdeep/followers,https://api.github.com/users/thejdeep/following{/other_user},https://api.github.com/users/thejdeep/gists{/gist_id},https://api.github.com/users/thejdeep/starred{/owner}{/repo},https://api.github.com/users/thejdeep/subscriptions,https://api.github.com/users/thejdeep/orgs,https://api.github.com/users/thejdeep/repos,https://api.github.com/users/thejdeep/events{/privacy},https://api.github.com/users/thejdeep/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34024,https://github.com/apache/spark/pull/34024,https://github.com/apache/spark/pull/34024.diff,https://github.com/apache/spark/pull/34024.patch,,https://api.github.com/repos/apache/spark/issues/34024/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
193,https://api.github.com/repos/apache/spark/issues/34010,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34010/labels{/name},https://api.github.com/repos/apache/spark/issues/34010/comments,https://api.github.com/repos/apache/spark/issues/34010/events,https://github.com/apache/spark/pull/34010,997566830,PR_kwDOAQXtWs4rzp_v,34010,"[SPARK-36770][SQL] Replace Unbounded Following window functions with Unbounded Preceding window function (First, Last)","[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,1,2021-09-15T22:19:22Z,2021-12-13T23:21:39Z,,NONE,,False,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
**Context**

The **UnboundedFollowingWindowFunctionFrame** has the time complexity of O(N^2), N is the number of rows in the current partition, more specific the complexity is O(N* (N - 1)/2).

**What happens internally in UnboundedFollowingWindowFunctionFrame?**
In the window frame, while processing each incoming row, it will go through current row till the end of partition to do re-calculation. This process will be repeated on each incoming row, which causes the high run-time complexity.

But UnboundedPrecedingWindowFunctionFrame has much better time complexity O(N), N is the number of rows in the current partition.

 
**What is the idea of the improvement?**

Give the big time complexity difference between UnboundedFollowingWindowFunctionFrame and UnboundedPrecedingWindowFunctionFrame, we can do following conversions to improve the time complexity of first() and last() from O(N^2) to O(N)

```
case 1:
first() OVER(PARTITION BY colA ORDER BY colB ASC ROWS CURRENT ROW AND UNBOUNDED FOLLOWING) 
converts to 
last()  OVER(PARTITION BY colA ORDER BY colB DEAC ROWS UNBOUNDED PRECEDING AND CURRENT ROW)

case 2:
last()  OVER(PARTITION BY colA ORDER BY colB ASC ROWS CURRENT ROW AND UNBOUNDED FOLLOWING) 
converts to 
first() OVER(PARTITION BY colA ORDER BY colB DESC ROWS UNBOUNDED PRECEDING AND CURRENT ROW)
```

**Summary**

Replace ""UNBOUNDED FOLLOWING"" with ""UNBOUNDED PRECEDING"", and flip the ORDER BY for the window functions first() and last() for ROWS.


### Why are the changes needed?
Improve the run-time performance for window function fist() and last() against ROWS UNBOUNDED FOLLOWING.


### Does this PR introduce _any_ user-facing change?
NO


### How was this patch tested?
Added unit test: sql/core/src/test/scala/org/apache/spark/sql/WindowFunctionOptimizerTestSuite.scala

",https://api.github.com/repos/apache/spark/issues/34010/timeline,,spark,apache,guibin,289256,MDQ6VXNlcjI4OTI1Ng==,https://avatars.githubusercontent.com/u/289256?v=4,,https://api.github.com/users/guibin,https://github.com/guibin,https://api.github.com/users/guibin/followers,https://api.github.com/users/guibin/following{/other_user},https://api.github.com/users/guibin/gists{/gist_id},https://api.github.com/users/guibin/starred{/owner}{/repo},https://api.github.com/users/guibin/subscriptions,https://api.github.com/users/guibin/orgs,https://api.github.com/users/guibin/repos,https://api.github.com/users/guibin/events{/privacy},https://api.github.com/users/guibin/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34010,https://github.com/apache/spark/pull/34010,https://github.com/apache/spark/pull/34010.diff,https://github.com/apache/spark/pull/34010.patch,,https://api.github.com/repos/apache/spark/issues/34010/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
194,https://api.github.com/repos/apache/spark/issues/34009,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34009/labels{/name},https://api.github.com/repos/apache/spark/issues/34009/comments,https://api.github.com/repos/apache/spark/issues/34009/events,https://github.com/apache/spark/pull/34009,997562866,PR_kwDOAQXtWs4rzpOV,34009,[SPARK-34378][SQL][AVRO] Enhance AvroSerializer validation to allow extra nullable Avro fields,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1988040187, 'node_id': 'MDU6TGFiZWwxOTg4MDQwMTg3', 'url': 'https://api.github.com/repos/apache/spark/labels/AVRO', 'name': 'AVRO', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,31,2021-09-15T22:12:39Z,2021-12-06T04:53:03Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?
Loosen the schema validation logic in `AvroSerializer` to accommodate the situation where a user has provided an explicit schema (via `avroSchema`) and this schema has extra fields which are not present in the Catalyst schema (the DF being written). Specifically, extra _nullable_ fields will be allowed and populated as null. _Required_ fields (non-null) will still be checked for existence.

### Why are the changes needed?
It's common for Avro schemas to evolve in a _compatible_ way (as discussed in Confluent's documentation on [Schema Evolution and Compatibility](https://docs.confluent.io/platform/current/schema-registry/avro.html); here I refer to `FULL` compatibility). Under such a scenario, new _optional_ fields are added to a schema. Producers are free to include the new field if they so choose, and consumers are free to read the new field if they so choose. It is optional on both sides.

Consider the following code:
```
val outputSchema = getOutputSchema()
df.write.format(""avro"").option(""avroSchema"", outputSchema).save(...)
```
If you have a situation where schemas are managed in some centralized repository (e.g. a [schema registry](https://docs.confluent.io/platform/current/schema-registry/index.html)), `outputSchema` may update at some point to add a new optional field, without you necessarily initiating any action on your side as a data producer. With the current code, this would cause the producer job to break, because validation would complain that the newly added field is not present in the DataFrame. Really, the producer should be able to continue producing data as normal even without adding the new field to the DataFrame it is writing out, because the field is optional.

### Does this PR introduce _any_ user-facing change?
Yes, when using the `avroSchema` option on the Avro data source during writes, validation is less strict, and allows for (compatible) schema evolution to be handled more gracefully.

### How was this patch tested?
New unit tests added. We've also been employing this logic internally for a few years, though the implementation was quite different due to recent changes in this area of the code.",https://api.github.com/repos/apache/spark/issues/34009/timeline,,spark,apache,xkrogen,6570401,MDQ6VXNlcjY1NzA0MDE=,https://avatars.githubusercontent.com/u/6570401?v=4,,https://api.github.com/users/xkrogen,https://github.com/xkrogen,https://api.github.com/users/xkrogen/followers,https://api.github.com/users/xkrogen/following{/other_user},https://api.github.com/users/xkrogen/gists{/gist_id},https://api.github.com/users/xkrogen/starred{/owner}{/repo},https://api.github.com/users/xkrogen/subscriptions,https://api.github.com/users/xkrogen/orgs,https://api.github.com/users/xkrogen/repos,https://api.github.com/users/xkrogen/events{/privacy},https://api.github.com/users/xkrogen/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34009,https://github.com/apache/spark/pull/34009,https://github.com/apache/spark/pull/34009.diff,https://github.com/apache/spark/pull/34009.patch,,https://api.github.com/repos/apache/spark/issues/34009/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
195,https://api.github.com/repos/apache/spark/issues/34000,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34000/labels{/name},https://api.github.com/repos/apache/spark/issues/34000/comments,https://api.github.com/repos/apache/spark/issues/34000/events,https://github.com/apache/spark/pull/34000,996465213,PR_kwDOAQXtWs4rwNS7,34000,[SPARK-36620][SHUFFLE] Add client side push based shuffle metrics,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1406605079, 'node_id': 'MDU6TGFiZWwxNDA2NjA1MDc5', 'url': 'https://api.github.com/repos/apache/spark/labels/WEB%20UI', 'name': 'WEB UI', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1406627200, 'node_id': 'MDU6TGFiZWwxNDA2NjI3MjAw', 'url': 'https://api.github.com/repos/apache/spark/labels/BUILD', 'name': 'BUILD', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,9,2021-09-14T21:42:22Z,2021-12-16T11:48:04Z,,CONTRIBUTOR,,False," ### What changes were proposed in this pull request?
This adds the following push based shuffle metrics like :
- Merger count and magnet enabled/disabled for stage
- Time spent on results finalization
- Counting actual number of blocks other than chunks
- Corrupt shuffle blocks chunks and fallback

 ### Why are the changes needed?
These changes help to understand the push based shuffle metrics of the application

 ### Does this PR introduce _any_ user-facing change?
Changes to API responses by SHS (eg: /stages)

 ### How was this patch tested?
Modified existing unit tests and also tested API response on event log files in SHS

Lead-authored by: Venkata krishnan Sowrirajan <vsowrirajan@linkedin.com>
Co-authored by: Chandni Singh <chsingh@linkedin.com>
Co-authored by: Thejdeep Gudivada <tgudivada@linkedin.com>",https://api.github.com/repos/apache/spark/issues/34000/timeline,,spark,apache,thejdeep,1708757,MDQ6VXNlcjE3MDg3NTc=,https://avatars.githubusercontent.com/u/1708757?v=4,,https://api.github.com/users/thejdeep,https://github.com/thejdeep,https://api.github.com/users/thejdeep/followers,https://api.github.com/users/thejdeep/following{/other_user},https://api.github.com/users/thejdeep/gists{/gist_id},https://api.github.com/users/thejdeep/starred{/owner}{/repo},https://api.github.com/users/thejdeep/subscriptions,https://api.github.com/users/thejdeep/orgs,https://api.github.com/users/thejdeep/repos,https://api.github.com/users/thejdeep/events{/privacy},https://api.github.com/users/thejdeep/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34000,https://github.com/apache/spark/pull/34000,https://github.com/apache/spark/pull/34000.diff,https://github.com/apache/spark/pull/34000.patch,,https://api.github.com/repos/apache/spark/issues/34000/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
196,https://api.github.com/repos/apache/spark/issues/33989,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/33989/labels{/name},https://api.github.com/repos/apache/spark/issues/33989/comments,https://api.github.com/repos/apache/spark/issues/33989/events,https://github.com/apache/spark/pull/33989,995623831,PR_kwDOAQXtWs4rtfgI,33989,[SPARK-36676][SQL][BUILD] Create shaded Hive module and upgrade Guava version to 30.1.1-jre,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1406627200, 'node_id': 'MDU6TGFiZWwxNDA2NjI3MjAw', 'url': 'https://api.github.com/repos/apache/spark/labels/BUILD', 'name': 'BUILD', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,38,2021-09-14T06:03:39Z,2021-10-14T18:02:28Z,,CONTRIBUTOR,,False,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->

This PR creates a new module `hive-shaded` which shades & relocates various dependencies from Hive, including Guava. By this means it also upgrades the Guava version from 14.0.1 to 30.1.1-jre.

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->

Spark currently use `hive-exec-core` which leaks a lot of dependencies to Spark, in particular Guava. As consequence, Spark is stuck with an ancient Guava version 14.0.1 which also carries CVE issues described in [SPARK-32502](https://issues.apache.org/jira/browse/SPARK-32502). 

By creating a shaded module, Spark is able to de-couple from those dependencies leaked by Hive and upgrade to newer versions of Guava. This also allows us to upgrade other dependencies whenever necessary, without having to wait for new Hive releases.

### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->

No. For Spark users who programmatically depending on modules such as `spark-hive`, the Hive dependencies will be replaced by the newly created `hive-shaded` module.

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->

Existing tests.",https://api.github.com/repos/apache/spark/issues/33989/timeline,,spark,apache,sunchao,506679,MDQ6VXNlcjUwNjY3OQ==,https://avatars.githubusercontent.com/u/506679?v=4,,https://api.github.com/users/sunchao,https://github.com/sunchao,https://api.github.com/users/sunchao/followers,https://api.github.com/users/sunchao/following{/other_user},https://api.github.com/users/sunchao/gists{/gist_id},https://api.github.com/users/sunchao/starred{/owner}{/repo},https://api.github.com/users/sunchao/subscriptions,https://api.github.com/users/sunchao/orgs,https://api.github.com/users/sunchao/repos,https://api.github.com/users/sunchao/events{/privacy},https://api.github.com/users/sunchao/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/33989,https://github.com/apache/spark/pull/33989,https://github.com/apache/spark/pull/33989.diff,https://github.com/apache/spark/pull/33989.patch,,https://api.github.com/repos/apache/spark/issues/33989/reactions,7,7,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
197,https://api.github.com/repos/apache/spark/issues/33986,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/33986/labels{/name},https://api.github.com/repos/apache/spark/issues/33986/comments,https://api.github.com/repos/apache/spark/issues/33986/events,https://github.com/apache/spark/pull/33986,995496899,PR_kwDOAQXtWs4rtG9r,33986,[SPARK-36727][SQL]Support sql overwrite a path that is also being read from when partit…,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,4,2021-09-14T01:49:05Z,2021-10-31T09:05:47Z,,NONE,,False,"### What changes were proposed in this pull request?
```
// non-partitioned table overwrite
CREATE TABLE tbl (col1 INT, col2 STRING) USING PARQUET;
INSERT OVERWRITE TABLE tbl SELECT 0,1;
INSERT OVERWRITE TABLE tbl SELECT * FROM tbl;

// partitioned table static overwrite
CREATE TABLE tbl (col1 INT, col2 STRING) USING PARQUET PARTITIONED BY (pt1 INT);
INSERT OVERWRITE TABLE tbl PARTITION(p1=2021) SELECT 0 AS col1,1 AS col2;
INSERT OVERWRITE TABLE tbl PARTITION(p1=2021) SELECT col1, col2 FROM WHERE p1=2021;
```
When we run the above query, an error will be throwed ""Cannot overwrite a path that is also being read from""
We need to support this operation when the spark.sql.sources.partitionOverwriteMode is dynamic

### How was this patch tested?
Unit tests -> InsertSuite.scala
",https://api.github.com/repos/apache/spark/issues/33986/timeline,,spark,apache,TongWei1105,68682646,MDQ6VXNlcjY4NjgyNjQ2,https://avatars.githubusercontent.com/u/68682646?v=4,,https://api.github.com/users/TongWei1105,https://github.com/TongWei1105,https://api.github.com/users/TongWei1105/followers,https://api.github.com/users/TongWei1105/following{/other_user},https://api.github.com/users/TongWei1105/gists{/gist_id},https://api.github.com/users/TongWei1105/starred{/owner}{/repo},https://api.github.com/users/TongWei1105/subscriptions,https://api.github.com/users/TongWei1105/orgs,https://api.github.com/users/TongWei1105/repos,https://api.github.com/users/TongWei1105/events{/privacy},https://api.github.com/users/TongWei1105/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/33986,https://github.com/apache/spark/pull/33986,https://github.com/apache/spark/pull/33986.diff,https://github.com/apache/spark/pull/33986.patch,,https://api.github.com/repos/apache/spark/issues/33986/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
198,https://api.github.com/repos/apache/spark/issues/33983,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/33983/labels{/name},https://api.github.com/repos/apache/spark/issues/33983/comments,https://api.github.com/repos/apache/spark/issues/33983/events,https://github.com/apache/spark/pull/33983,995447730,PR_kwDOAQXtWs4rs9iV,33983,[SPARK-33152] [SQL] New algorithm for ConstraintsPropagation rule to solve the problem of performance & OOM if the query plans have large expressions involving multiple aliases,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1982110661, 'node_id': 'MDU6TGFiZWwxOTgyMTEwNjYx', 'url': 'https://api.github.com/repos/apache/spark/labels/INFRA', 'name': 'INFRA', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,4,2021-09-14T00:07:41Z,2021-10-01T19:57:30Z,,NONE,,False,"### What changes were proposed in this pull request?

This PR proposes new algorithm to create &  store the constraints.
It tracks aliases in projection which eliminates the need of pessimistically generating all the permutations of a given constraint. It is also more effective in correctly identifying the filters which can be pruned , apart from minimizing the memory used as compared to the current code. This also has changes to push compound filters if the join condition is on multiple attributes and the constraint comprises of more than 1 attributes of the join conditions.

Presently I have kept the code which retains the old logic of constraint management along with the new logic. It is controlled by the sql conf property **spark.sql.optimizer.optimizedConstraintPropagation.enabled** which is by default true. Once the PR is approved it would make sense to remove the old code & merge the code of ConstraintSet into ExpressionSet and removing some if else blocks in the Optimizer & the function Optimizer.getAllConstraints and LogicalPlan.getAllValidConstraints.


The new logic is as follows:
In the class ConstraintSet which extends ExpressionSet, we track the aliases , along with the base constraint.
Any constraint which is added to the ConstraintSet is stored in the most canonicalized form (i.e consisting of only those attributes which are part of the output set and NOT the Alias's attribute).

for eg consider a hypothetical plan

>            Filter( z > 10 && a1 + b2 > 10)
                                 |
>         Projection1 ( a, a as a1, a as a2, b , b as b1, b as b2, c, a +b as z)
                                 |
 >                 Filter ( a + b > 10)
                                 |
 >              base relation (a, b, c, d)

At the node Filter the constraint set will just have constraint a + b > 10
At the Node Projection1 , the constraint set will have
constraint a + b > 10
and maintain following buffers
buff1 -> a , a1.attribute, a2. attribute
buff2 -> b, b1.attribute, b2.attribute
buff3 -> a + b, z.attribute

constraint a + b > 10 is already canonicalized in terms of output attributes.

Now there are two filters on top of projection1
Filter( z > 10) and Filter ( a1 + b2 > 10)

To prune the above two filters, we canonicalize z as a + b ( from the data maintained in the ConstraintSet) & check if the underlying set contains a +b > 10 & so can be pruned.
For Filter a1 + b2 > 10, we identify the buffer to which a1 & b2 belong to and replace it with 0th elements of the buffer, which will yield a +b > 10, and so filter can be pruned.

Now suppose there is another Project2 ( a1, a2, b1, b2, z, c)
i.e say attributes a & b are no longer part of OutputSet.

such that the plan looks like:
>         Projection2 ( a1,  a2,  b1,  b2, c,  z)
                                |
>            Filter( z > 10 && a1 + b2 > 10)
                                 |
>         Projection1 ( a, a as a1, a as a2, b , b as b1, b as b2, c, a +b as z)
                                 |
 >                 Filter ( a + b > 10)
                                 |
 >              base relation (a, b, c, d)

**The idea is that ""as much as possible try to make a constraint survive.**

So in Project2 , the atttributes a & b are being eliminated.
we have a constraint a + b > 10 which is dependent on it.
so in the ConstraintSet of the ProjectP2, we update it such that
constraint a + b > 10 becomes ----> a1 + b1 > 10
**buff1   ->  a , a1, a2   will become --> a1, a2
buff2  ->  b , b1, b2.  will become  --> b1, b2
buff3   ->  a +b , z  will become  -->. a1 + b1 , z**

This way by tracking aliases & just storing the canonicalized base constraints we can eliminate the need of pessimistically generating all combination of constraints.

**This PR also eliminates the need of EqualNullSafe constraints for the alias.
It also is able to handle the literal boolean constraints.**

**_ For inferring new Filter from constraints _**
we use following logic
New Filter = Filter.constraints -- ( Filter.child.constraints ++ Filter.constraints.convertToCanonicalizedIfRequired(Filter.conditions) )
So the idea is that new filter conditions without redundancy can be obtained by difference of current node's constraints & the child node's constraints & the condition itself properly canonicalized in terms of base attributes which will be part of the output set of filter node.

**_For inferring new filters for Join push down,_**
 we identify all the equality conditions & then the attributes are segregated on the lines of LHS & RHS of joins. So to identify filters for push down on RHS side, we get all equality atttributes of LHS side & ask the ConstraintSet to return all the constraints which are subset of the passed LHS attributes. The LHS attributes are appropriately canonicalized & the ConstraintSet identified.
Once the constraints are know, we can replace the attributes with the corresponding RHS attributes. This helps in identifying the compound filters for push down & not just single attribute filters.

_**Below is a description of the changes proposed.**_

ConstraintSet: This is the class which does the tracking of the aliases , stores the constraints in the canonicalized form, updates the constraints using available aliases if any of the attribute comprising constraint is getting eliminated. The contains method of this class is used for filter pruning. It also identifies those constraints which can generated new filters for push down in join nodes.
Rest all the changes are just to integrate the new logic as well as retain the old constraints logic.
Pls notice that related to tpcds plan stability , I had to add new golden files for q75. The change as such is trivial.
previously pushed filter was generated as
PushedFilters: [IsNotNull(cr_order_number), IsNotNull(cr_item_sk)]
and with the change it is
PushedFilters: [IsNotNull(cr_item_sk), IsNotNull(cr_order_number)]


### Does this PR introduce any user-facing change?
No


### Why are the changes needed?

1. This issue if not fixed can cause OutOfMemory issue or unacceptable query compilation times.
Added a test **""plan equivalence with case statements and performance comparison with benefit of more than 10x conservatively""  in  org.apache.spark.sql.catalyst.plans.OptimizedConstraintPropagationSuite**. With this PR the compilation _**time is 247 ms vs 13958 ms without the change**_
2. It is more effective in filter pruning as is evident in some of the tests in org.apache.spark.sql.catalyst.plans.OptimizedConstraintPropagationSuite where current code is not able to identify the redundant filter in some cases.
3. It is able to generate a better optimized plan for join queries as it can push compound predicates.
4. The current logic can miss a lot of possible cases of removing redundant predicates, as it fails to take into account if same attribute or its aliases are repeated multiple times in a complex expression.
5. There are cases where some of the optimizer rules involving removal of redundant predicates fail to remove on the basis of constraint data. In some cases the rule works, just by the virtue of previous rules helping it out to cover the inaccuracy. That the ConstraintPropagation rule & its function of removal of redundant filters & addition of new inferred filters is dependent on the working of some of the other unrelated previous optimizer rules is behaving, is indicative of issues.
6. It does away with all the EqualNullSafe constraints as this logic does not need those constraints to be created.
7.  There is atleast one test in existing **ConstraintPropagationSuite** which is missing a IsNotNull constraints because the code incorrectly generated a EqualsNullSafeConstraint instead of EqualTo constraint, when using the existing Constraints code.  With these changes, the test correctly creates an EqualTo constraint, resulting in an inferred IsNotNull constraint
8. It does away with the current combinatorial  logic of evaluation all the constraints can cause compilation to run into hours or cause OOM. The number of constraints stored is exactly the same as the number of filters encountered


### How was this patch tested?
Many new tests are added. All existing tests are passing cleanly.
Code is functional in our env. for many months  without any issue",https://api.github.com/repos/apache/spark/issues/33983/timeline,,spark,apache,ahshahid,12415848,MDQ6VXNlcjEyNDE1ODQ4,https://avatars.githubusercontent.com/u/12415848?v=4,,https://api.github.com/users/ahshahid,https://github.com/ahshahid,https://api.github.com/users/ahshahid/followers,https://api.github.com/users/ahshahid/following{/other_user},https://api.github.com/users/ahshahid/gists{/gist_id},https://api.github.com/users/ahshahid/starred{/owner}{/repo},https://api.github.com/users/ahshahid/subscriptions,https://api.github.com/users/ahshahid/orgs,https://api.github.com/users/ahshahid/repos,https://api.github.com/users/ahshahid/events{/privacy},https://api.github.com/users/ahshahid/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/33983,https://github.com/apache/spark/pull/33983,https://github.com/apache/spark/pull/33983.diff,https://github.com/apache/spark/pull/33983.patch,,https://api.github.com/repos/apache/spark/issues/33983/reactions,2,2,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
199,https://api.github.com/repos/apache/spark/issues/33980,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/33980/labels{/name},https://api.github.com/repos/apache/spark/issues/33980/comments,https://api.github.com/repos/apache/spark/issues/33980/events,https://github.com/apache/spark/pull/33980,994925162,MDExOlB1bGxSZXF1ZXN0NzMyNzc1NTk2,33980,[SPARK-32285][PYTHON] Add PySpark support for nested timestamps with arrow,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1982260031, 'node_id': 'MDU6TGFiZWwxOTgyMjYwMDMx', 'url': 'https://api.github.com/repos/apache/spark/labels/PYTHON', 'name': 'PYTHON', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,14,2021-09-13T13:57:23Z,2021-11-11T12:20:25Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?
Added nested timestamp support for Pyspark with Arrow
Following code will run through this PR


```
from pyspark.sql.types import StructType, TimestampType, StructField, ArrayType, LongType
spark.conf.set(""spark.sql.execution.arrow.pyspark.fallback.enabled"", ""False"")
spark.conf.set(""spark.sql.execution.arrow.pyspark.enabled"", ""True"")

import datetime
import pandas as pd
origin = pd.DataFrame({""a"": [[datetime.datetime(2012, 2, 2, 2, 2, 2)]]})
df = spark.createDataFrame(origin, schema=StructType([StructField(""a"", ArrayType(TimestampType()), True)]))
ts = datetime.datetime(2015, 11, 1, 0, 30)

schema = StructType([StructField(""a"", ArrayType(TimestampType()), True)])

df = spark.createDataFrame([([ts, ts],)], schema=schema)

df.toPandas()
```


### Why are the changes needed?
This change is required to convert ArrayType(TimeStamp) to pandas via arrow. 


### Does this PR introduce any user-facing change?
Yes user will be able to convert DF which contain Arraytype(Timestamp) to pandas

### How was this patch tested?
unit tests",https://api.github.com/repos/apache/spark/issues/33980/timeline,,spark,apache,pralabhkumar,16147255,MDQ6VXNlcjE2MTQ3MjU1,https://avatars.githubusercontent.com/u/16147255?v=4,,https://api.github.com/users/pralabhkumar,https://github.com/pralabhkumar,https://api.github.com/users/pralabhkumar/followers,https://api.github.com/users/pralabhkumar/following{/other_user},https://api.github.com/users/pralabhkumar/gists{/gist_id},https://api.github.com/users/pralabhkumar/starred{/owner}{/repo},https://api.github.com/users/pralabhkumar/subscriptions,https://api.github.com/users/pralabhkumar/orgs,https://api.github.com/users/pralabhkumar/repos,https://api.github.com/users/pralabhkumar/events{/privacy},https://api.github.com/users/pralabhkumar/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/33980,https://github.com/apache/spark/pull/33980,https://github.com/apache/spark/pull/33980.diff,https://github.com/apache/spark/pull/33980.patch,,https://api.github.com/repos/apache/spark/issues/33980/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
200,https://api.github.com/repos/apache/spark/issues/33941,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/33941/labels{/name},https://api.github.com/repos/apache/spark/issues/33941/comments,https://api.github.com/repos/apache/spark/issues/33941/events,https://github.com/apache/spark/pull/33941,991698454,MDExOlB1bGxSZXF1ZXN0NzMwMTI2MjI4,33941,[WIP][SPARK-36699][Core] Reuse compatible executors for stage-level scheduling,"[{'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,8,2021-09-09T02:43:22Z,2021-12-02T15:29:52Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?

https://issues.apache.org/jira/browse/SPARK-36699

We proposed to optionally change behavior of stage-level scheduling by reusing compatible executors. Two executors binding to different resource profiles are **compatible** only when the executorResources (cores in particular if not defining custom resources) are the same, but taskResources can be different. When the executors are compatible, the tasks can be allocated to any of them even when in the different profiles. Users defining profiles should make sure the different taskResources are properly specified against the same executorResources. 
A SparkConf option `spark.dynamicAllocation.reuseExecutors` is defined to change the default behavior which is not reusing executors. When this option is turned on, dynamic allocation will count all compatible executors number to meet init/min/max executor number restrictions. 
The first PR will focus on reusing executors with same cores without custom resources.

### Why are the changes needed?
Current stage-level scheduling allocated separated set of executors for different executor profiles. This approach simplified implementation, however is a waste of executor resources when the existing executors have enough resources to run the following tasks.

The typical user scenario is for different stages, user wants to use different core number for the task with same executor resources. For instance in CPU machine learning scenario, to achieve the best performance, given the same executor resources, when in ETL stage, user will allocate 1 core per task and many tasks, and in the following CPU training stage, user will use more cores per task and less tasks. In the existing implementation, two separated profiles and executors are created. Reusing executors will get better CPU resource utilization and better performance.

### Does this PR introduce _any_ user-facing change?
No

### How was this patch tested?
Unit tests

",https://api.github.com/repos/apache/spark/issues/33941/timeline,,spark,apache,xwu99,23566414,MDQ6VXNlcjIzNTY2NDE0,https://avatars.githubusercontent.com/u/23566414?v=4,,https://api.github.com/users/xwu99,https://github.com/xwu99,https://api.github.com/users/xwu99/followers,https://api.github.com/users/xwu99/following{/other_user},https://api.github.com/users/xwu99/gists{/gist_id},https://api.github.com/users/xwu99/starred{/owner}{/repo},https://api.github.com/users/xwu99/subscriptions,https://api.github.com/users/xwu99/orgs,https://api.github.com/users/xwu99/repos,https://api.github.com/users/xwu99/events{/privacy},https://api.github.com/users/xwu99/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/33941,https://github.com/apache/spark/pull/33941,https://github.com/apache/spark/pull/33941.diff,https://github.com/apache/spark/pull/33941.patch,,https://api.github.com/repos/apache/spark/issues/33941/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
201,https://api.github.com/repos/apache/spark/issues/33934,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/33934/labels{/name},https://api.github.com/repos/apache/spark/issues/33934/comments,https://api.github.com/repos/apache/spark/issues/33934/events,https://github.com/apache/spark/pull/33934,990760324,MDExOlB1bGxSZXF1ZXN0NzI5MzIyMDg5,33934,[SPARK-36691][PYTHON] PythonRunner failed should pass error message to ApplicationMaster too,"[{'id': 1427970157, 'node_id': 'MDU6TGFiZWwxNDI3OTcwMTU3', 'url': 'https://api.github.com/repos/apache/spark/labels/YARN', 'name': 'YARN', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,32,2021-09-08T06:21:37Z,2021-11-22T03:35:57Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?
In current pyspark, stderr and stdout are print together, if python script exit, PythonRunner will only throw a `SparkUserAppsException` with exit code 1. Then pass this error to AM.
In cluster mode, client side only got exception `SparkUserAppsException` and show
```
User application exited with 1.
```
Without correct error message. Then user need to  check ApplicationMaster's stdout log file to find out why their job failed. 

In this pr, make PythonRunner can throw exception message to backend.


### Why are the changes needed?
Make user to know error message more easy.


### Does this PR introduce _any_ user-facing change?
In cluster mode, user can directly see pyspark's error message in client side.

### How was this patch tested?
If we run a sql with wrong table in python script. In ApplicationMaster and client side log will show
```
21/09/08 14:08:42 ERROR Client: Application diagnostics message: User application exited with 1.
Exception in thread ""main"" org.apache.spark.SparkException: Application application_1630930053097_708441 finished with failed status
	at org.apache.spark.deploy.yarn.Client.run(Client.scala:1150)
	at org.apache.spark.deploy.yarn.YarnClusterApplication.start(Client.scala:1530)
	at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:845)
	at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:161)
	at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:184)
	at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:86)
	at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:920)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:929)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
```

Now will show
```
21/09/08 14:08:42 ERROR Client: Application diagnostics message: User application exited with 1 and error message Traceback (most recent call last):
  File ""test.py"", line 68, in <module>
    res = client.sql(exec_sql)
  File ""/mnt/ssd/0/yarn/nm-local-dir/usercache/yi.zhu/appcache/application_1630930053097_708441/container_e236_1630930053097_708441_02_000002/pyspark.zip/pyspark/sql/session.py"", line 767, in sql
  File ""/mnt/ssd/0/yarn/nm-local-dir/usercache/yi.zhu/appcache/application_1630930053097_708441/container_e236_1630930053097_708441_02_000002/py4j-0.10.7-src.zip/py4j/java_gateway.py"", line 1257, in __call__
  File ""/mnt/ssd/0/yarn/nm-local-dir/usercache/yi.zhu/appcache/application_1630930053097_708441/container_e236_1630930053097_708441_02_000002/pyspark.zip/pyspark/sql/utils.py"", line 69, in deco
pyspark.sql.utils.AnalysisException: u""Table or view not found: `xxxx`.`xxxxxx`; line 14 pos 9;\n'InsertIntoTable 'UnresolvedRelation `xxxx`.`xxxxxx`, Map(dt -> None, country -> None), true, false\n+- 'Repartition 50, true\n   +- 'Project [cast('get_json_object('data, $.shopid) as bigint) AS shopid#4, cast('get_json_object('data, $.itemid) as bigint) AS itemid#5, cast('get_json_object('data, $.quantity) as bigint) AS quantity#6, 'userid, 'platform, 'page_type, 'log_timestamp, 'utc_date AS dt#7, 'grass_region AS country#8]\n      +- 'Filter ((('utc_date = cast(2021-01-01 as date)) && ('grass_region = ID)) && ('operation = action_add_to_cart_success))\n         +- 'SubqueryAlias `di`\n            +- 'UnresolvedRelation `xxxxxx`.`xxxxxx`\n""

Exception in thread ""main"" org.apache.spark.SparkException: Application application_1630930053097_708441 finished with failed status
	at org.apache.spark.deploy.yarn.Client.run(Client.scala:1150)
	at org.apache.spark.deploy.yarn.YarnClusterApplication.start(Client.scala:1530)
	at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:845)
	at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:161)
	at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:184)
	at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:86)
	at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:920)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:929)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
```
",https://api.github.com/repos/apache/spark/issues/33934/timeline,,spark,apache,AngersZhuuuu,46485123,MDQ6VXNlcjQ2NDg1MTIz,https://avatars.githubusercontent.com/u/46485123?v=4,,https://api.github.com/users/AngersZhuuuu,https://github.com/AngersZhuuuu,https://api.github.com/users/AngersZhuuuu/followers,https://api.github.com/users/AngersZhuuuu/following{/other_user},https://api.github.com/users/AngersZhuuuu/gists{/gist_id},https://api.github.com/users/AngersZhuuuu/starred{/owner}{/repo},https://api.github.com/users/AngersZhuuuu/subscriptions,https://api.github.com/users/AngersZhuuuu/orgs,https://api.github.com/users/AngersZhuuuu/repos,https://api.github.com/users/AngersZhuuuu/events{/privacy},https://api.github.com/users/AngersZhuuuu/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/33934,https://github.com/apache/spark/pull/33934,https://github.com/apache/spark/pull/33934.diff,https://github.com/apache/spark/pull/33934.patch,,https://api.github.com/repos/apache/spark/issues/33934/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
202,https://api.github.com/repos/apache/spark/issues/33932,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/33932/labels{/name},https://api.github.com/repos/apache/spark/issues/33932/comments,https://api.github.com/repos/apache/spark/issues/33932/events,https://github.com/apache/spark/pull/33932,990713129,MDExOlB1bGxSZXF1ZXN0NzI5MjgxOTA3,33932,[SPARK-33781][SHUFFLE] Improve caching of MergeStatus on the executor side to save memory,"[{'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,3,2021-09-08T04:52:23Z,2021-10-14T17:10:54Z,,CONTRIBUTOR,,False,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->
This is one of the patches for SPARK-33235: Push-based Shuffle Improvement Tasks.
At high level, in `MapOutputTrackerWorker`, if serialized `MergeStatuse` array size is larger than threshold(`spark.shuffle.push.mergeResult.minSizeForReducedCache`), cache the much more compact serialized bytes instead and only cache the deserialized `MergeStatus` objects that are needed (within `startPartitionId` until `endPartitionId`). Then deserialize the `MergeStatus` array from the cached serialized bytes again.


### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->
For large shuffles with 10s or 100s of thousands of shuffle partitions, caching the entire deserialized and decompressed MergeStatus array on the executor side, while perhaps only 0.1% of them are going to be used by the tasks running in this executor is a huge waste of memory.
This change helps save memory as well as helps with reducing GC pressure on executor side.


### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->
Yes. This PR introduces a client-side config for push-based shuffle(`spark.shuffle.push.mergeResult.minSizeForReducedCache`). If push-based shuffle is turned-off then the users will not see any change.


### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->
Added unit test.
Verified effectiveness with jobs that would fail due to GC issue otherwise.

Ran the benchmark using GitHub Actions and did not observe any performance penalties. The results are attached in this PR:
```
core/benchmarks/MapStatusesSerDeserBenchmark-jdk11-results.txt
core/benchmarks/MapStatusesSerDeserBenchmark-results.txt
```

Lead-authored-by: Min Shen mshen@linkedin.com
Co-authored-by: Chandni Singh chsingh@linkedin.com
Co-authored-by: Minchu Yang minyang@linkedin.com",https://api.github.com/repos/apache/spark/issues/33932/timeline,,spark,apache,rmcyang,31781684,MDQ6VXNlcjMxNzgxNjg0,https://avatars.githubusercontent.com/u/31781684?v=4,,https://api.github.com/users/rmcyang,https://github.com/rmcyang,https://api.github.com/users/rmcyang/followers,https://api.github.com/users/rmcyang/following{/other_user},https://api.github.com/users/rmcyang/gists{/gist_id},https://api.github.com/users/rmcyang/starred{/owner}{/repo},https://api.github.com/users/rmcyang/subscriptions,https://api.github.com/users/rmcyang/orgs,https://api.github.com/users/rmcyang/repos,https://api.github.com/users/rmcyang/events{/privacy},https://api.github.com/users/rmcyang/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/33932,https://github.com/apache/spark/pull/33932,https://github.com/apache/spark/pull/33932.diff,https://github.com/apache/spark/pull/33932.patch,,https://api.github.com/repos/apache/spark/issues/33932/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
203,https://api.github.com/repos/apache/spark/issues/33914,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/33914/labels{/name},https://api.github.com/repos/apache/spark/issues/33914/comments,https://api.github.com/repos/apache/spark/issues/33914/events,https://github.com/apache/spark/pull/33914,988464013,MDExOlB1bGxSZXF1ZXN0NzI3MzY4MDY0,33914,[SPARK-32268][SQL] Dynamic bloom filter join pruning,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,20,2021-09-05T09:26:44Z,2021-12-10T02:25:31Z,,MEMBER,,True,"### What changes were proposed in this pull request?

Reduce the shuffle data can significantly improve the query performance and increase Spark cluster stability.

This PR implements dynamic bloom filter join pruning to reduce the shuffle data. The main changes:
- Add `BuildBloomFilter` and `InBloomFilter` UDF.
- Enhance `RepartitionByExpression` to support shuffle with `ENSURE_REQUIREMENTS` origin. Dynamic bloom filter join pruning use it to reuse exchange.
- Add a new rule `DynamicBloomFilterPruning`.
- Dynamic bloom filter join pruning supports AQE.
- Support dynamic bloom filter nested with dynamic partition pruning when AQE enabled.
- Add an AQE optimization rule(`OptimizeBloomFilterJoin`) to update the `expectedNumItems` base on stats collected by AQE which is more accurate.



### Why are the changes needed?

Improve query performance.

### Does this PR introduce _any_ user-facing change?

No.

### How was this patch tested?

Unit test and benchmark test:

SQL | Before this PR | After this PR
--- | --- | ---
q12 | 11 | 8
q20 | 6 | 6
q37 | 29 | 13
q50 | 78 | 29
q80 | 14 | 14
q82 | 38 | 16
q93 | 126 | 108
q98 | 6 | 6



</body>

</html>
",https://api.github.com/repos/apache/spark/issues/33914/timeline,,spark,apache,wangyum,5399861,MDQ6VXNlcjUzOTk4NjE=,https://avatars.githubusercontent.com/u/5399861?v=4,,https://api.github.com/users/wangyum,https://github.com/wangyum,https://api.github.com/users/wangyum/followers,https://api.github.com/users/wangyum/following{/other_user},https://api.github.com/users/wangyum/gists{/gist_id},https://api.github.com/users/wangyum/starred{/owner}{/repo},https://api.github.com/users/wangyum/subscriptions,https://api.github.com/users/wangyum/orgs,https://api.github.com/users/wangyum/repos,https://api.github.com/users/wangyum/events{/privacy},https://api.github.com/users/wangyum/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/33914,https://github.com/apache/spark/pull/33914,https://github.com/apache/spark/pull/33914.diff,https://github.com/apache/spark/pull/33914.patch,,https://api.github.com/repos/apache/spark/issues/33914/reactions,5,4,0,0,0,0,1,0,0,,,,,,,,,,,,,,,,,,
204,https://api.github.com/repos/apache/spark/issues/33905,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/33905/labels{/name},https://api.github.com/repos/apache/spark/issues/33905/comments,https://api.github.com/repos/apache/spark/issues/33905/events,https://github.com/apache/spark/pull/33905,987363612,MDExOlB1bGxSZXF1ZXN0NzI2NDYwMzY5,33905,[SPARK-36658][SQL] Expose execution id to QueryExecutionListener,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,4,2021-09-03T03:35:23Z,2021-09-27T08:10:05Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?
Now in QueryExecutionListener we have exposed API to get the query execution information:
def onSuccess(funcName: String, qe: QueryExecution, durationNs: Long): Unit
def onFailure(funcName: String, qe: QueryExecution, exception: Exception): Unit

But we can not get a clear information that which query is this. In Spark SQL, I think that executionId is the identifier of a query execution. It makes sense to expose executionId to the QueryExecutionListener, so that people can easily find the exact query in UI or history server to track more information of the query execution.

### Why are the changes needed?
Easier to find the matching query in UI. 

### Does this PR introduce _any_ user-facing change?
Developer Api change.

### How was this patch tested?
Existing UTs.
",https://api.github.com/repos/apache/spark/issues/33905/timeline,,spark,apache,ivoson,15122230,MDQ6VXNlcjE1MTIyMjMw,https://avatars.githubusercontent.com/u/15122230?v=4,,https://api.github.com/users/ivoson,https://github.com/ivoson,https://api.github.com/users/ivoson/followers,https://api.github.com/users/ivoson/following{/other_user},https://api.github.com/users/ivoson/gists{/gist_id},https://api.github.com/users/ivoson/starred{/owner}{/repo},https://api.github.com/users/ivoson/subscriptions,https://api.github.com/users/ivoson/orgs,https://api.github.com/users/ivoson/repos,https://api.github.com/users/ivoson/events{/privacy},https://api.github.com/users/ivoson/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/33905,https://github.com/apache/spark/pull/33905,https://github.com/apache/spark/pull/33905.diff,https://github.com/apache/spark/pull/33905.patch,,https://api.github.com/repos/apache/spark/issues/33905/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
205,https://api.github.com/repos/apache/spark/issues/33896,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/33896/labels{/name},https://api.github.com/repos/apache/spark/issues/33896/comments,https://api.github.com/repos/apache/spark/issues/33896/events,https://github.com/apache/spark/pull/33896,986156305,MDExOlB1bGxSZXF1ZXN0NzI1Mzg5Nzc4,33896,[SPARK-33701][SHUFFLE] Adaptive shuffle merge finalization for push-based shuffle,"[{'id': 1981471081, 'node_id': 'MDU6TGFiZWwxOTgxNDcxMDgx', 'url': 'https://api.github.com/repos/apache/spark/labels/DOCS', 'name': 'DOCS', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,74,2021-09-02T05:21:50Z,2021-12-31T04:47:53Z,,CONTRIBUTOR,,False,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->
As part of SPARK-32920 implemented a simple approach to finalization for push-based shuffle. Shuffle merge finalization is the final operation happens at the end of the stage when all the tasks are completed asking all the external shuffle services to complete the shuffle merge for the stage. Once this request is completed no more shuffle pushes will be accepted. With this approach, `DAGScheduler` waits for a fixed time of 10s (`spark.shuffle.push.finalize.timeout`) to allow some time for the inflight shuffle pushes to complete, but this adds additional overhead to stages with very little shuffles.

In this PR, instead of waiting for fixed amount of time before shuffle merge finalization now this is controlled adaptively if min threshold number of map tasks shuffle push (`spark.shuffle.push.minPushRatio`) completed then shuffle merge finalization will be scheduled. Also additionally if the total shuffle generated is lesser than min threshold shuffle size (`spark.shuffle.push.minShuffleSizeToWait`) then immediately shuffle merge finalization is scheduled.
### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->
This is a performance improvement to the existing functionality

### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->
Yes additional user facing configs `spark.shuffle.push.minPushRatio` and `spark.shuffle.push.minShuffleSizeToWait`

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->
Added unit tests in `DAGSchedulerSuite`, `ShuffleBlockPusherSuite`

Lead-authored-by: Min Shen <mshen@linkedin.com>
Co-authored-by: Venkata krishnan Sowrirajan <vsowrirajan@linkedin.com>",https://api.github.com/repos/apache/spark/issues/33896/timeline,,spark,apache,venkata91,8871522,MDQ6VXNlcjg4NzE1MjI=,https://avatars.githubusercontent.com/u/8871522?v=4,,https://api.github.com/users/venkata91,https://github.com/venkata91,https://api.github.com/users/venkata91/followers,https://api.github.com/users/venkata91/following{/other_user},https://api.github.com/users/venkata91/gists{/gist_id},https://api.github.com/users/venkata91/starred{/owner}{/repo},https://api.github.com/users/venkata91/subscriptions,https://api.github.com/users/venkata91/orgs,https://api.github.com/users/venkata91/repos,https://api.github.com/users/venkata91/events{/privacy},https://api.github.com/users/venkata91/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/33896,https://github.com/apache/spark/pull/33896,https://github.com/apache/spark/pull/33896.diff,https://github.com/apache/spark/pull/33896.patch,,https://api.github.com/repos/apache/spark/issues/33896/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
206,https://api.github.com/repos/apache/spark/issues/33893,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/33893/labels{/name},https://api.github.com/repos/apache/spark/issues/33893/comments,https://api.github.com/repos/apache/spark/issues/33893/events,https://github.com/apache/spark/pull/33893,985082957,MDExOlB1bGxSZXF1ZXN0NzI0NDM3OTE4,33893,[SPARK-36638][SQL] Generalize OptimizeSkewedJoin,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,48,2021-09-01T12:19:25Z,2021-12-22T18:49:08Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?
This PR aims to generalize `OptimizeSkewedJoin` to support all patterns that can be handled by current _split-duplicate_ strategy:

1, find the _splittable_ shuffle query stages by the semantics of internal nodes;

2, for each _splittable_ shuffle query stage, check whether skew partitions exists, if true, split them into specs;

3, handle _Combinatorial Explosion_: for each skew partition, check whether the combination number is too large, if so, re-split the stages to keep a reasonable number of combinations. For example, for partition 0, stage A/B/C are split into 100/100/100 specs, respectively. Then there are 1M combinations, which is too large, and will cause performance regression.

4, attach new specs to shuffle query stages;


### Why are the changes needed?
to Generalize OptimizeSkewedJoin 


### Does this PR introduce _any_ user-facing change?
two additional configs are added


### How was this patch tested?
existing testsuites, added testsuites, some cases on our productive system
",https://api.github.com/repos/apache/spark/issues/33893/timeline,,spark,apache,zhengruifeng,7322292,MDQ6VXNlcjczMjIyOTI=,https://avatars.githubusercontent.com/u/7322292?v=4,,https://api.github.com/users/zhengruifeng,https://github.com/zhengruifeng,https://api.github.com/users/zhengruifeng/followers,https://api.github.com/users/zhengruifeng/following{/other_user},https://api.github.com/users/zhengruifeng/gists{/gist_id},https://api.github.com/users/zhengruifeng/starred{/owner}{/repo},https://api.github.com/users/zhengruifeng/subscriptions,https://api.github.com/users/zhengruifeng/orgs,https://api.github.com/users/zhengruifeng/repos,https://api.github.com/users/zhengruifeng/events{/privacy},https://api.github.com/users/zhengruifeng/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/33893,https://github.com/apache/spark/pull/33893,https://github.com/apache/spark/pull/33893.diff,https://github.com/apache/spark/pull/33893.patch,,https://api.github.com/repos/apache/spark/issues/33893/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
207,https://api.github.com/repos/apache/spark/issues/33888,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/33888/labels{/name},https://api.github.com/repos/apache/spark/issues/33888/comments,https://api.github.com/repos/apache/spark/issues/33888/events,https://github.com/apache/spark/pull/33888,984860912,MDExOlB1bGxSZXF1ZXN0NzI0MjQ3OTIx,33888,[SPARK-36634][SQL] Support access and read parquet file by column ordinal,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,36,2021-09-01T08:24:43Z,2021-10-07T16:50:28Z,,CONTRIBUTOR,,False,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->

Add a config `spark.sql.parquet.accessByColumnOrdinal` 

When true, we access the parquet files by column original instead of catalyst schema mapping at the executor side. 

This is useful when the parquet file meta is inconsistent with those in Metastore, e.g. creating a table with existing parquet files with column names renamed, the column names are changed by external systems.

- What if the number of columns/inner fields does not match?
  - if the number of requests columns (M)is greater than the one(N) in the parquet file, the [M, N-1] of the outputs will be filled with nulls, which is also the same as the current name-based mapping
  - if the number of requests columns (M)is less than or equal to the one(N) in the parquet file, the [0, M-1] of fields will be token in order.
- What if the types are not compatible?
  - In this case, it throws unsupportedSchemaColumnConvertError 

By default, it is off

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->

- What's the concrete use case that requires this feature?

Parquet's schema evolution is implementation-dependent and may causes inconsistency from file to file or file to metastore. Sometimes, 1) the table Spark's processing might be produced by other systems, e.g. renamed by hive https://issues.apache.org/jira/browse/HIVE-6938, 2)some operations that do not introduce a force schema checking, e.g. `set location, `add partition`. With this feature, the users as data consumers can still read those data produced by different providers.

See also, https://issues.apache.org/jira/browse/IMPALA-2835


better data accessibility 

### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->

NO

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->

newly added test",https://api.github.com/repos/apache/spark/issues/33888/timeline,,spark,apache,yaooqinn,8326978,MDQ6VXNlcjgzMjY5Nzg=,https://avatars.githubusercontent.com/u/8326978?v=4,,https://api.github.com/users/yaooqinn,https://github.com/yaooqinn,https://api.github.com/users/yaooqinn/followers,https://api.github.com/users/yaooqinn/following{/other_user},https://api.github.com/users/yaooqinn/gists{/gist_id},https://api.github.com/users/yaooqinn/starred{/owner}{/repo},https://api.github.com/users/yaooqinn/subscriptions,https://api.github.com/users/yaooqinn/orgs,https://api.github.com/users/yaooqinn/repos,https://api.github.com/users/yaooqinn/events{/privacy},https://api.github.com/users/yaooqinn/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/33888,https://github.com/apache/spark/pull/33888,https://github.com/apache/spark/pull/33888.diff,https://github.com/apache/spark/pull/33888.patch,,https://api.github.com/repos/apache/spark/issues/33888/reactions,1,0,0,0,0,0,0,0,1,,,,,,,,,,,,,,,,,,
208,https://api.github.com/repos/apache/spark/issues/33878,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/33878/labels{/name},https://api.github.com/repos/apache/spark/issues/33878/comments,https://api.github.com/repos/apache/spark/issues/33878/events,https://github.com/apache/spark/pull/33878,983990492,MDExOlB1bGxSZXF1ZXN0NzIzNDgyNjIy,33878,[SPARK-36303][SQL] Refactor fourthteenth set of 20 query execution errors to use error classes,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,3,2021-08-31T15:22:54Z,2021-09-28T01:30:09Z,,CONTRIBUTOR,,False,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->


### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->Adds error classes to some of the exceptions in QueryExecutionErrors.


### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->Improves auditing for developers and adds useful fields for users (error class and SQLSTATE).


### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->Existing tests
",https://api.github.com/repos/apache/spark/issues/33878/timeline,,spark,apache,dgd-contributor,84778052,MDQ6VXNlcjg0Nzc4MDUy,https://avatars.githubusercontent.com/u/84778052?v=4,,https://api.github.com/users/dgd-contributor,https://github.com/dgd-contributor,https://api.github.com/users/dgd-contributor/followers,https://api.github.com/users/dgd-contributor/following{/other_user},https://api.github.com/users/dgd-contributor/gists{/gist_id},https://api.github.com/users/dgd-contributor/starred{/owner}{/repo},https://api.github.com/users/dgd-contributor/subscriptions,https://api.github.com/users/dgd-contributor/orgs,https://api.github.com/users/dgd-contributor/repos,https://api.github.com/users/dgd-contributor/events{/privacy},https://api.github.com/users/dgd-contributor/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/33878,https://github.com/apache/spark/pull/33878,https://github.com/apache/spark/pull/33878.diff,https://github.com/apache/spark/pull/33878.patch,,https://api.github.com/repos/apache/spark/issues/33878/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
209,https://api.github.com/repos/apache/spark/issues/33839,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/33839/labels{/name},https://api.github.com/repos/apache/spark/issues/33839/comments,https://api.github.com/repos/apache/spark/issues/33839/events,https://github.com/apache/spark/pull/33839,979434339,MDExOlB1bGxSZXF1ZXN0NzE5ODQ3OTQx,33839,[SPARK-36291][SQL] Refactor second set of 20 in QueryExecutionErrors to use error classes,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,8,2021-08-25T17:00:58Z,2021-09-27T01:03:58Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?

Refactor some exceptions in QueryExecutionErrors to use error classes.

```
inputTypeUnsupportedError
invalidFractionOfSecondError
overflowInSumOfDecimalError
overflowInIntegralDivideError
mapSizeExceedArraySizeWhenZipMapError
copyNullFieldNotAllowedError
literalTypeUnsupportedError
noDefaultForDataTypeError
doGenCodeOfAliasShouldNotBeCalledError
orderedOperationUnsupportedByDataTypeError
regexGroupIndexLessThanZeroError
regexGroupIndexExceedGroupCountError
invalidUrlError
dataTypeOperationUnsupportedError
mergeUnsupportedByWindowFunctionError
dataTypeUnexpectedError
typeUnsupportedError
negativeValueUnexpectedError
addNewFunctionMismatchedWithFunctionError
cannotGenerateCodeForUncomparableTypeError
```
### Why are the changes needed?

There are currently ~350 exceptions in this file; so this PR only focuses on the second set of 20.

### Does this PR introduce _any_ user-facing change?
No
### How was this patch tested?
Existed UT Testcase",https://api.github.com/repos/apache/spark/issues/33839/timeline,,spark,apache,dgd-contributor,84778052,MDQ6VXNlcjg0Nzc4MDUy,https://avatars.githubusercontent.com/u/84778052?v=4,,https://api.github.com/users/dgd-contributor,https://github.com/dgd-contributor,https://api.github.com/users/dgd-contributor/followers,https://api.github.com/users/dgd-contributor/following{/other_user},https://api.github.com/users/dgd-contributor/gists{/gist_id},https://api.github.com/users/dgd-contributor/starred{/owner}{/repo},https://api.github.com/users/dgd-contributor/subscriptions,https://api.github.com/users/dgd-contributor/orgs,https://api.github.com/users/dgd-contributor/repos,https://api.github.com/users/dgd-contributor/events{/privacy},https://api.github.com/users/dgd-contributor/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/33839,https://github.com/apache/spark/pull/33839,https://github.com/apache/spark/pull/33839.diff,https://github.com/apache/spark/pull/33839.patch,,https://api.github.com/repos/apache/spark/issues/33839/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
210,https://api.github.com/repos/apache/spark/issues/33828,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/33828/labels{/name},https://api.github.com/repos/apache/spark/issues/33828/comments,https://api.github.com/repos/apache/spark/issues/33828/events,https://github.com/apache/spark/pull/33828,978663528,MDExOlB1bGxSZXF1ZXN0NzE5MjE1ODU3,33828,[SPARK-36579][CORE][SQL] Make spark source stagingDir can be customized,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1406587328, 'node_id': 'MDU6TGFiZWwxNDA2NTg3MzI4', 'url': 'https://api.github.com/repos/apache/spark/labels/STRUCTURED%20STREAMING', 'name': 'STRUCTURED STREAMING', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,87,2021-08-25T03:14:48Z,2021-12-13T06:12:09Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?
Consider such cases:

1. we close a job when it is doing dynamic partition insert, it will remain such staging dir under table's path.  So we make the staging dir customized like hive can avoid remain such staging dir under table path.
2. In hive's API, if we specify a staging dir, not use default staging dir (under table path), it can directly rename to target path and can avoid many hdfs file operations. In spark currently only dynamic partition insert support staging dir, we can do this like https://github.com/apache/spark/pull/33811
3. We can support add a file commit protocol that support staging dir for all types of insert, then when we use that commit protocol, wen can do:
    - Insert into non-partitioned table form it self
    - Insert into partition table's statistic partition and read data from target partition
    - Insert into different partition using statistic partition together

### Why are the changes needed?
Make spark data source insert's  stagingDir can be customized and then we can do more optimize base on this.


### Does this PR introduce _any_ user-facing change?
User can define staging dir by `spark.exec.stagingDir`

### How was this patch tested?
Added UT
",https://api.github.com/repos/apache/spark/issues/33828/timeline,,spark,apache,AngersZhuuuu,46485123,MDQ6VXNlcjQ2NDg1MTIz,https://avatars.githubusercontent.com/u/46485123?v=4,,https://api.github.com/users/AngersZhuuuu,https://github.com/AngersZhuuuu,https://api.github.com/users/AngersZhuuuu/followers,https://api.github.com/users/AngersZhuuuu/following{/other_user},https://api.github.com/users/AngersZhuuuu/gists{/gist_id},https://api.github.com/users/AngersZhuuuu/starred{/owner}{/repo},https://api.github.com/users/AngersZhuuuu/subscriptions,https://api.github.com/users/AngersZhuuuu/orgs,https://api.github.com/users/AngersZhuuuu/repos,https://api.github.com/users/AngersZhuuuu/events{/privacy},https://api.github.com/users/AngersZhuuuu/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/33828,https://github.com/apache/spark/pull/33828,https://github.com/apache/spark/pull/33828.diff,https://github.com/apache/spark/pull/33828.patch,,https://api.github.com/repos/apache/spark/issues/33828/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
211,https://api.github.com/repos/apache/spark/issues/33820,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/33820/labels{/name},https://api.github.com/repos/apache/spark/issues/33820/comments,https://api.github.com/repos/apache/spark/issues/33820/events,https://github.com/apache/spark/pull/33820,977839928,MDExOlB1bGxSZXF1ZXN0NzE4NTI1NDAy,33820,[WIP][SPARK-36571][SQL] Add new NewSQLHadoopMapReduceCommitProtocol resolve conflict when write into partition table's different partition,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,36,2021-08-24T08:22:36Z,2021-10-15T10:20:14Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?
For current data source insert commit protocol, it have such problem:
  case a: both job a and job b write data into partitioned table a with different statistic partition, it will have conflict since they use same tmp location `${table_location}/_temporary/0/....`, when job a finish it will clean this location, may cause clear job b's data.
  case b: for current dynamic partition insert, if we kill a job is writing data, will remain data under table location.


For current non dynamic partition insert in v1, it have step:
    1.  Task attempts firstly write files under the  intermediate path, e.g. `/path/to/outputPath/_temporary/{appAttemptId}/_temporary/{taskAttemptId}/{part_spec_path}/xxx.parquet`.
    2. Then task commit file to  `/path/to/outputPath/_temporary/{appId}/_temporary/{taskId}/{part_spec_path}/xxx.parquet`.
    3. Job commit move file to `/path/to/outputPath/{part_spec_path}/xxx.parquet`.

For current dynamic partition insert in v1, it have step:
    1.  Task attempts firstly write files under the  intermediate path, e.g.` /path/to/outputPath/.spark-staging-{jobId}/_temporary/{appAttemptId}/_temporary/{taskAttemptId}/a=1/b=1/xxx.parquet`.
    2. Then task commit file to  `/path/to/outputPath/.spark-staging-{jobId}/_temporary/{appId}/_temporary/{taskId}/a=1/b=1/xxx.parquet`.
    3. Job commit move file to `/path/to/outputPath/a=1/b=1/xxx.parquet`.

In this pr's  `SQLPathHadoopMapReduceCommitProtocol `, for all type of insert:
  1.  Task attempts firstly write files under the  intermediate path, e.g. `{staging_dir}/_temporary/{appAttemptId}/_temporary/{taskAttemptId}/{part_spec_path}/xxx.parquet`.
  2. Then task commit file to  `{staging_dir}/_temporary/{appId}/_temporary/{taskId}/{part_spec_path}/xxx.parquet`.
  3. Job commit move file to `/path/to/outputPath/{part_spec_path}/xxx.parquet`.

Benefit:
   1. We may don't need to logic in HadoopMapReduceCommitProtocol to handle file for dynamic partition insert
   2. Can support concurrent insert into partitioned table with statistic partition
   3. If we use staging dir not under target path, we can we many optimize about file operation such as just mv folder or just rename folder.

### Why are the changes needed?
Provide a more flexible commit protocol and won't impact perf


### Does this PR introduce _any_ user-facing change?
User can set sql commit protocol to `SQLPathHadoopMapReduceCommitProtocol` to use a commit protocol with staging dir


### How was this patch tested?
Added UT
",https://api.github.com/repos/apache/spark/issues/33820/timeline,,spark,apache,AngersZhuuuu,46485123,MDQ6VXNlcjQ2NDg1MTIz,https://avatars.githubusercontent.com/u/46485123?v=4,,https://api.github.com/users/AngersZhuuuu,https://github.com/AngersZhuuuu,https://api.github.com/users/AngersZhuuuu/followers,https://api.github.com/users/AngersZhuuuu/following{/other_user},https://api.github.com/users/AngersZhuuuu/gists{/gist_id},https://api.github.com/users/AngersZhuuuu/starred{/owner}{/repo},https://api.github.com/users/AngersZhuuuu/subscriptions,https://api.github.com/users/AngersZhuuuu/orgs,https://api.github.com/users/AngersZhuuuu/repos,https://api.github.com/users/AngersZhuuuu/events{/privacy},https://api.github.com/users/AngersZhuuuu/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/33820,https://github.com/apache/spark/pull/33820,https://github.com/apache/spark/pull/33820.diff,https://github.com/apache/spark/pull/33820.patch,,https://api.github.com/repos/apache/spark/issues/33820/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
212,https://api.github.com/repos/apache/spark/issues/33744,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/33744/labels{/name},https://api.github.com/repos/apache/spark/issues/33744/comments,https://api.github.com/repos/apache/spark/issues/33744/events,https://github.com/apache/spark/pull/33744,970956310,MDExOlB1bGxSZXF1ZXN0NzEyODExMjI1,33744,[SPARK-36403][PYTHON] Implement `Index.putmask` ,"[{'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1982260031, 'node_id': 'MDU6TGFiZWwxOTgyMjYwMDMx', 'url': 'https://api.github.com/repos/apache/spark/labels/PYTHON', 'name': 'PYTHON', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,44,2021-08-14T17:04:02Z,2021-11-12T15:12:10Z,,NONE,,False,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->

Implement `Index.putmask`

This pull request is based on https://github.com/databricks/koalas/pull/1560


### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->

`putmask` returns a new Index of the values set with the mask.
`putmask` is supported in pandas. PySpark should support that as well.

### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->

Yes. `Index.putmask` can be used.
```python
>>> pidx = pd.Index([""a"", ""b"", ""c"", ""d"", ""e""])
>>> psidx = ps.from_pandas(pidx)
>>> psidx.putmask(psidx < ""c"", ""k"").sort_values()
Index(['c', 'd', 'e', 'k', 'k'], dtype='object')
>>> psidx.putmask(psidx < ""c"", [""g"", ""h"", ""i"", ""j"", ""k""]).sort_values()
Index(['c', 'd', 'e', 'g', 'h'], dtype='object')
>>> psidx.putmask(psidx < ""c"", (""g"", ""h"", ""i"", ""j"", ""k"")).sort_values()
Index(['c', 'd', 'e', 'g', 'h'], dtype='object')
>>> psidx.putmask(psidx < ""c"", ps.Index([""g"", ""h"", ""i"", ""j"", ""k""])).sort_values()
Index(['c', 'd', 'e', 'g', 'h'], dtype='object')
>>> psidx.putmask(psidx < ""c"", ""MASKED"").sort_values()
Index(['MASKED', 'MASKED', 'c', 'd', 'e'], dtype='object')
```


### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->

Unit tests.
",https://api.github.com/repos/apache/spark/issues/33744/timeline,,spark,apache,beobest2,7010554,MDQ6VXNlcjcwMTA1NTQ=,https://avatars.githubusercontent.com/u/7010554?v=4,,https://api.github.com/users/beobest2,https://github.com/beobest2,https://api.github.com/users/beobest2/followers,https://api.github.com/users/beobest2/following{/other_user},https://api.github.com/users/beobest2/gists{/gist_id},https://api.github.com/users/beobest2/starred{/owner}{/repo},https://api.github.com/users/beobest2/subscriptions,https://api.github.com/users/beobest2/orgs,https://api.github.com/users/beobest2/repos,https://api.github.com/users/beobest2/events{/privacy},https://api.github.com/users/beobest2/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/33744,https://github.com/apache/spark/pull/33744,https://github.com/apache/spark/pull/33744.diff,https://github.com/apache/spark/pull/33744.patch,,https://api.github.com/repos/apache/spark/issues/33744/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
213,https://api.github.com/repos/apache/spark/issues/33723,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/33723/labels{/name},https://api.github.com/repos/apache/spark/issues/33723/comments,https://api.github.com/repos/apache/spark/issues/33723/events,https://github.com/apache/spark/pull/33723,968682497,MDExOlB1bGxSZXF1ZXN0NzEwNzgyNjMy,33723,[SPARK-36496][SQL] Remove literals from grouping expressions when using the DataFrame withColumn API,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,9,2021-08-12T12:00:32Z,2021-10-19T09:37:09Z,,CONTRIBUTOR,,False,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
Move the `RemoveLiteralFromGroupExpressions` and `RemoveRepetitionFromGroupExpressions` rules from a separate batch to the `operatorOptimizationBatch`.

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->
The `RemoveLiteralFromGroupExpressions` does not work in some cases if it is in a separate batch.
The added UT would fail with:
```
[info] - SPARK-36496: Remove literals from grouping expressions *** FAILED *** (2 seconds, 955 milliseconds)
[info]   == FAIL: Plans do not match ===
[info]   !Aggregate [*id#0L, null], [*id#0L, null AS a#0, count(1) AS count#0L]   Aggregate [*id#0L], [*id#0L, null AS a#0, count(1) AS count#0L]
[info]    +- Range (0, 100, step=1, splits=Some(2))                               +- Range (0, 100, step=1, splits=Some(2)) (PlanTest.scala:174)
```

### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->
No

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->
New UT",https://api.github.com/repos/apache/spark/issues/33723/timeline,,spark,apache,tanelk,3342974,MDQ6VXNlcjMzNDI5NzQ=,https://avatars.githubusercontent.com/u/3342974?v=4,,https://api.github.com/users/tanelk,https://github.com/tanelk,https://api.github.com/users/tanelk/followers,https://api.github.com/users/tanelk/following{/other_user},https://api.github.com/users/tanelk/gists{/gist_id},https://api.github.com/users/tanelk/starred{/owner}{/repo},https://api.github.com/users/tanelk/subscriptions,https://api.github.com/users/tanelk/orgs,https://api.github.com/users/tanelk/repos,https://api.github.com/users/tanelk/events{/privacy},https://api.github.com/users/tanelk/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/33723,https://github.com/apache/spark/pull/33723,https://github.com/apache/spark/pull/33723.diff,https://github.com/apache/spark/pull/33723.patch,,https://api.github.com/repos/apache/spark/issues/33723/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
214,https://api.github.com/repos/apache/spark/issues/33675,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/33675/labels{/name},https://api.github.com/repos/apache/spark/issues/33675/comments,https://api.github.com/repos/apache/spark/issues/33675/events,https://github.com/apache/spark/pull/33675,963293573,MDExOlB1bGxSZXF1ZXN0NzA1OTM4MDYx,33675,[SPARK-27997][K8S] Add support for kubernetes OAuth Token refresh,"[{'id': 1406605057, 'node_id': 'MDU6TGFiZWwxNDA2NjA1MDU3', 'url': 'https://api.github.com/repos/apache/spark/labels/KUBERNETES', 'name': 'KUBERNETES', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981471081, 'node_id': 'MDU6TGFiZWwxOTgxNDcxMDgx', 'url': 'https://api.github.com/repos/apache/spark/labels/DOCS', 'name': 'DOCS', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,31,2021-08-07T19:52:11Z,2021-12-14T16:17:11Z,,NONE,,False,"### What changes were proposed in this pull request?

This change allows a spark user to provide a class which implements fabric's OAuthTokenProvider to refresh tokens throughout the life of the spark app.

```
spark.kubernetes.authenticate.submission.oauthTokenProvider=<token>
spark.kubernetes.authenticate.driver.oauthTokenProvider=<token>
spark.kubernetes.authenticate.oauthTokenProvider=<token>
```

https://javadoc.io/doc/io.fabric8/kubernetes-client/5.6.0/io/fabric8/kubernetes/client/OAuthTokenProvider.html


### Why are the changes needed?

Currently, while running spark on kubernetes, one should specify oauth tokens via config before starting an application.
```
spark.kubernetes.authenticate.submission.oauthToken=<token>
spark.kubernetes.authenticate.driver.oauthToken=<token>
spark.kubernetes.authenticate.oauthToken=<token>
```

The token has an expiration time (usually an hour, for GKE) and there is no way to update the token in the runtime. The spark app starts to throw exceptions.
```
io.fabric8.kubernetes.client.KubernetesClientException: Unauthorized
	at io.fabric8.kubernetes.client.dsl.internal.WatchConnectionManager$1.onFailure(WatchConnectionManager.java:202)
	at okhttp3.internal.ws.RealWebSocket.failWebSocket(RealWebSocket.java:571)
	at okhttp3.internal.ws.RealWebSocket$2.onResponse(RealWebSocket.java:198)
	at okhttp3.RealCall$AsyncCall.execute(RealCall.java:203)
	at okhttp3.internal.NamedRunnable.run(NamedRunnable.java:32)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
```


### Does this PR introduce _any_ user-facing change?
Yes, a configuration option `spark.kubernetes.client.oauth.token.provider.class` is added. 


### How was this patch tested?
A class which implements OAuthTokenProvider interface[0] was added into the classpath on driver node with no other spark options for tokens specified 
It was also tested with expired tokens specified, and the token was updated via the user-provided class.
```
--conf spark.kubernetes.authenticate.submission.oauthToken=<expired>
--conf spark.kubernetes.authenticate.driver.oauthToken=<expired> 
--conf spark.kubernetes.authenticate.oauthToken=<expired>
```
There is no need to use any other token-related configuration options if this class is provided.

An example of the user-provided class for GKE
[0] https://gist.github.com/haodemon/5490fefdb258275c1f805d584319090b

```scala
import io.fabric8.kubernetes.client.OAuthTokenProvider

class OAuthGoogleTokenProvider extends OAuthTokenProvider {
  private val binary = ""gcloud""
  private val args = ""config config-helper --format=json""

  override def getToken: String = {
    val response = (binary + "" "" + args).!!
    val token = new ObjectMapper().readTree(response)
      .get(""credential"")
      .get(""access_token"")
    token.getTextValue
  }
}
```
",https://api.github.com/repos/apache/spark/issues/33675/timeline,,spark,apache,haodemon,3372489,MDQ6VXNlcjMzNzI0ODk=,https://avatars.githubusercontent.com/u/3372489?v=4,,https://api.github.com/users/haodemon,https://github.com/haodemon,https://api.github.com/users/haodemon/followers,https://api.github.com/users/haodemon/following{/other_user},https://api.github.com/users/haodemon/gists{/gist_id},https://api.github.com/users/haodemon/starred{/owner}{/repo},https://api.github.com/users/haodemon/subscriptions,https://api.github.com/users/haodemon/orgs,https://api.github.com/users/haodemon/repos,https://api.github.com/users/haodemon/events{/privacy},https://api.github.com/users/haodemon/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/33675,https://github.com/apache/spark/pull/33675,https://github.com/apache/spark/pull/33675.diff,https://github.com/apache/spark/pull/33675.patch,,https://api.github.com/repos/apache/spark/issues/33675/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
215,https://api.github.com/repos/apache/spark/issues/33674,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/33674/labels{/name},https://api.github.com/repos/apache/spark/issues/33674/comments,https://api.github.com/repos/apache/spark/issues/33674/events,https://github.com/apache/spark/pull/33674,963224719,MDExOlB1bGxSZXF1ZXN0NzA1ODg3MzMz,33674,[Spark-36328][CORE][SQL] Reuse the FileSystem delegation token while querying partitioned hive table.,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,12,2021-08-07T12:51:00Z,2021-12-09T10:39:47Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?

Add the credentials from previous JobConf into the new JobConf to reuse the FileSystem Delegation Token.

### Why are the changes needed?

Spark Job creates a new JobConf (which will have a new Credentials) for every hive table partition, the token is not reused and gets fetched for every partition. This is slowing down the query as each delegation token has to go through KDC and SSL handshake on Secure Clusters.

### Does this PR introduce _any_ user-facing change?

Yes, while user querying partitioned hive table.

### How was this patch tested?

new test added.
",https://api.github.com/repos/apache/spark/issues/33674/timeline,,spark,apache,Shockang,28219857,MDQ6VXNlcjI4MjE5ODU3,https://avatars.githubusercontent.com/u/28219857?v=4,,https://api.github.com/users/Shockang,https://github.com/Shockang,https://api.github.com/users/Shockang/followers,https://api.github.com/users/Shockang/following{/other_user},https://api.github.com/users/Shockang/gists{/gist_id},https://api.github.com/users/Shockang/starred{/owner}{/repo},https://api.github.com/users/Shockang/subscriptions,https://api.github.com/users/Shockang/orgs,https://api.github.com/users/Shockang/repos,https://api.github.com/users/Shockang/events{/privacy},https://api.github.com/users/Shockang/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/33674,https://github.com/apache/spark/pull/33674,https://github.com/apache/spark/pull/33674.diff,https://github.com/apache/spark/pull/33674.patch,,https://api.github.com/repos/apache/spark/issues/33674/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
216,https://api.github.com/repos/apache/spark/issues/33625,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/33625/labels{/name},https://api.github.com/repos/apache/spark/issues/33625/comments,https://api.github.com/repos/apache/spark/issues/33625/events,https://github.com/apache/spark/pull/33625,959587708,MDExOlB1bGxSZXF1ZXN0NzAyNjg4MDkz,33625,[WIP][SPARK-36397][PYTHON] Implement DataFrame.mode,"[{'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1982260031, 'node_id': 'MDU6TGFiZWwxOTgyMjYwMDMx', 'url': 'https://api.github.com/repos/apache/spark/labels/PYTHON', 'name': 'PYTHON', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,29,2021-08-03T22:45:25Z,2021-12-13T17:30:52Z,,CONTRIBUTOR,,True,"### What changes were proposed in this pull request?
Implement DataFrame.mode (along index axis).


### Why are the changes needed?
Get the mode(s) of each element along the selected axis is a common functionality, which is supported in pandas. We should support that.

### Does this PR introduce _any_ user-facing change?
Yes. `DataFrame.mode` can be used now.

```py
>>> psdf = ps.DataFrame(
...     [(""bird"", 2, 2), (""mammal"", 4, np.nan), (""arthropod"", 8, 0), (""bird"", 2, np.nan)],
...     index=(""falcon"", ""horse"", ""spider"", ""ostrich""),
...     columns=(""species"", ""legs"", ""wings""),
... )
>>> psdf
           species  legs  wings                                                 
falcon        bird     2    2.0
horse       mammal     4    NaN
spider   arthropod     8    0.0
ostrich       bird     2    NaN

>>> psdf.mode()
  species  legs  wings
0    bird   2.0    0.0
1    None   NaN    2.0

>>> psdf.mode(dropna=False)
  species  legs  wings
0    bird     2    NaN

>>> psdf.mode(numeric_only=True)
   legs  wings
0   2.0    0.0
1   NaN    2.0
```

### How was this patch tested?
Unit tests.
",https://api.github.com/repos/apache/spark/issues/33625/timeline,,spark,apache,xinrong-databricks,47337188,MDQ6VXNlcjQ3MzM3MTg4,https://avatars.githubusercontent.com/u/47337188?v=4,,https://api.github.com/users/xinrong-databricks,https://github.com/xinrong-databricks,https://api.github.com/users/xinrong-databricks/followers,https://api.github.com/users/xinrong-databricks/following{/other_user},https://api.github.com/users/xinrong-databricks/gists{/gist_id},https://api.github.com/users/xinrong-databricks/starred{/owner}{/repo},https://api.github.com/users/xinrong-databricks/subscriptions,https://api.github.com/users/xinrong-databricks/orgs,https://api.github.com/users/xinrong-databricks/repos,https://api.github.com/users/xinrong-databricks/events{/privacy},https://api.github.com/users/xinrong-databricks/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/33625,https://github.com/apache/spark/pull/33625,https://github.com/apache/spark/pull/33625.diff,https://github.com/apache/spark/pull/33625.patch,,https://api.github.com/repos/apache/spark/issues/33625/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
217,https://api.github.com/repos/apache/spark/issues/33572,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/33572/labels{/name},https://api.github.com/repos/apache/spark/issues/33572/comments,https://api.github.com/repos/apache/spark/issues/33572/events,https://github.com/apache/spark/pull/33572,955563966,MDExOlB1bGxSZXF1ZXN0Njk5Mjk2MTk3,33572,[SPARK-36180][SQL] Store TIMESTAMP_NTZ into hive catalog as TIMESTAMP,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,9,2021-07-29T07:52:57Z,2021-11-18T03:15:00Z,,CONTRIBUTOR,,False,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->

This PR fix a issue that HMS can not recognize timestamp_ntz by mapping timestamp_ntz to `timestamp` of hive

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->

The hive 2.3.9 does not have 2 timestamp or a type named timestamp_ntz.
FYI, In hive 3.0, the will be a timestamp with local timezone added.

### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->

no, timestamp_ntz is new and not public yet
### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->

new test",https://api.github.com/repos/apache/spark/issues/33572/timeline,,spark,apache,yaooqinn,8326978,MDQ6VXNlcjgzMjY5Nzg=,https://avatars.githubusercontent.com/u/8326978?v=4,,https://api.github.com/users/yaooqinn,https://github.com/yaooqinn,https://api.github.com/users/yaooqinn/followers,https://api.github.com/users/yaooqinn/following{/other_user},https://api.github.com/users/yaooqinn/gists{/gist_id},https://api.github.com/users/yaooqinn/starred{/owner}{/repo},https://api.github.com/users/yaooqinn/subscriptions,https://api.github.com/users/yaooqinn/orgs,https://api.github.com/users/yaooqinn/repos,https://api.github.com/users/yaooqinn/events{/privacy},https://api.github.com/users/yaooqinn/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/33572,https://github.com/apache/spark/pull/33572,https://github.com/apache/spark/pull/33572.diff,https://github.com/apache/spark/pull/33572.patch,,https://api.github.com/repos/apache/spark/issues/33572/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
218,https://api.github.com/repos/apache/spark/issues/33559,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/33559/labels{/name},https://api.github.com/repos/apache/spark/issues/33559/comments,https://api.github.com/repos/apache/spark/issues/33559/events,https://github.com/apache/spark/pull/33559,954819646,MDExOlB1bGxSZXF1ZXN0Njk4NjU2MjM2,33559,[SPARK-34265][WIP][PYTHON] Instrument Python UDFs using SQL metrics ,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1406627200, 'node_id': 'MDU6TGFiZWwxNDA2NjI3MjAw', 'url': 'https://api.github.com/repos/apache/spark/labels/BUILD', 'name': 'BUILD', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981471081, 'node_id': 'MDU6TGFiZWwxOTgxNDcxMDgx', 'url': 'https://api.github.com/repos/apache/spark/labels/DOCS', 'name': 'DOCS', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1982260031, 'node_id': 'MDU6TGFiZWwxOTgyMjYwMDMx', 'url': 'https://api.github.com/repos/apache/spark/labels/PYTHON', 'name': 'PYTHON', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,1,2021-07-28T12:47:57Z,2021-10-19T09:42:43Z,,CONTRIBUTOR,,False,"### What changes are proposed in this pull request?

This proposes to add SQLMetrics instrumentation for Python UDF execution.
The proposed metrics are:

- data sent to Python workers
- data returned from Python workers
- number of rows processed


### Why are the changes needed?
This aims at improving monitoring and performance troubleshooting of Python UDFs.
In particular as an aid to answer performance-related questions such as:
why is the UDF slow?, how much work it has done so far?, etc.

### Does this PR introduce _any_ user-facing change?
SQL metrics are made available in the WEB UI.  
See the following examples:  

![image1](https://user-images.githubusercontent.com/5243162/127323340-f0132da1-e19c-4d81-b5dc-a534ea9346ee.png)
  
![image2](https://issues.apache.org/jira/secure/attachment/13031153/Python_UDF_instrumentation_lite_BatchEvalPython.png)

### How was this patch tested?

Manually tested + a Python unit test has been added.

Example code used for testing:

```
from pyspark.sql.functions import col, pandas_udf
import time

@pandas_udf(""long"")
def test_pandas(col1):
  time.sleep(0.02)
  return col1 * col1

spark.udf.register(""test_pandas"", test_pandas)
spark.sql(""select rand(42)*rand(51)*rand(12) col1 from range(10000000)"").createOrReplaceTempView(""t1"")
spark.sql(""select max(test_pandas(col1)) from t1"").collect()
```

This is used to test with more data pushed to the Python workers

```
from pyspark.sql.functions import col, pandas_udf
import time

@pandas_udf(""long"")
def test_pandas(col1,col2,col3,col4,col5,col6,col7,col8,col9,col10,col11,col12,col13,col14,col15,col16,col17):
  time.sleep(0.02)
  return col1

spark.udf.register(""test_pandas"", test_pandas)
spark.sql(""select rand(42)*rand(51)*rand(12) col1 from range(10000000)"").createOrReplaceTempView(""t1"")
spark.sql(""select max(test_pandas(col1,col1+1,col1+2,col1+3,col1+4,col1+5,col1+6,col1+7,col1+8,col1+9,col1+10,col1+11,col1+12,col1+13,col1+14,col1+15,col1+16)) from t1"").collect()
```

This is for testing Python UDF (non pandas)

`from pyspark.sql.functions import udf; spark.range(100).select(udf(lambda x: x/1)(""id"")).collect()`
  ",https://api.github.com/repos/apache/spark/issues/33559/timeline,,spark,apache,LucaCanali,5243162,MDQ6VXNlcjUyNDMxNjI=,https://avatars.githubusercontent.com/u/5243162?v=4,,https://api.github.com/users/LucaCanali,https://github.com/LucaCanali,https://api.github.com/users/LucaCanali/followers,https://api.github.com/users/LucaCanali/following{/other_user},https://api.github.com/users/LucaCanali/gists{/gist_id},https://api.github.com/users/LucaCanali/starred{/owner}{/repo},https://api.github.com/users/LucaCanali/subscriptions,https://api.github.com/users/LucaCanali/orgs,https://api.github.com/users/LucaCanali/repos,https://api.github.com/users/LucaCanali/events{/privacy},https://api.github.com/users/LucaCanali/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/33559,https://github.com/apache/spark/pull/33559,https://github.com/apache/spark/pull/33559.diff,https://github.com/apache/spark/pull/33559.patch,,https://api.github.com/repos/apache/spark/issues/33559/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
219,https://api.github.com/repos/apache/spark/issues/33550,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/33550/labels{/name},https://api.github.com/repos/apache/spark/issues/33550/comments,https://api.github.com/repos/apache/spark/issues/33550/events,https://github.com/apache/spark/pull/33550,954435490,MDExOlB1bGxSZXF1ZXN0Njk4MzMxNTI3,33550,[SPARK-36321][K8S] Do not fail application in kubernetes if name is too long,"[{'id': 1406605057, 'node_id': 'MDU6TGFiZWwxNDA2NjA1MDU3', 'url': 'https://api.github.com/repos/apache/spark/labels/KUBERNETES', 'name': 'KUBERNETES', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1496047285, 'node_id': 'MDU6TGFiZWwxNDk2MDQ3Mjg1', 'url': 'https://api.github.com/repos/apache/spark/labels/EXAMPLES', 'name': 'EXAMPLES', 'color': 'ededed', 'default': False, 'description': ''}]",open,False,,[],,39,2021-07-28T03:23:33Z,2021-10-11T15:43:24Z,,CONTRIBUTOR,,True,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->
Use short string as executor pod name prefix if app name is long.

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->
If we have a long spark app name and start with k8s master, we will get the execption.
```
java.lang.IllegalArgumentException: 'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa-89fe2f7ae71c3570' in spark.kubernetes.executor.podNamePrefix is invalid. must conform https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#dns-label-names and the value length <= 47
	at org.apache.spark.internal.config.TypedConfigBuilder.$anonfun$checkValue$1(ConfigBuilder.scala:108)
	at org.apache.spark.internal.config.TypedConfigBuilder.$anonfun$transform$1(ConfigBuilder.scala:101)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.internal.config.OptionalConfigEntry.readFrom(ConfigEntry.scala:239)
	at org.apache.spark.internal.config.OptionalConfigEntry.readFrom(ConfigEntry.scala:214)
	at org.apache.spark.SparkConf.get(SparkConf.scala:261)
	at org.apache.spark.deploy.k8s.KubernetesConf.get(KubernetesConf.scala:67)
	at org.apache.spark.deploy.k8s.KubernetesExecutorConf.<init>(KubernetesConf.scala:147)
	at org.apache.spark.deploy.k8s.KubernetesConf$.createExecutorConf(KubernetesConf.scala:231)
	at org.apache.spark.scheduler.cluster.k8s.ExecutorPodsAllocator.$anonfun$requestNewExecutors$2(ExecutorPodsAllocator.scala:367)
```
Use app name as the executor pod name is the Spark internal behavior and we should not make application failure.

### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->
yes

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->
Add test

the new log:
```
21/07/28 09:35:53 INFO SparkEnv: Registering OutputCommitCoordinator
21/07/28 09:35:54 INFO Utils: Successfully started service 'SparkUI' on port 41926.
21/07/28 09:35:54 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://:41926
21/07/28 09:35:54 WARN KubernetesClusterManager: Use spark-c460617aeac0fda9 as the executor pod's name prefix due to spark.app.name is too long. Please set 'spark.kubernetes.executor.podNamePrefix' if you need a custom executor pod's name prefix.
21/07/28 09:35:54 INFO SparkKubernetesClientFactory: Auto-configuring K8S client using current context from users K8S config file
21/07/28 09:35:55 WARN Utils: spark.executor.instances less than spark.dynamicAllocation.minExecutors is invalid, ignoring its setting, please update your configs.
```

verify the config:
![image](https://user-images.githubusercontent.com/12025282/127258223-fbcaaac8-451d-4c55-8c09-e802511a510d.png)

verify the executor pod name
![image](https://user-images.githubusercontent.com/12025282/127258284-be15b862-b826-4440-9a11-023d69c61fc4.png)
",https://api.github.com/repos/apache/spark/issues/33550/timeline,,spark,apache,ulysses-you,12025282,MDQ6VXNlcjEyMDI1Mjgy,https://avatars.githubusercontent.com/u/12025282?v=4,,https://api.github.com/users/ulysses-you,https://github.com/ulysses-you,https://api.github.com/users/ulysses-you/followers,https://api.github.com/users/ulysses-you/following{/other_user},https://api.github.com/users/ulysses-you/gists{/gist_id},https://api.github.com/users/ulysses-you/starred{/owner}{/repo},https://api.github.com/users/ulysses-you/subscriptions,https://api.github.com/users/ulysses-you/orgs,https://api.github.com/users/ulysses-you/repos,https://api.github.com/users/ulysses-you/events{/privacy},https://api.github.com/users/ulysses-you/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/33550,https://github.com/apache/spark/pull/33550,https://github.com/apache/spark/pull/33550.diff,https://github.com/apache/spark/pull/33550.patch,,https://api.github.com/repos/apache/spark/issues/33550/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
220,https://api.github.com/repos/apache/spark/issues/33522,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/33522/labels{/name},https://api.github.com/repos/apache/spark/issues/33522/comments,https://api.github.com/repos/apache/spark/issues/33522/events,https://github.com/apache/spark/pull/33522,953034263,MDExOlB1bGxSZXF1ZXN0Njk3MTM5NDU4,33522,[SPARK-36290][SQL] Pull out join condition,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,51,2021-07-26T15:37:56Z,2021-11-06T16:48:18Z,,MEMBER,,False,"### What changes were proposed in this pull request?

Similar to [`PullOutGroupingExpressions`](https://github.com/wangyum/spark/blob/7fd3f8f9ec55b364525407213ba1c631705686c5/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/PullOutGroupingExpressions.scala#L48). This pr add a new rule(`PullOutJoinCondition`) to pull out join condition. Otherwise the expression in join condition may be evaluated three times(`ShuffleExchangeExec`, `SortExec` and the join itself). For example:
```sql
CREATE TABLE t1 using parquet AS select id as a, id as b from range(100000000L);
CREATE TABLE t2 using parquet AS select id as a, id as b from range(200000000L);
SELECT t1.* FROM t1 JOIN t2 ON translate(t1.a, '123', 'abc') = translate(t2.a, '123', 'abc');
```
Before this pr:
```
== Physical Plan ==
AdaptiveSparkPlan isFinalPlan=false
+- Project [a#6L, b#7L]
   +- SortMergeJoin [translate(cast(a#6L as string), 123, abc)], [translate(cast(a#8L as string), 123, abc)], Inner
      :- Sort [translate(cast(a#6L as string), 123, abc) ASC NULLS FIRST], false, 0
      :  +- Exchange hashpartitioning(translate(cast(a#6L as string), 123, abc), 5), ENSURE_REQUIREMENTS, [id=#89]
      :     +- Filter isnotnull(a#6L)
      :        +- FileScan parquet default.t1[a#6L,b#7L]
      +- Sort [translate(cast(a#8L as string), 123, abc) ASC NULLS FIRST], false, 0
         +- Exchange hashpartitioning(translate(cast(a#8L as string), 123, abc), 5), ENSURE_REQUIREMENTS, [id=#90]
            +- Filter isnotnull(a#8L)
               +- FileScan parquet default.t2[a#8L]
```
After this pr:
```
== Physical Plan ==
AdaptiveSparkPlan isFinalPlan=false
+- Project [a#6L, b#7L]
   +- SortMergeJoin [translate(CAST(spark_catalog.default.t1.a AS STRING), '123', 'abc')#12], [translate(CAST(spark_catalog.default.t2.a AS STRING), '123', 'abc')#13], Inner
      :- Sort [translate(CAST(spark_catalog.default.t1.a AS STRING), '123', 'abc')#12 ASC NULLS FIRST], false, 0
      :  +- Exchange hashpartitioning(translate(CAST(spark_catalog.default.t1.a AS STRING), '123', 'abc')#12, 5), ENSURE_REQUIREMENTS, [id=#53]
      :     +- Project [a#6L, b#7L, translate(cast(a#6L as string), 123, abc) AS translate(CAST(spark_catalog.default.t1.a AS STRING), '123', 'abc')#12]
      :        +- Filter isnotnull(translate(cast(a#6L as string), 123, abc))
      :           +- FileScan parquet default.t1[a#6L,b#7L]
      +- Sort [translate(CAST(spark_catalog.default.t2.a AS STRING), '123', 'abc')#13 ASC NULLS FIRST], false, 0
         +- Exchange hashpartitioning(translate(CAST(spark_catalog.default.t2.a AS STRING), '123', 'abc')#13, 5), ENSURE_REQUIREMENTS, [id=#54]
            +- Project [translate(cast(a#8L as string), 123, abc) AS translate(CAST(spark_catalog.default.t2.a AS STRING), '123', 'abc')#13]
               +- Filter isnotnull(translate(cast(a#8L as string), 123, abc))
                  +- FileScan parquet default.t2[a#8L]
```

### Why are the changes needed?

Improve query performance.

### Does this PR introduce _any_ user-facing change?

No.


### How was this patch tested?

Unit test and benchmark test:
```scala
import org.apache.spark.benchmark.Benchmark
val numRows = 1024 * 1024 * 15
spark.sql(s""CREATE TABLE t1 using parquet AS select id as a, id as b from range(${numRows}L)"")
spark.sql(s""CREATE TABLE t2 using parquet AS select id as a, id as b from range(${numRows}L)"")
val benchmark = new Benchmark(""Benchmark pull out join condition"", numRows, minNumIters = 5)

Seq(false, true).foreach { pullOutEnabled =>
  val name = s""Pull out join condition ${if (pullOutEnabled) ""(Enabled)"" else ""(Disabled)""}""
  benchmark.addCase(name) { _ =>
    withSQLConf(""spark.sql.pullOutJoinCondition"" -> s""$pullOutEnabled"") {
      spark.sql(""SELECT t1.* FROM t1 JOIN t2 ON translate(t1.a, '123', 'abc') = translate(t2.a, '123', 'abc')"").write.format(""noop"").mode(""Overwrite"").save()
    }
  }
}
benchmark.run()
```
Benchmark result:
```
Java HotSpot(TM) 64-Bit Server VM 1.8.0_251-b08 on Mac OS X 10.15.7
Intel(R) Core(TM) i9-9980HK CPU @ 2.40GHz
Benchmark pull out join condition:        Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)   Relative
------------------------------------------------------------------------------------------------------------------------
Pull out join condition (Disabled)                30197          34046         690          0.5        1919.9       1.0X
Pull out join condition (Enabled)                 19631          20484         535          0.8        1248.1       1.5X
```
",https://api.github.com/repos/apache/spark/issues/33522/timeline,,spark,apache,wangyum,5399861,MDQ6VXNlcjUzOTk4NjE=,https://avatars.githubusercontent.com/u/5399861?v=4,,https://api.github.com/users/wangyum,https://github.com/wangyum,https://api.github.com/users/wangyum/followers,https://api.github.com/users/wangyum/following{/other_user},https://api.github.com/users/wangyum/gists{/gist_id},https://api.github.com/users/wangyum/starred{/owner}{/repo},https://api.github.com/users/wangyum/subscriptions,https://api.github.com/users/wangyum/orgs,https://api.github.com/users/wangyum/repos,https://api.github.com/users/wangyum/events{/privacy},https://api.github.com/users/wangyum/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/33522,https://github.com/apache/spark/pull/33522,https://github.com/apache/spark/pull/33522.diff,https://github.com/apache/spark/pull/33522.patch,,https://api.github.com/repos/apache/spark/issues/33522/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
221,https://api.github.com/repos/apache/spark/issues/33446,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/33446/labels{/name},https://api.github.com/repos/apache/spark/issues/33446/comments,https://api.github.com/repos/apache/spark/issues/33446/events,https://github.com/apache/spark/pull/33446,948883378,MDExOlB1bGxSZXF1ZXN0NjkzNjU5MzA1,33446,[SPARK-36215][SHUFFLE] Add logging for slow fetches to diagnose external shuffle service issues,"[{'id': 1981471081, 'node_id': 'MDU6TGFiZWwxOTgxNDcxMDgx', 'url': 'https://api.github.com/repos/apache/spark/labels/DOCS', 'name': 'DOCS', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,14,2021-07-20T17:33:49Z,2021-11-13T18:19:27Z,,CONTRIBUTOR,,False,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
Add logging to `ShuffleBlockFetcherIterator` to log ""slow"" fetches, where slow is defined by two confs: `spark.reducer.shuffleFetchSlowLogThreshold.time` and `spark.reducer.shuffleFetchSlowLogThreshold.bytesPerSec`
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->


### Why are the changes needed?
Currently we can see from the metrics that a task or stage has slow fetches, and the logs indicate *all* of the shuffle servers those tasks were fetching from, but often this is a big set (dozens or even hundreds) and narrowing down which one caused issues can be very difficult. This change makes it easier to understand which fetch is ""slow"".
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->


### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->
Adds two configs `spark.reducer.shuffleFetchSlowLogThreshold.time` and `spark.reducer.shuffleFetchSlowLogThreshold.bytesPerSec`


### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->
Added unit test",https://api.github.com/repos/apache/spark/issues/33446/timeline,,spark,apache,shardulm94,6961317,MDQ6VXNlcjY5NjEzMTc=,https://avatars.githubusercontent.com/u/6961317?v=4,,https://api.github.com/users/shardulm94,https://github.com/shardulm94,https://api.github.com/users/shardulm94/followers,https://api.github.com/users/shardulm94/following{/other_user},https://api.github.com/users/shardulm94/gists{/gist_id},https://api.github.com/users/shardulm94/starred{/owner}{/repo},https://api.github.com/users/shardulm94/subscriptions,https://api.github.com/users/shardulm94/orgs,https://api.github.com/users/shardulm94/repos,https://api.github.com/users/shardulm94/events{/privacy},https://api.github.com/users/shardulm94/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/33446,https://github.com/apache/spark/pull/33446,https://github.com/apache/spark/pull/33446.diff,https://github.com/apache/spark/pull/33446.patch,,https://api.github.com/repos/apache/spark/issues/33446/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
222,https://api.github.com/repos/apache/spark/issues/33404,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/33404/labels{/name},https://api.github.com/repos/apache/spark/issues/33404/comments,https://api.github.com/repos/apache/spark/issues/33404/events,https://github.com/apache/spark/pull/33404,946850336,MDExOlB1bGxSZXF1ZXN0NjkxOTUyMTk4,33404,[SPARK-36194][SQL] Remove the aggregation from left semi/anti join if the same aggregation has already been done on left side,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,29,2021-07-17T15:59:39Z,2021-11-02T03:40:10Z,,MEMBER,,False,"### What changes were proposed in this pull request?

Remove the aggregation from left semi/anti join if the same aggregation has already been done on left side. For example:
```sql
set spark.sql.autoBroadcastJoinThreshold=-1; -- avoid PushDownLeftSemiAntiJoin
create table t1 using parquet as select id a, id as b from range(10);
create table t2 using parquet as select id as a, id as b from range(8);
select t11.a, t11.b from (select distinct a, b from t1) t11 left semi join t2 on (t11.a = t2.a) group by t11.a, t11.b;
```

Before this PR:
```
== Optimized Logical Plan ==
Aggregate [a#6L, b#7L], [a#6L, b#7L], Statistics(sizeInBytes=1492.0 B)
+- Join LeftSemi, (a#6L = a#8L), Statistics(sizeInBytes=1492.0 B)
   :- Aggregate [a#6L, b#7L], [a#6L, b#7L], Statistics(sizeInBytes=1492.0 B)
   :  +- Filter isnotnull(a#6L), Statistics(sizeInBytes=1492.0 B)
   :     +- Relation default.t1[a#6L,b#7L] parquet, Statistics(sizeInBytes=1492.0 B)
   +- Project [a#8L], Statistics(sizeInBytes=984.0 B)
      +- Filter isnotnull(a#8L), Statistics(sizeInBytes=1476.0 B)
         +- Relation default.t2[a#8L,b#9L] parquet, Statistics(sizeInBytes=1476.0 B)
```

After this PR:
```
== Optimized Logical Plan ==
Join LeftSemi, (a#6L = a#8L), Statistics(sizeInBytes=1492.0 B)
:- Aggregate [a#6L, b#7L], [a#6L, b#7L], Statistics(sizeInBytes=1492.0 B)
:  +- Filter isnotnull(a#6L), Statistics(sizeInBytes=1492.0 B)
:     +- Relation default.t1[a#6L,b#7L] parquet, Statistics(sizeInBytes=1492.0 B)
+- Project [a#8L], Statistics(sizeInBytes=984.0 B)
   +- Filter isnotnull(a#8L), Statistics(sizeInBytes=1476.0 B)
      +- Relation default.t2[a#8L,b#9L] parquet, Statistics(sizeInBytes=1476.0 B)
```

This rule can be disabled by:
```sql
set spark.sql.optimizer.excludedRules=org.apache.spark.sql.catalyst.optimizer.RemoveRedundantAggregatesInLeftSemiAntiJoin;
```

### Why are the changes needed?

Improve query performance.

### Does this PR introduce _any_ user-facing change?

No.

### How was this patch tested?

Unit test and TPC-DS benchmark test.

SQL | Before this PR(Seconds) | After this PR(Seconds)
-- | -- | --
q14a | 174  | 165
q38 | 26 | 23
q87 | 30 | 26
",https://api.github.com/repos/apache/spark/issues/33404/timeline,,spark,apache,wangyum,5399861,MDQ6VXNlcjUzOTk4NjE=,https://avatars.githubusercontent.com/u/5399861?v=4,,https://api.github.com/users/wangyum,https://github.com/wangyum,https://api.github.com/users/wangyum/followers,https://api.github.com/users/wangyum/following{/other_user},https://api.github.com/users/wangyum/gists{/gist_id},https://api.github.com/users/wangyum/starred{/owner}{/repo},https://api.github.com/users/wangyum/subscriptions,https://api.github.com/users/wangyum/orgs,https://api.github.com/users/wangyum/repos,https://api.github.com/users/wangyum/events{/privacy},https://api.github.com/users/wangyum/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/33404,https://github.com/apache/spark/pull/33404,https://github.com/apache/spark/pull/33404.diff,https://github.com/apache/spark/pull/33404.patch,,https://api.github.com/repos/apache/spark/issues/33404/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
223,https://api.github.com/repos/apache/spark/issues/33314,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/33314/labels{/name},https://api.github.com/repos/apache/spark/issues/33314/comments,https://api.github.com/repos/apache/spark/issues/33314/events,https://github.com/apache/spark/pull/33314,942685825,MDExOlB1bGxSZXF1ZXN0Njg4Mzg1NTc1,33314,[SPARK-36118][SQL] Add bitmap functions for Spark SQL,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1982110661, 'node_id': 'MDU6TGFiZWwxOTgyMTEwNjYx', 'url': 'https://api.github.com/repos/apache/spark/labels/INFRA', 'name': 'INFRA', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,12,2021-07-13T03:56:43Z,2021-10-27T03:18:20Z,,NONE,,False,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->
add functions of bitmap building and computing cardinality for Spark SQL, If this is ok, I will update function.scala and FunctionRegistry.scala.


### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->
Bitmaps are used more and more widely, and many frameworks have native support, such as Clickhouse

### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->
No

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->
CI, it performs well on billions of rows based on our real demand",https://api.github.com/repos/apache/spark/issues/33314/timeline,,spark,apache,ReachInfi,87301083,MDQ6VXNlcjg3MzAxMDgz,https://avatars.githubusercontent.com/u/87301083?v=4,,https://api.github.com/users/ReachInfi,https://github.com/ReachInfi,https://api.github.com/users/ReachInfi/followers,https://api.github.com/users/ReachInfi/following{/other_user},https://api.github.com/users/ReachInfi/gists{/gist_id},https://api.github.com/users/ReachInfi/starred{/owner}{/repo},https://api.github.com/users/ReachInfi/subscriptions,https://api.github.com/users/ReachInfi/orgs,https://api.github.com/users/ReachInfi/repos,https://api.github.com/users/ReachInfi/events{/privacy},https://api.github.com/users/ReachInfi/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/33314,https://github.com/apache/spark/pull/33314,https://github.com/apache/spark/pull/33314.diff,https://github.com/apache/spark/pull/33314.patch,,https://api.github.com/repos/apache/spark/issues/33314/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
224,https://api.github.com/repos/apache/spark/issues/33257,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/33257/labels{/name},https://api.github.com/repos/apache/spark/issues/33257/comments,https://api.github.com/repos/apache/spark/issues/33257/events,https://github.com/apache/spark/pull/33257,939520094,MDExOlB1bGxSZXF1ZXN0Njg1NzI3NzQ3,33257,[SPARK-36039][K8S] Fix executor pod hadoop conf mount,"[{'id': 1406605057, 'node_id': 'MDU6TGFiZWwxNDA2NjA1MDU3', 'url': 'https://api.github.com/repos/apache/spark/labels/KUBERNETES', 'name': 'KUBERNETES', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,23,2021-07-08T06:33:34Z,2021-12-24T07:43:16Z,,NONE,,False,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error message, please read the guideline first:
     https://spark.apache.org/error-message-guidelines.html
-->

### What changes were proposed in this pull request?
Fix executor pod hadoop conf mount.
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->


### Why are the changes needed?
Arg --conf spark.kubernetes.hadoop.configMapName for executor pod not working.
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->


### Does this PR introduce _any_ user-facing change?
No.
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->


### How was this patch tested?
UT.
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->",https://api.github.com/repos/apache/spark/issues/33257/timeline,,spark,apache,cutiechi,27606234,MDQ6VXNlcjI3NjA2MjM0,https://avatars.githubusercontent.com/u/27606234?v=4,,https://api.github.com/users/cutiechi,https://github.com/cutiechi,https://api.github.com/users/cutiechi/followers,https://api.github.com/users/cutiechi/following{/other_user},https://api.github.com/users/cutiechi/gists{/gist_id},https://api.github.com/users/cutiechi/starred{/owner}{/repo},https://api.github.com/users/cutiechi/subscriptions,https://api.github.com/users/cutiechi/orgs,https://api.github.com/users/cutiechi/repos,https://api.github.com/users/cutiechi/events{/privacy},https://api.github.com/users/cutiechi/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/33257,https://github.com/apache/spark/pull/33257,https://github.com/apache/spark/pull/33257.diff,https://github.com/apache/spark/pull/33257.patch,,https://api.github.com/repos/apache/spark/issues/33257/reactions,1,1,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
225,https://api.github.com/repos/apache/spark/issues/33174,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/33174/labels{/name},https://api.github.com/repos/apache/spark/issues/33174/comments,https://api.github.com/repos/apache/spark/issues/33174/events,https://github.com/apache/spark/pull/33174,934604265,MDExOlB1bGxSZXF1ZXN0NjgxNjQ4Njcw,33174,[SPARK-35721][PYTHON] Path level discover for python unittests,"[{'id': 1406627200, 'node_id': 'MDU6TGFiZWwxNDA2NjI3MjAw', 'url': 'https://api.github.com/repos/apache/spark/labels/BUILD', 'name': 'BUILD', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1982110661, 'node_id': 'MDU6TGFiZWwxOTgyMTEwNjYx', 'url': 'https://api.github.com/repos/apache/spark/labels/INFRA', 'name': 'INFRA', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1982260031, 'node_id': 'MDU6TGFiZWwxOTgyMjYwMDMx', 'url': 'https://api.github.com/repos/apache/spark/labels/PYTHON', 'name': 'PYTHON', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,20,2021-07-01T09:06:21Z,2021-09-29T18:50:08Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?
Add path level discover for python unittests.
![image](https://user-images.githubusercontent.com/1736354/124094503-6bdeb980-da8b-11eb-9bbe-b086024f6902.png)

Change list:
- Introduce a **python_discover_paths** in modules.
- Add **_discover_python_unittests** function: it would be called in pthon/run-tests.py to load test module.
- Add **_append_discovred_goals function**: call _discover_python_unittests to refresh m.python_test_goals
- if modules have python_test_goals or **python_discover_paths** would also be considered as python tests.
- Fix: Move logging.basicConfig to head to make sure logging config before any possible logging print.
- Fix: Change python/pyspark/testing/utils.py SPARK_HOME use _find_spark_home to get value.
- Fix: export py4j PYTHONPATH before run test.

Note:
- **Why use walk_packages but not unittest.defaultTestLoader.discover?** we use `pkgutil.walk_packages` and `unittest.defaultTestLoader.loadTestsFromModule` to load test modules, consider we will add doctest discover in future, we can add something like blow as the impletations of doctest discover: 
```python
import doctest

def _contain_doctests_class(module):
    suite = doctest.DocTestSuite(module)
    if suite.countTestCases():
        return True
    else:
        return False
```
- **Why we doesn't add doctests in here**? Currently, not all modules doctests are added to `python_test_goals`, that means these doctests doesn't be excuted, so better add discover doctests in a separate PR.

- **What's the deps of discover?** the test discover will do real import for every modules, so we need install **all deps of PySpark test modules** before run-tests otherwise the ImportError would be raised.



### Why are the changes needed?
Now we need to specify the python test cases by manually when we add a new testcase. Sometime, we forgot to add the testcase to module list, the testcase would not be executed.

Such as:

pyspark-core pyspark.tests.test_pin_thread

Thus we need some auto-discover way to find all testcase rather than specified every case by manually.

related: https://github.com/apache/spark/pull/32867

### Does this PR introduce _any_ user-facing change?
No


### How was this patch tested?
1. Add doc tests for _discover_python_unittests.
2. Compare the CI results (this patch and before), see diff in:
Build modules: pyspark-sql, pyspark-mllib, pyspark-resource: https://www.diffchecker.com/4RAQydBB
Build modules: pyspark-core, pyspark-streaming, pyspark-ml: https://www.diffchecker.com/F1ccZDKG
Build modules: pyspark-pandas：https://www.diffchecker.com/eBDne4uA
Build modules: pyspark-pandas-slow：https://www.diffchecker.com/lySQGrhA
3. local test for python modules:
./dev/run-tests --parallelism 2 --modules ""pyspark-sql""",https://api.github.com/repos/apache/spark/issues/33174/timeline,,spark,apache,Yikun,1736354,MDQ6VXNlcjE3MzYzNTQ=,https://avatars.githubusercontent.com/u/1736354?v=4,,https://api.github.com/users/Yikun,https://github.com/Yikun,https://api.github.com/users/Yikun/followers,https://api.github.com/users/Yikun/following{/other_user},https://api.github.com/users/Yikun/gists{/gist_id},https://api.github.com/users/Yikun/starred{/owner}{/repo},https://api.github.com/users/Yikun/subscriptions,https://api.github.com/users/Yikun/orgs,https://api.github.com/users/Yikun/repos,https://api.github.com/users/Yikun/events{/privacy},https://api.github.com/users/Yikun/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/33174,https://github.com/apache/spark/pull/33174,https://github.com/apache/spark/pull/33174.diff,https://github.com/apache/spark/pull/33174.patch,,https://api.github.com/repos/apache/spark/issues/33174/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
226,https://api.github.com/repos/apache/spark/issues/33008,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/33008/labels{/name},https://api.github.com/repos/apache/spark/issues/33008/comments,https://api.github.com/repos/apache/spark/issues/33008/events,https://github.com/apache/spark/pull/33008,926741465,MDExOlB1bGxSZXF1ZXN0Njc1MDA0NTAx,33008,[WIP][SPARK-35801][SQL] Support DELETE operations that require rewriting data,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,23,2021-06-22T01:40:22Z,2021-11-13T04:15:08Z,,CONTRIBUTOR,,True,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error message, please read the guideline first:
     https://spark.apache.org/error-message-guidelines.html
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->

This WIP PR shows how we can use the proposed API in SPARK-35801 (per [design doc](https://docs.google.com/document/d/12Ywmc47j3l2WF4anG5vL4qlrhT2OKigb7_EbIKhxg60)) to support DELETE statements that require rewriting data.

**Note**: This PR must be split into a number of smaller PRs if we decide to adopt this approach. All changes are grouped here only to simplify the review process and support the design doc.

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->

These changes are required so that Spark can provide support for DELETE, UPDATE, MERGE statements.

### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->

Yes, this PR introduces a set of new APIs for Data Source V2.

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->

This PR comes with a trivial test. More tests to come.
",https://api.github.com/repos/apache/spark/issues/33008/timeline,,spark,apache,aokolnychyi,6235869,MDQ6VXNlcjYyMzU4Njk=,https://avatars.githubusercontent.com/u/6235869?v=4,,https://api.github.com/users/aokolnychyi,https://github.com/aokolnychyi,https://api.github.com/users/aokolnychyi/followers,https://api.github.com/users/aokolnychyi/following{/other_user},https://api.github.com/users/aokolnychyi/gists{/gist_id},https://api.github.com/users/aokolnychyi/starred{/owner}{/repo},https://api.github.com/users/aokolnychyi/subscriptions,https://api.github.com/users/aokolnychyi/orgs,https://api.github.com/users/aokolnychyi/repos,https://api.github.com/users/aokolnychyi/events{/privacy},https://api.github.com/users/aokolnychyi/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/33008,https://github.com/apache/spark/pull/33008,https://github.com/apache/spark/pull/33008.diff,https://github.com/apache/spark/pull/33008.patch,,https://api.github.com/repos/apache/spark/issues/33008/reactions,11,0,0,0,11,0,0,0,0,,,,,,,,,,,,,,,,,,
227,https://api.github.com/repos/apache/spark/issues/32987,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/32987/labels{/name},https://api.github.com/repos/apache/spark/issues/32987/comments,https://api.github.com/repos/apache/spark/issues/32987/events,https://github.com/apache/spark/pull/32987,925601145,MDExOlB1bGxSZXF1ZXN0Njc0MDM2MzE5,32987,[SPARK-35564][SQL] Support subexpression elimination for conditionally evaluated expressions,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,36,2021-06-20T13:35:30Z,2021-12-23T20:54:09Z,,CONTRIBUTOR,,False,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error message, please read the guideline first:
     https://spark.apache.org/error-message-guidelines.html
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->
I am proposing to add support for conditionally evaluated expressions during subexpression elimination. Currently, only expressions that will definitely be always at least twice are candidates for subexpression elimination. This PR updates that logic so that expressions that are always evaluated at least once and conditionally evaluated at least once are also candidates for subexpression elimination. This helps optimize a common case during data normalization and cleaning and want to null out values that don't match a certain pattern, where you have something like:

```
transformed = F.regexp_replace(F.lower(F.trim('my_column')))
df.withColumn('normalized_value', F.when(F.length(transformed) > 0, transformed))
```
or
```
df.withColumn('normalized_value', F.when(transformed.rlike(<some regex>), transformed))
```

In these cases, `transformed` will always be fully calculated twice, because it might only be needed once. I am proposing creating a subexpression for `transformed` in this case.

In practice I've seen a decrease in runtime and codegen size of 10-30% in our production pipelines that heavily make use of this type of logic.

The only potential downside is creating extra subexpressions, and therefore function calls, more than necessary. This should only be an issue for certain edge cases where your conditional overwhelming evaluates to false. And then the only overhead is running your conditional logic potentially in a separate function rather than inlined in the codegen. I added a config to control this behavior if that is actually a real concern to anyone, but I'd be happy to just remove the config.

I also updated some of the existing logic for common expressions in coalesce and when that are actually better handled by the new logic, since you are only guaranteed to have the first value of a Coalesce evaluated, as well as the first conditional of a CaseWhen expression.


### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->
To increase the performance of conditional expressions.


### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->
No, just performance improvements.

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->
New and updated UT.
",https://api.github.com/repos/apache/spark/issues/32987/timeline,,spark,apache,Kimahriman,3536454,MDQ6VXNlcjM1MzY0NTQ=,https://avatars.githubusercontent.com/u/3536454?v=4,,https://api.github.com/users/Kimahriman,https://github.com/Kimahriman,https://api.github.com/users/Kimahriman/followers,https://api.github.com/users/Kimahriman/following{/other_user},https://api.github.com/users/Kimahriman/gists{/gist_id},https://api.github.com/users/Kimahriman/starred{/owner}{/repo},https://api.github.com/users/Kimahriman/subscriptions,https://api.github.com/users/Kimahriman/orgs,https://api.github.com/users/Kimahriman/repos,https://api.github.com/users/Kimahriman/events{/privacy},https://api.github.com/users/Kimahriman/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/32987,https://github.com/apache/spark/pull/32987,https://github.com/apache/spark/pull/32987.diff,https://github.com/apache/spark/pull/32987.patch,,https://api.github.com/repos/apache/spark/issues/32987/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
228,https://api.github.com/repos/apache/spark/issues/32769,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/32769/labels{/name},https://api.github.com/repos/apache/spark/issues/32769/comments,https://api.github.com/repos/apache/spark/issues/32769/events,https://github.com/apache/spark/pull/32769,910450034,MDExOlB1bGxSZXF1ZXN0NjYwODY4NjAz,32769,[SPARK-35630][SQL] ExpandExec should not introduce unnecessary exchanges,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,18,2021-06-03T12:36:03Z,2021-12-08T12:54:09Z,,CONTRIBUTOR,,False,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error message, please read the guideline first:
     https://spark.apache.org/error-message-guidelines.html
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->
Improved `ExpandExec` so it would retain its child's outputPartitioning, when possible. 

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->
Currently `ExpandExecs` `outputPartitioning` is always `UnknownPartitioning(0)`. In some cases we do actually know the correct output partitioning. In those cases we could reduce the number of exchanges by using the correct partitioning.

### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->
No

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->
New UT
",https://api.github.com/repos/apache/spark/issues/32769/timeline,,spark,apache,tanelk,3342974,MDQ6VXNlcjMzNDI5NzQ=,https://avatars.githubusercontent.com/u/3342974?v=4,,https://api.github.com/users/tanelk,https://github.com/tanelk,https://api.github.com/users/tanelk/followers,https://api.github.com/users/tanelk/following{/other_user},https://api.github.com/users/tanelk/gists{/gist_id},https://api.github.com/users/tanelk/starred{/owner}{/repo},https://api.github.com/users/tanelk/subscriptions,https://api.github.com/users/tanelk/orgs,https://api.github.com/users/tanelk/repos,https://api.github.com/users/tanelk/events{/privacy},https://api.github.com/users/tanelk/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/32769,https://github.com/apache/spark/pull/32769,https://github.com/apache/spark/pull/32769.diff,https://github.com/apache/spark/pull/32769.patch,,https://api.github.com/repos/apache/spark/issues/32769/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
229,https://api.github.com/repos/apache/spark/issues/32766,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/32766/labels{/name},https://api.github.com/repos/apache/spark/issues/32766/comments,https://api.github.com/repos/apache/spark/issues/32766/events,https://github.com/apache/spark/pull/32766,910342711,MDExOlB1bGxSZXF1ZXN0NjYwNzc5MDcw,32766,[SPARK-35627][CORE] Decommission executors in batches to not overload network bandwidth,"[{'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,8,2021-06-03T10:14:59Z,2021-10-05T22:21:13Z,,NONE,,False,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error message, please read the guideline first:
     https://spark.apache.org/error-message-guidelines.html
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->
This PR is adding a thread which will run at scheduled interval to ask a batch of executors to start decommissioning themselves.

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->
Currenlty, each executor is asked to starts offloading rdd and shuffle blocks as soon it is decommissioned. This can overload the network bandwidth of the application.

### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->
No


### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->
UT in progress",https://api.github.com/repos/apache/spark/issues/32766/timeline,,spark,apache,q2w,12875634,MDQ6VXNlcjEyODc1NjM0,https://avatars.githubusercontent.com/u/12875634?v=4,,https://api.github.com/users/q2w,https://github.com/q2w,https://api.github.com/users/q2w/followers,https://api.github.com/users/q2w/following{/other_user},https://api.github.com/users/q2w/gists{/gist_id},https://api.github.com/users/q2w/starred{/owner}{/repo},https://api.github.com/users/q2w/subscriptions,https://api.github.com/users/q2w/orgs,https://api.github.com/users/q2w/repos,https://api.github.com/users/q2w/events{/privacy},https://api.github.com/users/q2w/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/32766,https://github.com/apache/spark/pull/32766,https://github.com/apache/spark/pull/32766.diff,https://github.com/apache/spark/pull/32766.patch,,https://api.github.com/repos/apache/spark/issues/32766/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
230,https://api.github.com/repos/apache/spark/issues/32655,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/32655/labels{/name},https://api.github.com/repos/apache/spark/issues/32655/comments,https://api.github.com/repos/apache/spark/issues/32655/events,https://github.com/apache/spark/pull/32655,900075089,MDExOlB1bGxSZXF1ZXN0NjUxNjg1MjE0,32655,[SPARK-33743]change TimestampType match to datetime2 instead of datetime for MsSQLServerDialect,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,4,2021-05-24T22:46:36Z,2021-11-10T00:37:20Z,,NONE,,False,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error message, please read the guideline first:
     https://spark.apache.org/error-message-guidelines.html
-->

SPARK-33743 is to change datetime datatype mapping in JDBC mssqldialect.
### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->
override def getJDBCType(dt: DataType): Option[JdbcType] = dt match {
case TimestampType => Some(JdbcType(""DATETIME2"", java.sql.Types.TIMESTAMP))

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->
Spark datetime type is timestamp type. This supports a microsecond resolution.
Sql supports 2 date time types:

datetime can support only milli seconds resolution (0 to 999).
datetime2 is extension of datetime , is compatible with datetime and supports 0 to 9999999 sub second resolution.
datetime2 (Transact-SQL) - SQL Server | Microsoft Docs
datetime (Transact-SQL) - SQL Server | Microsoft Docs

Currently MsSQLServerDialect maps timestamp type to datetime. Datetime only allows 3 digits of microseconds. This implies results in errors when writing timestamp with more than 3 digits of microseconds to sql server table. We want to map timestamp to datetime2, which is compatible with datetime but allows 7 digits of microseconds.

### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->
No.

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->
Unit tests were updated and passed in JDBCSuit.scala.
E2E test done with SQL Server.",https://api.github.com/repos/apache/spark/issues/32655/timeline,,spark,apache,luxu1-ms,68044595,MDQ6VXNlcjY4MDQ0NTk1,https://avatars.githubusercontent.com/u/68044595?v=4,,https://api.github.com/users/luxu1-ms,https://github.com/luxu1-ms,https://api.github.com/users/luxu1-ms/followers,https://api.github.com/users/luxu1-ms/following{/other_user},https://api.github.com/users/luxu1-ms/gists{/gist_id},https://api.github.com/users/luxu1-ms/starred{/owner}{/repo},https://api.github.com/users/luxu1-ms/subscriptions,https://api.github.com/users/luxu1-ms/orgs,https://api.github.com/users/luxu1-ms/repos,https://api.github.com/users/luxu1-ms/events{/privacy},https://api.github.com/users/luxu1-ms/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/32655,https://github.com/apache/spark/pull/32655,https://github.com/apache/spark/pull/32655.diff,https://github.com/apache/spark/pull/32655.patch,,https://api.github.com/repos/apache/spark/issues/32655/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
231,https://api.github.com/repos/apache/spark/issues/32562,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/32562/labels{/name},https://api.github.com/repos/apache/spark/issues/32562/comments,https://api.github.com/repos/apache/spark/issues/32562/events,https://github.com/apache/spark/pull/32562,892694415,MDExOlB1bGxSZXF1ZXN0NjQ1MzMyNTY1,32562,[WIP][SPARK-35414][SQL] Submit broadcast collect job first to avoid broadcast timeout in AQE,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,3,2021-05-16T15:26:35Z,2021-11-06T06:41:44Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?
1. replace executeCollectIterator() by executeCollectIteratorFuture() in SparkPlan.scala to run collect query in async way and return the future of collect result
2. in BroadcastExchangeExec->relationFuture, call executeCollectIteratorFuture() in current thread and get the collectFuture, wait collectFuture in ""broadcast-exchange"" thread 


### Why are the changes needed?
#31269 gives a partial fix to SPARK-33933, which is not a perfect solution. This changes can make sure the broadcast collect job is submitted before shuffle map jobs. #31269 ensure the calling of materialize() of BroadcastQueryStage is before ShuffleQueryStage. In BroadcastQueryStage's materialize(), doPrepare() will call relationFuture, which will submit collect job before return the future.

### Does this PR introduce _any_ user-facing change?
No


### How was this patch tested?
Add UT
",https://api.github.com/repos/apache/spark/issues/32562/timeline,,spark,apache,zhongyu09,3882710,MDQ6VXNlcjM4ODI3MTA=,https://avatars.githubusercontent.com/u/3882710?v=4,,https://api.github.com/users/zhongyu09,https://github.com/zhongyu09,https://api.github.com/users/zhongyu09/followers,https://api.github.com/users/zhongyu09/following{/other_user},https://api.github.com/users/zhongyu09/gists{/gist_id},https://api.github.com/users/zhongyu09/starred{/owner}{/repo},https://api.github.com/users/zhongyu09/subscriptions,https://api.github.com/users/zhongyu09/orgs,https://api.github.com/users/zhongyu09/repos,https://api.github.com/users/zhongyu09/events{/privacy},https://api.github.com/users/zhongyu09/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/32562,https://github.com/apache/spark/pull/32562,https://github.com/apache/spark/pull/32562.diff,https://github.com/apache/spark/pull/32562.patch,,https://api.github.com/repos/apache/spark/issues/32562/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
232,https://api.github.com/repos/apache/spark/issues/32477,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/32477/labels{/name},https://api.github.com/repos/apache/spark/issues/32477/comments,https://api.github.com/repos/apache/spark/issues/32477/events,https://github.com/apache/spark/pull/32477,880493723,MDExOlB1bGxSZXF1ZXN0NjM0MTQ4NjEz,32477,[SPARK-35348][SQL] Support the utils for escapse the regex for ANSI SQL: SIMILAR TO … ESCAPE syntax,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,35,2021-05-08T08:21:29Z,2021-12-15T07:13:14Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?
`ANSI SQL: SIMILAR TO ... ESCAPE` is very useful.
There are some mainstream database support the syntax.
**PostgreSQL**:
https://www.postgresql.org/docs/current/functions-matching.html#FUNCTIONS-SIMILARTO-REGEXP

**Redshift**:
https://docs.aws.amazon.com/redshift/latest/dg/pattern-matching-conditions-similar-to.html

**Sybase**:
http://infocenter.sybase.com/help/index.jsp?topic=/com.sybase.help.sqlanywhere.12.0.0/dbreference/like-regexp-similarto.html

**Firebird**:
http://firebirdsql.org/file/documentation/html/en/refdocs/fblangref25/firebird-25-language-reference.html#fblangref25-commons-predsiimilarto

This util supports the following pattern-matching metacharacters:

Operator | Description
-- | --
% | Matches any sequence of zero or more characters.
_ | Matches any single character.
\| | Denotes alternation (either of two alternatives).
\* | Repeat the previous item zero or more times.
\+ | Repeat the previous item one or more times.
? | Repeat the previous item zero or one time.
{m} | Repeat the previous item exactly m times.
{m,} | Repeat the previous item m or more times.
{m,n} | Repeat the previous item at least m and not more than n times.
() | Parentheses group items into a single logical item.
[...] | A bracket expression specifies a character class, just as in POSIX regular expressions.

**Note**
`SIMILAR TO` is similar to `RLIKE`, but with the following differences:
       1. The `SIMILAR TO` operator returns true only if its pattern matches the entire string,
          unlike `RLIKE` behavior, where the pattern can match any portion of the string.
       2. The regex string allow use _ and % as wildcard characters denoting any single character
          and any string, respectively (these are comparable to . and .* in POSIX regular
          expressions).
       3. The regex string allow use escape character like `LIKE` behavior.
       4. '.', '^' and '$' is not a meta character for `SIMILAR TO`.

### Why are the changes needed?
`ANSI SQL: SIMILAR TO ... ESCAPE` is very useful.


### Does this PR introduce _any_ user-facing change?
Yes, a new feature.


### How was this patch tested?
New tests
",https://api.github.com/repos/apache/spark/issues/32477/timeline,,spark,apache,beliefer,8486025,MDQ6VXNlcjg0ODYwMjU=,https://avatars.githubusercontent.com/u/8486025?v=4,,https://api.github.com/users/beliefer,https://github.com/beliefer,https://api.github.com/users/beliefer/followers,https://api.github.com/users/beliefer/following{/other_user},https://api.github.com/users/beliefer/gists{/gist_id},https://api.github.com/users/beliefer/starred{/owner}{/repo},https://api.github.com/users/beliefer/subscriptions,https://api.github.com/users/beliefer/orgs,https://api.github.com/users/beliefer/repos,https://api.github.com/users/beliefer/events{/privacy},https://api.github.com/users/beliefer/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/32477,https://github.com/apache/spark/pull/32477,https://github.com/apache/spark/pull/32477.diff,https://github.com/apache/spark/pull/32477.patch,,https://api.github.com/repos/apache/spark/issues/32477/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
233,https://api.github.com/repos/apache/spark/issues/32397,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/32397/labels{/name},https://api.github.com/repos/apache/spark/issues/32397/comments,https://api.github.com/repos/apache/spark/issues/32397/events,https://github.com/apache/spark/pull/32397,870986343,MDExOlB1bGxSZXF1ZXN0NjI2MDgyMDI3,32397,"[SPARK-35084][CORE] Spark 3: supporting ""--packages"" in  k8s cluster mode","[{'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,53,2021-04-29T12:56:44Z,2021-12-28T11:33:29Z,,NONE,,False,"### What changes were proposed in this pull request?
Supporting '--packages' in the k8s cluster mode

### Why are the changes needed?
In spark 3, '--packages' in the k8s cluster mode is not supported. I expected that managing dependencies by using packages like spark 2.

Spark 2.4.5

https://github.com/apache/spark/blob/v2.4.5/core/src/main/scala/org/apache/spark/deploy/SparkSubmit.scala

```scala
     if (!isMesosCluster && !isStandAloneCluster) {
      // Resolve maven dependencies if there are any and add classpath to jars. Add them to py-files
      // too for packages that include Python code
      val resolvedMavenCoordinates = DependencyUtils.resolveMavenDependencies(
        args.packagesExclusions, args.packages, args.repositories, args.ivyRepoPath,
        args.ivySettingsPath)
      
      if (!StringUtils.isBlank(resolvedMavenCoordinates)) {
        args.jars = mergeFileLists(args.jars, resolvedMavenCoordinates)
        if (args.isPython || isInternal(args.primaryResource)) {
          args.pyFiles = mergeFileLists(args.pyFiles, resolvedMavenCoordinates)
        }
      } 
      
      // install any R packages that may have been passed through --jars or --packages.
      // Spark Packages may contain R source code inside the jar.
      if (args.isR && !StringUtils.isBlank(args.jars)) {
        RPackageUtils.checkAndBuildRPackage(args.jars, printStream, args.verbose)
      }
    } 
 ```

Spark 3.0.2

https://github.com/apache/spark/blob/v3.0.2/core/src/main/scala/org/apache/spark/deploy/SparkSubmit.scala

```scala
       if (!StringUtils.isBlank(resolvedMavenCoordinates)) {
        // In K8s client mode, when in the driver, add resolved jars early as we might need
        // them at the submit time for artifact downloading.
        // For example we might use the dependencies for downloading
        // files from a Hadoop Compatible fs eg. S3. In this case the user might pass:
        // --packages com.amazonaws:aws-java-sdk:1.7.4:org.apache.hadoop:hadoop-aws:2.7.6
        if (isKubernetesClusterModeDriver) {
          val loader = getSubmitClassLoader(sparkConf)
          for (jar <- resolvedMavenCoordinates.split("","")) {
            addJarToClasspath(jar, loader)
          }
        } else if (isKubernetesCluster) {
          // We need this in K8s cluster mode so that we can upload local deps
          // via the k8s application, like in cluster mode driver
          childClasspath ++= resolvedMavenCoordinates.split("","")
        } else {
          args.jars = mergeFileLists(args.jars, resolvedMavenCoordinates)
          if (args.isPython || isInternal(args.primaryResource)) {
            args.pyFiles = mergeFileLists(args.pyFiles, resolvedMavenCoordinates)
          }
        }
      }
```

unlike spark2, in spark 3, jars are not added in any place.

### Does this PR introduce _any_ user-facing change?
Unlike spark 2, resolved jars are added not in cluster mode spark submit but in driver.

It's because in spark 3, the feature is added that is uploading jars with prefix ""file://"" to s3.
So, if resolved jars are added in spark submit, every jars from packages are uploading to s3! When I tested it, it is very bad experience to me.

### How was this patch tested?
In my k8s environment, i tested the code.
",https://api.github.com/repos/apache/spark/issues/32397/timeline,,spark,apache,ocworld,13185662,MDQ6VXNlcjEzMTg1NjYy,https://avatars.githubusercontent.com/u/13185662?v=4,,https://api.github.com/users/ocworld,https://github.com/ocworld,https://api.github.com/users/ocworld/followers,https://api.github.com/users/ocworld/following{/other_user},https://api.github.com/users/ocworld/gists{/gist_id},https://api.github.com/users/ocworld/starred{/owner}{/repo},https://api.github.com/users/ocworld/subscriptions,https://api.github.com/users/ocworld/orgs,https://api.github.com/users/ocworld/repos,https://api.github.com/users/ocworld/events{/privacy},https://api.github.com/users/ocworld/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/32397,https://github.com/apache/spark/pull/32397,https://github.com/apache/spark/pull/32397.diff,https://github.com/apache/spark/pull/32397.patch,,https://api.github.com/repos/apache/spark/issues/32397/reactions,9,9,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
234,https://api.github.com/repos/apache/spark/issues/32365,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/32365/labels{/name},https://api.github.com/repos/apache/spark/issues/32365/comments,https://api.github.com/repos/apache/spark/issues/32365/events,https://github.com/apache/spark/pull/32365,868688914,MDExOlB1bGxSZXF1ZXN0NjI0MTcyNjM2,32365,[SPARK-35228][SQL] Add expression ToHiveString for keep consistent between hive/spark format in df.show,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,78,2021-04-27T10:07:37Z,2021-09-28T10:29:09Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?
Since `spark-sql` and `df.show` show result in different style of string, 
`spark-sql` show data follow hive and  `df.show` show result in spark's result.
Here we add expression `ToHiveString` for keep consistent between `df.show` and `spark-sql` shell

It means we can use this expression to `df.show()` show result in hive format.

The main different between cast to string  and toPrettyString
|  DateType   | Cast(value, StringType) |  ToPrettyString(value)|
|  ----  | ----  | --- |
| DayTimeIntervalType  | INTERVAL [-]'[-]d HH:mm:ss.nnnnnnnnn' DAY TO SECOND |  [-]d HH:mm:ss.nnnnnnnnn   |
| YearMonthIntervalType  | INTERVAL [-]'[-]YYYY-MM' YEAR TO MONTH |  [-]YYYY-MM   |
| ArrayType  | [elem, elem, elem] |  [elem,elem,elem]   |
| MapType  | {key1 -> value1, key2 -> value2} |  {key1:value1,key2:value2} |
| StructType  | {1, 2.0, 3.0} |  {""c1"":1,""c2"":2.0,""c3"":3.0}  |
| DecimalType |   decimal.toString |  decimal.toPlainString |

### Why are the changes needed?
Add expression ToHiveString for keep consistent between hive/spark format in df.show

### Does this PR introduce _any_ user-facing change?
No

### How was this patch tested?
Added UT
",https://api.github.com/repos/apache/spark/issues/32365/timeline,,spark,apache,AngersZhuuuu,46485123,MDQ6VXNlcjQ2NDg1MTIz,https://avatars.githubusercontent.com/u/46485123?v=4,,https://api.github.com/users/AngersZhuuuu,https://github.com/AngersZhuuuu,https://api.github.com/users/AngersZhuuuu/followers,https://api.github.com/users/AngersZhuuuu/following{/other_user},https://api.github.com/users/AngersZhuuuu/gists{/gist_id},https://api.github.com/users/AngersZhuuuu/starred{/owner}{/repo},https://api.github.com/users/AngersZhuuuu/subscriptions,https://api.github.com/users/AngersZhuuuu/orgs,https://api.github.com/users/AngersZhuuuu/repos,https://api.github.com/users/AngersZhuuuu/events{/privacy},https://api.github.com/users/AngersZhuuuu/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/32365,https://github.com/apache/spark/pull/32365,https://github.com/apache/spark/pull/32365.diff,https://github.com/apache/spark/pull/32365.patch,,https://api.github.com/repos/apache/spark/issues/32365/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
235,https://api.github.com/repos/apache/spark/issues/32332,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/32332/labels{/name},https://api.github.com/repos/apache/spark/issues/32332/comments,https://api.github.com/repos/apache/spark/issues/32332/events,https://github.com/apache/spark/pull/32332,866947011,MDExOlB1bGxSZXF1ZXN0NjIyNzIwNDQ1,32332,[SPARK-35211][PYTHON] verify inferred schema for _create_dataframe,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1982260031, 'node_id': 'MDU6TGFiZWwxOTgyMjYwMDMx', 'url': 'https://api.github.com/repos/apache/spark/labels/PYTHON', 'name': 'PYTHON', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,40,2021-04-25T08:14:25Z,2021-11-17T03:35:04Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?
Do extra schema verification after it is inferred

This PR do not introduce any semantic changes except for the extra schema verification.

This pr fixes SPARK-35211 when schema verification is turned on. If schema verification is turned off, the bug described in SPARK-35211 still exists. I will create another PR to solve the issue.


### Why are the changes needed?
``` python
spark.conf.set(""spark.sql.execution.arrow.pyspark.enabled"", ""false"")
from pyspark.testing.sqlutils  import ExamplePoint
import pandas as pd
pdf = pd.DataFrame({'point': pd.Series([ExamplePoint(1, 1), ExamplePoint(2, 2)])})
df = spark.createDataFrame(pdf)
df.show()
```
The result is not correct because of incorrect type conversion. Here is the incorrect result:
```
+----------+
|     point|
+----------+
|(0.0, 0.0)|
|(0.0, 0.0)|
+----------+
```

With this PR, type check will be performed:
```
(spark) ➜  spark git:(sadhen/SPARK-35211) ✗ bin/pyspark
Python 3.8.8 (default, Feb 24 2021, 13:46:16)
[Clang 10.0.0 ] :: Anaconda, Inc. on darwin
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to ""WARN"".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
21/04/24 17:42:23 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /__ / .__/\_,_/_/ /_/\_\   version 3.2.0-SNAPSHOT
      /_/

Using Python version 3.8.8 (default, Feb 24 2021 13:46:16)
Spark context Web UI available at http://172.30.0.12:4040
Spark context available as 'sc' (master = local[*], app id = local-1619257343692).
SparkSession available as 'spark'.
>>> spark.conf.set(""spark.sql.execution.arrow.pyspark.enabled"", ""false"")
>>> from pyspark.testing.sqlutils  import ExamplePoint
>>> import pandas as pd
>>> pdf = pd.DataFrame({'point': pd.Series([ExamplePoint(1, 1), ExamplePoint(2, 2)])})
>>> df = spark.createDataFrame(pdf)
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/Users/da/github/apache/spark/python/pyspark/sql/session.py"", line 653, in createDataFrame
    return super(SparkSession, self).createDataFrame(
  File ""/Users/da/github/apache/spark/python/pyspark/sql/pandas/conversion.py"", line 340, in createDataFrame
    return self._create_dataframe(data, schema, samplingRatio, verifySchema)
  File ""/Users/da/github/apache/spark/python/pyspark/sql/session.py"", line 699, in _create_dataframe
    rdd, schema = self._createFromLocal(map(prepare, data), schema)
  File ""/Users/da/github/apache/spark/python/pyspark/sql/session.py"", line 499, in _createFromLocal
    data = list(data)
  File ""/Users/da/github/apache/spark/python/pyspark/sql/session.py"", line 688, in prepare
    verify_func(obj)
  File ""/Users/da/github/apache/spark/python/pyspark/sql/types.py"", line 1409, in verify
    verify_value(obj)
  File ""/Users/da/github/apache/spark/python/pyspark/sql/types.py"", line 1390, in verify_struct
    verifier(v)
  File ""/Users/da/github/apache/spark/python/pyspark/sql/types.py"", line 1409, in verify
    verify_value(obj)
  File ""/Users/da/github/apache/spark/python/pyspark/sql/types.py"", line 1304, in verify_udf
    verifier(dataType.toInternal(obj))
  File ""/Users/da/github/apache/spark/python/pyspark/sql/types.py"", line 1409, in verify
    verify_value(obj)
  File ""/Users/da/github/apache/spark/python/pyspark/sql/types.py"", line 1354, in verify_array
    element_verifier(i)
  File ""/Users/da/github/apache/spark/python/pyspark/sql/types.py"", line 1409, in verify
    verify_value(obj)
  File ""/Users/da/github/apache/spark/python/pyspark/sql/types.py"", line 1403, in verify_default
    verify_acceptable_types(obj)
  File ""/Users/da/github/apache/spark/python/pyspark/sql/types.py"", line 1291, in verify_acceptable_types
    raise TypeError(new_msg(""%s can not accept object %r in type %s""
TypeError: element in array field point: DoubleType can not accept object 1 in type <class 'int'>
```


### Does this PR introduce _any_ user-facing change?
No



### How was this patch tested?
unit test

```
python/run-tests --testnames pyspark.sql.tests.test_dataframe
```
",https://api.github.com/repos/apache/spark/issues/32332/timeline,,spark,apache,darcy-shen,1267865,MDQ6VXNlcjEyNjc4NjU=,https://avatars.githubusercontent.com/u/1267865?v=4,,https://api.github.com/users/darcy-shen,https://github.com/darcy-shen,https://api.github.com/users/darcy-shen/followers,https://api.github.com/users/darcy-shen/following{/other_user},https://api.github.com/users/darcy-shen/gists{/gist_id},https://api.github.com/users/darcy-shen/starred{/owner}{/repo},https://api.github.com/users/darcy-shen/subscriptions,https://api.github.com/users/darcy-shen/orgs,https://api.github.com/users/darcy-shen/repos,https://api.github.com/users/darcy-shen/events{/privacy},https://api.github.com/users/darcy-shen/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/32332,https://github.com/apache/spark/pull/32332,https://github.com/apache/spark/pull/32332.diff,https://github.com/apache/spark/pull/32332.patch,,https://api.github.com/repos/apache/spark/issues/32332/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
236,https://api.github.com/repos/apache/spark/issues/32298,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/32298/labels{/name},https://api.github.com/repos/apache/spark/issues/32298/comments,https://api.github.com/repos/apache/spark/issues/32298/events,https://github.com/apache/spark/pull/32298,864996333,MDExOlB1bGxSZXF1ZXN0NjIxMTMxODQ3,32298,[SPARK-34079][SQL] Merge non-correlated scalar subqueries,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,94,2021-04-22T14:08:16Z,2021-12-10T14:01:10Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?
This PR adds a new optimizer rule `MergeScalarSubqueries` to merge multiple non-correlated `ScalarSubquery`s to compute multiple scalar values once.

E.g. the following query:
```
SELECT
  (SELECT avg(a) FROM t GROUP BY b),
  (SELECT sum(b) FROM t GROUP BY b)
```
is optimized from:
```
Project [scalar-subquery#231 [] AS scalarsubquery()#241, scalar-subquery#232 [] AS scalarsubquery()#242L]
:  :- Aggregate [b#234], [avg(a#233) AS avg(a)#236]
:  :  +- Relation default.t[a#233,b#234] parquet
:  +- Aggregate [b#240], [sum(b#240) AS sum(b)#238L]
:     +- Project [b#240]
:        +- Relation default.t[a#239,b#240] parquet
+- OneRowRelation
```
to:
```
CommonScalarSubqueries [scalar-subquery#250 []]
:  +- Project [named_struct(avg(a), avg(a)#236, sum(b), sum(b)#238L) AS mergedValue#249]
:     +- Aggregate [b#234], [avg(a#233) AS avg(a)#236, sum(b#234) AS sum(b)#238L]
:        +- Project [a#233, b#234]
:           +- Relation default.t[a#233,b#234] parquet
+- Project [scalarsubqueryreference(0, 0, DoubleType, 231) AS scalarsubquery()#241,
            scalarsubqueryreference(0, 1, LongType, 232) AS scalarsubquery()#242L]
   +- OneRowRelation
```
and in the physical plan subquery references are replaced to the same planned subquery instance and are reused:
```
== Physical Plan ==
AdaptiveSparkPlan isFinalPlan=true
+- == Final Plan ==
   *(1) Project [Subquery subquery#250, [id=#104].avg(a) AS scalarsubquery()#241, ReusedSubquery Subquery subquery#250, [id=#104].sum(b) AS scalarsubquery()#242L]
   :  :- Subquery subquery#250, [id=#104]
   :  :  +- AdaptiveSparkPlan isFinalPlan=true
         +- == Final Plan ==
            *(2) HashAggregate(keys=[b#234], functions=[avg(a#233), sum(b#234)], output=[mergedValue#249])
            +- CustomShuffleReader coalesced
               +- ShuffleQueryStage 0
                  +- Exchange hashpartitioning(b#234, 5), ENSURE_REQUIREMENTS, [id=#125]
                     +- *(1) HashAggregate(keys=[b#234], functions=[partial_avg(a#233), partial_sum(b#234)], output=[b#234, sum#252, count#253L, sum#254L])
                        +- *(1) ColumnarToRow
                           +- FileScan parquet default.t[a#233,b#234] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<a:int,b:int>
         +- == Initial Plan ==
            HashAggregate(keys=[b#234], functions=[avg(a#233), sum(b#234)], output=[mergedValue#249])
            +- Exchange hashpartitioning(b#234, 5), ENSURE_REQUIREMENTS, [id=#102]
               +- HashAggregate(keys=[b#234], functions=[partial_avg(a#233), partial_sum(b#234)], output=[b#234, sum#252, count#253L, sum#254L])
                  +- FileScan parquet default.t[a#233,b#234] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<a:int,b:int>
   :  +- ReusedSubquery Subquery subquery#250, [id=#104]
   +- *(1) Scan OneRowRelation[]
+- == Initial Plan ==
   ...
```

Please note that the above simple example could be easily optimized into a common select expression without reuse node, but this PR can handle more complex queries as well. 

### Why are the changes needed?
Performance improvement.
```
[info] TPCDS Snappy:                             Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)   Relative
[info] ------------------------------------------------------------------------------------------------------------------------
[info] q9 - MergeScalarSubqueries off                    51090          54205         NaN          0.0      Infinity       1.0X
[info] q9 - MergeScalarSubqueries on                     17478          18497        1184          0.0      Infinity       2.9X

[info] TPCDS Snappy:                             Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)   Relative
[info] ------------------------------------------------------------------------------------------------------------------------
[info] q9b - MergeScalarSubqueries off                   14879          15092         164          0.0      Infinity       1.0X
[info] q9b - MergeScalarSubqueries on                     3554           3691         121          0.0      Infinity       4.2X
```
Please find `q9b` in the description of SPARK-34079. It is a variant of [q9.sql](https://github.com/apache/spark/blob/master/sql/core/src/test/resources/tpcds/q9.sql) using CTE.
The performance improvement in case of `q9` comes from merging 15 subqueries into 5 and in case of `q9b` it comes from merging 5 subqueries into 1.

### Does this PR introduce _any_ user-facing change?
No.

### How was this patch tested?
Existing and new UTs.
",https://api.github.com/repos/apache/spark/issues/32298/timeline,,spark,apache,peter-toth,7253827,MDQ6VXNlcjcyNTM4Mjc=,https://avatars.githubusercontent.com/u/7253827?v=4,,https://api.github.com/users/peter-toth,https://github.com/peter-toth,https://api.github.com/users/peter-toth/followers,https://api.github.com/users/peter-toth/following{/other_user},https://api.github.com/users/peter-toth/gists{/gist_id},https://api.github.com/users/peter-toth/starred{/owner}{/repo},https://api.github.com/users/peter-toth/subscriptions,https://api.github.com/users/peter-toth/orgs,https://api.github.com/users/peter-toth/repos,https://api.github.com/users/peter-toth/events{/privacy},https://api.github.com/users/peter-toth/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/32298,https://github.com/apache/spark/pull/32298,https://github.com/apache/spark/pull/32298.diff,https://github.com/apache/spark/pull/32298.patch,,https://api.github.com/repos/apache/spark/issues/32298/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
237,https://api.github.com/repos/apache/spark/issues/32289,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/32289/labels{/name},https://api.github.com/repos/apache/spark/issues/32289/comments,https://api.github.com/repos/apache/spark/issues/32289/events,https://github.com/apache/spark/pull/32289,864624262,MDExOlB1bGxSZXF1ZXN0NjIwODI3NjQ5,32289,[SPARK-33357][K8S] Support Spark application managing with SparkAppHandle on Kubernetes,"[{'id': 1406605057, 'node_id': 'MDU6TGFiZWwxNDA2NjA1MDU3', 'url': 'https://api.github.com/repos/apache/spark/labels/KUBERNETES', 'name': 'KUBERNETES', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,11,2021-04-22T07:16:00Z,2021-09-28T06:24:50Z,,NONE,,False,"Co-authored-by: hongdd <hongdongdong@cmss.chinamobile.com>

### What changes were proposed in this pull request?
Supporting SparkAppHandle object to be able to manage a running Spark application on Kubernetes. It can be used to monitor the application changes and to stop the application by pod deletion.

This Pull Request has been raised due to a inactivity of a previous one - #30520


### Why are the changes needed?
There is an inconsistency in the Spark application managing with SparkAppHandle object between Kubernetes and other resource managers such as Yarn/Mesos.

Currently, this feature is not properly implemented on Kubernetes which may cause some issues.


### Does this PR introduce _any_ user-facing change?
Yes, it changes the behavior of `SparkAppHandle` object which the user may use to communicate with the launched Spark application. Its interface is remained as it is. Some missing functionalities have been implemented.


### How was this patch tested?
Few unit tests has been added. May be found in org.apache.spark.deploy.k8s.submit package:

- `PodStatusWatcherSuite` - new ones
- `ClientSuite` - added some
",https://api.github.com/repos/apache/spark/issues/32289/timeline,,spark,apache,grarkydev,54981921,MDQ6VXNlcjU0OTgxOTIx,https://avatars.githubusercontent.com/u/54981921?v=4,,https://api.github.com/users/grarkydev,https://github.com/grarkydev,https://api.github.com/users/grarkydev/followers,https://api.github.com/users/grarkydev/following{/other_user},https://api.github.com/users/grarkydev/gists{/gist_id},https://api.github.com/users/grarkydev/starred{/owner}{/repo},https://api.github.com/users/grarkydev/subscriptions,https://api.github.com/users/grarkydev/orgs,https://api.github.com/users/grarkydev/repos,https://api.github.com/users/grarkydev/events{/privacy},https://api.github.com/users/grarkydev/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/32289,https://github.com/apache/spark/pull/32289,https://github.com/apache/spark/pull/32289.diff,https://github.com/apache/spark/pull/32289.patch,,https://api.github.com/repos/apache/spark/issues/32289/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
238,https://api.github.com/repos/apache/spark/issues/32031,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/32031/labels{/name},https://api.github.com/repos/apache/spark/issues/32031/comments,https://api.github.com/repos/apache/spark/issues/32031/events,https://github.com/apache/spark/pull/32031,848854905,MDExOlB1bGxSZXF1ZXN0NjA3NzI2MTY3,32031,[WIP] Initial work of Remote Shuffle Service on Kubernetes,"[{'id': 1406627200, 'node_id': 'MDU6TGFiZWwxNDA2NjI3MjAw', 'url': 'https://api.github.com/repos/apache/spark/labels/BUILD', 'name': 'BUILD', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981471081, 'node_id': 'MDU6TGFiZWwxOTgxNDcxMDgx', 'url': 'https://api.github.com/repos/apache/spark/labels/DOCS', 'name': 'DOCS', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,69,2021-04-01T23:37:24Z,2021-12-08T17:29:53Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?

This PR contains Remote Shuffle Service on Kubernetes. The code is mostly copied from [Uber Remote Shuffle Service](https://github.com/uber/RemoteShuffleService) and modified with some renaming. Also added Kubernetes related support which does not exist in original Uber Remote Shuffle Service.

See [here](https://github.com/hiboyang/spark/tree/remote-shuffle-service/remote-shuffle-service) for how to build and run remote shuffle service in Kubernetes. This is initial work and comments/suggestions are welcome.

### Why are the changes needed?

It is still difficult to use dynamic allocation with Spark on Kubernetes. There are several disaggregated/remote shuffle solutions in different companies. Hopefully we could get a remote shuffle implementation into Spark and enhanced in the future by the Spark community.

### Does this PR introduce _any_ user-facing change?

Yes, user could set Spark config (spark.shuffle.manager=org.apache.spark.shuffle.RssShuffleManager) to run Spark applications with remote shuffle service. It will make Spark use the new RssShuffleManager to write/read shuffle data to/from remote shuffle service.

### How was this patch tested?

Manually tested with Spark application in Kubernetes.
",https://api.github.com/repos/apache/spark/issues/32031/timeline,,spark,apache,hiboyang,14280154,MDQ6VXNlcjE0MjgwMTU0,https://avatars.githubusercontent.com/u/14280154?v=4,,https://api.github.com/users/hiboyang,https://github.com/hiboyang,https://api.github.com/users/hiboyang/followers,https://api.github.com/users/hiboyang/following{/other_user},https://api.github.com/users/hiboyang/gists{/gist_id},https://api.github.com/users/hiboyang/starred{/owner}{/repo},https://api.github.com/users/hiboyang/subscriptions,https://api.github.com/users/hiboyang/orgs,https://api.github.com/users/hiboyang/repos,https://api.github.com/users/hiboyang/events{/privacy},https://api.github.com/users/hiboyang/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/32031,https://github.com/apache/spark/pull/32031,https://github.com/apache/spark/pull/32031.diff,https://github.com/apache/spark/pull/32031.patch,,https://api.github.com/repos/apache/spark/issues/32031/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
239,https://api.github.com/repos/apache/spark/issues/31997,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/31997/labels{/name},https://api.github.com/repos/apache/spark/issues/31997/comments,https://api.github.com/repos/apache/spark/issues/31997/events,https://github.com/apache/spark/pull/31997,843617958,MDExOlB1bGxSZXF1ZXN0NjAyOTc1MzY1,31997,Search PYSPARK_PYTHON in configurations,"[{'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1982260031, 'node_id': 'MDU6TGFiZWwxOTgyMjYwMDMx', 'url': 'https://api.github.com/repos/apache/spark/labels/PYTHON', 'name': 'PYTHON', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,7,2021-03-29T17:40:57Z,2021-10-05T00:23:55Z,,NONE,,False,"Check if the PYSPARK_PYTHON was defined in the configurations passed to the context.

### What changes were proposed in this pull request?

Searches for the PYSPARK_PYTHON environment variable in the configurations passed to the Context in Python interface.

### Why are the changes needed?

When the variable is not defined in the local OS the corresponding block will use the `python3`, [line 230](https://github.com/ggarciabas/spark/blob/master/python/pyspark/context.py#L230):
```python
self.pythonExec = os.environ.get(""PYSPARK_PYTHON"", 'python3')
```
However, if some specific virtual environment is sent to the executors and/or the python path is changed in the executors, the configuration `spark.executorEnv.PYSPARK_PYTHON` will not be considered.",https://api.github.com/repos/apache/spark/issues/31997/timeline,,spark,apache,ggarciabas,3122199,MDQ6VXNlcjMxMjIxOTk=,https://avatars.githubusercontent.com/u/3122199?v=4,,https://api.github.com/users/ggarciabas,https://github.com/ggarciabas,https://api.github.com/users/ggarciabas/followers,https://api.github.com/users/ggarciabas/following{/other_user},https://api.github.com/users/ggarciabas/gists{/gist_id},https://api.github.com/users/ggarciabas/starred{/owner}{/repo},https://api.github.com/users/ggarciabas/subscriptions,https://api.github.com/users/ggarciabas/orgs,https://api.github.com/users/ggarciabas/repos,https://api.github.com/users/ggarciabas/events{/privacy},https://api.github.com/users/ggarciabas/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/31997,https://github.com/apache/spark/pull/31997,https://github.com/apache/spark/pull/31997.diff,https://github.com/apache/spark/pull/31997.patch,,https://api.github.com/repos/apache/spark/issues/31997/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
240,https://api.github.com/repos/apache/spark/issues/31267,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/31267/labels{/name},https://api.github.com/repos/apache/spark/issues/31267/comments,https://api.github.com/repos/apache/spark/issues/31267/events,https://github.com/apache/spark/pull/31267,790587745,MDExOlB1bGxSZXF1ZXN0NTU4NzczNDE3,31267,[SPARK-21195][CORE] MetricSystem should pick up dynamically registered metrics in sources,"[{'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1984378631, 'node_id': 'MDU6TGFiZWwxOTg0Mzc4NjMx', 'url': 'https://api.github.com/repos/apache/spark/labels/DSTREAM', 'name': 'DSTREAM', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,25,2021-01-21T01:23:55Z,2021-09-28T03:29:08Z,,NONE,,False,"### What changes were proposed in this pull request?
MetricSystem picks up new metrics from sources that are added throughout execution. If you do measurements via dynamic proxies you might not want to redeclare all metrics that the proxies will create and you'd prefer them to get populated as they're being produced. Right now all sources are processed only onceat startup and metrics are picked up only if they have been registered statically at compile time. Behaviour I am proposing lets you not have to declare metrics in two places.

This had been previously suggested in https://github.com/apache/spark/pull/18406 and https://github.com/apache/spark/pull/29980. I have reduced the scope of the change to just dynamic metric registration.

### Why are the changes needed?
Currently there's no way to access MetricRegistry that MetricsSystem uses to hold its state and as such it's not possible to reprocess a source. MetricsSystem throws if any metric had already been registered previously.

n.b. the MetricRegistry is added as a constructor argument to make testing easier but could as well be accessed via reflection as a private variable.

### Does this PR introduce _any_ user-facing change?
No

### How was this patch tested?
Added tests",https://api.github.com/repos/apache/spark/issues/31267/timeline,,spark,apache,robert3005,512084,MDQ6VXNlcjUxMjA4NA==,https://avatars.githubusercontent.com/u/512084?v=4,,https://api.github.com/users/robert3005,https://github.com/robert3005,https://api.github.com/users/robert3005/followers,https://api.github.com/users/robert3005/following{/other_user},https://api.github.com/users/robert3005/gists{/gist_id},https://api.github.com/users/robert3005/starred{/owner}{/repo},https://api.github.com/users/robert3005/subscriptions,https://api.github.com/users/robert3005/orgs,https://api.github.com/users/robert3005/repos,https://api.github.com/users/robert3005/events{/privacy},https://api.github.com/users/robert3005/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/31267,https://github.com/apache/spark/pull/31267,https://github.com/apache/spark/pull/31267.diff,https://github.com/apache/spark/pull/31267.patch,,https://api.github.com/repos/apache/spark/issues/31267/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
241,https://api.github.com/repos/apache/spark/issues/30565,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/30565/labels{/name},https://api.github.com/repos/apache/spark/issues/30565/comments,https://api.github.com/repos/apache/spark/issues/30565/events,https://github.com/apache/spark/pull/30565,754993855,MDExOlB1bGxSZXF1ZXN0NTMwNzkwMjQz,30565,[WIP][SPARK-33625][SQL] Subexpression elimination for whole-stage codegen in Filter,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,21,2020-12-02T06:41:47Z,2021-10-29T00:13:52Z,,MEMBER,,True,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->

This patch proposes to enable whole-stage subexpression elimination for Filter.

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->

We made subexpression elimination available for whole-stage codegen in ProjectExec. Another one operator that frequently runs into subexpressions, is Filter. We should also make whole-stage codegen subexpression elimination in FilterExec too.

### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->

No

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->

Unit test",https://api.github.com/repos/apache/spark/issues/30565/timeline,,spark,apache,viirya,68855,MDQ6VXNlcjY4ODU1,https://avatars.githubusercontent.com/u/68855?v=4,,https://api.github.com/users/viirya,https://github.com/viirya,https://api.github.com/users/viirya/followers,https://api.github.com/users/viirya/following{/other_user},https://api.github.com/users/viirya/gists{/gist_id},https://api.github.com/users/viirya/starred{/owner}{/repo},https://api.github.com/users/viirya/subscriptions,https://api.github.com/users/viirya/orgs,https://api.github.com/users/viirya/repos,https://api.github.com/users/viirya/events{/privacy},https://api.github.com/users/viirya/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/30565,https://github.com/apache/spark/pull/30565,https://github.com/apache/spark/pull/30565.diff,https://github.com/apache/spark/pull/30565.patch,,https://api.github.com/repos/apache/spark/issues/30565/reactions,2,2,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
242,https://api.github.com/repos/apache/spark/issues/29719,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/29719/labels{/name},https://api.github.com/repos/apache/spark/issues/29719/comments,https://api.github.com/repos/apache/spark/issues/29719/events,https://github.com/apache/spark/pull/29719,698516471,MDExOlB1bGxSZXF1ZXN0NDg0MzUxMTEz,29719,[SPARK-32846][SQL][PYTHON] Support createDataFrame from an RDD of pd.DataFrames,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1982260031, 'node_id': 'MDU6TGFiZWwxOTgyMjYwMDMx', 'url': 'https://api.github.com/repos/apache/spark/labels/PYTHON', 'name': 'PYTHON', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,17,2020-09-10T21:32:27Z,2021-10-21T15:40:58Z,,NONE,,False,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
-->

### What changes were proposed in this pull request?
Added support to `createDataFrame` to receive an RDD of `pd.DataFrame` objects, and convert them using arrow into an RDD of record batches which is then directly converted to a spark DF.

Added a `pandasRDD` flag to `createDataFrame` to distinguish between `RDD[pd.DataFrame]` and other RDDs without peeking into their content.

```python
from pyspark.sql import SparkSession
import pyspark
import pyarrow as pa
import numpy as np
import pandas as pd
import re

spark = SparkSession \
    .builder \
    .master(""local"") \
    .appName(""Python RDD[pd.DataFrame] to spark DF example"") \
    .getOrCreate()

# Enable Arrow-based columnar data transfers
spark.conf.set(""spark.sql.execution.arrow.pyspark.enabled"", ""true"")
sc = spark.sparkContext

# Create a spark DF from an RDD of pandas DFs
prdd = sc.range(0, 4).map(lambda x: pd.DataFrame([[x,]*4], columns=list('ABCD')))

prdd_large = sc.range(0, 32, numSlices=32). \
    map(lambda x: pd.DataFrame(np.random.randint(0, 100, size=(40 << 15, 4)), columns=list('ABCD')))

df = spark.createDataFrame(prdd, schema=None, pandasRDD=True)
df.toPandas()
```
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->

<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->

<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->


### How was this patch tested?
Added a new test using for creating a spark DF from an RDD of pandas dataframes. 
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->
",https://api.github.com/repos/apache/spark/issues/29719/timeline,,spark,apache,linar-jether,11908736,MDQ6VXNlcjExOTA4NzM2,https://avatars.githubusercontent.com/u/11908736?v=4,,https://api.github.com/users/linar-jether,https://github.com/linar-jether,https://api.github.com/users/linar-jether/followers,https://api.github.com/users/linar-jether/following{/other_user},https://api.github.com/users/linar-jether/gists{/gist_id},https://api.github.com/users/linar-jether/starred{/owner}{/repo},https://api.github.com/users/linar-jether/subscriptions,https://api.github.com/users/linar-jether/orgs,https://api.github.com/users/linar-jether/repos,https://api.github.com/users/linar-jether/events{/privacy},https://api.github.com/users/linar-jether/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/29719,https://github.com/apache/spark/pull/29719,https://github.com/apache/spark/pull/29719.diff,https://github.com/apache/spark/pull/29719.patch,,https://api.github.com/repos/apache/spark/issues/29719/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
243,https://api.github.com/repos/apache/spark/issues/29330,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/29330/labels{/name},https://api.github.com/repos/apache/spark/issues/29330/comments,https://api.github.com/repos/apache/spark/issues/29330/events,https://github.com/apache/spark/pull/29330,671807202,MDExOlB1bGxSZXF1ZXN0NDYxOTUzMTc2,29330,[SPARK-32432][SQL] Added support for reading ORC/Parquet files with SymlinkTextInputFormat,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,19,2020-08-03T05:36:20Z,2021-11-11T00:01:45Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->
This pull-request is to add support for reading ORC/Parquet files with SymlinkTextInputFormat in Apache Spark.

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->
Hive style symlink (SymlinkTextInputFormat) is commonly used in different analytic engines including prestodb and prestosql.
Currently SymlinkTextInputFormat works with JSON/CSV files but does not work with ORC/Parquet files in Apache Spark (and Apache Hive).
On the other hand, prestodb and prestosql support SymlinkTextInputFormat with ORC/Parquet files.
This pull-request is to add support for reading ORC/Parquet files with SymlinkTextInputFormat in Apache Spark.

See details in the JIRA.  SPARK-32432

### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->
Yes.
Currently Spark returns exceptions if users try to use SymlinkTextInputFormat with ORC/Parquet files.
With this patch, Spark can handle symlink which indicates locations of ORC/Parquet files.

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->
I added a new test suite `SymlinkSuite` and confirmed it passed.

```
$ ./build/sbt ""project hive"" ""test-only org.apache.spark.sql.hive.SymlinkSuite""
```",https://api.github.com/repos/apache/spark/issues/29330/timeline,,spark,apache,moomindani,1304020,MDQ6VXNlcjEzMDQwMjA=,https://avatars.githubusercontent.com/u/1304020?v=4,,https://api.github.com/users/moomindani,https://github.com/moomindani,https://api.github.com/users/moomindani/followers,https://api.github.com/users/moomindani/following{/other_user},https://api.github.com/users/moomindani/gists{/gist_id},https://api.github.com/users/moomindani/starred{/owner}{/repo},https://api.github.com/users/moomindani/subscriptions,https://api.github.com/users/moomindani/orgs,https://api.github.com/users/moomindani/repos,https://api.github.com/users/moomindani/events{/privacy},https://api.github.com/users/moomindani/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/29330,https://github.com/apache/spark/pull/29330,https://github.com/apache/spark/pull/29330.diff,https://github.com/apache/spark/pull/29330.patch,,https://api.github.com/repos/apache/spark/issues/29330/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
244,https://api.github.com/repos/apache/spark/issues/28642,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/28642/labels{/name},https://api.github.com/repos/apache/spark/issues/28642/comments,https://api.github.com/repos/apache/spark/issues/28642/events,https://github.com/apache/spark/pull/28642,624755391,MDExOlB1bGxSZXF1ZXN0NDIzMTAyMzI2,28642,[SPARK-31809][SQL] Infer IsNotNull from some special equality join keys,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1982260031, 'node_id': 'MDU6TGFiZWwxOTgyMjYwMDMx', 'url': 'https://api.github.com/repos/apache/spark/labels/PYTHON', 'name': 'PYTHON', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,32,2020-05-26T09:52:56Z,2021-11-01T00:45:49Z,,MEMBER,,False,"### What changes were proposed in this pull request?

We can infer `IsNotNull` from some special equality join keys. For example:
```sql
CREATE TABLE t1(a string, b string, c string) using parquet;
CREATE TABLE t2(a string, b decimal(38, 18), c string) using parquet;
SELECT t1.* FROM t1 JOIN t2 ON coalesce(t1.a, t1.b)=t2.a; -- case 1
SELECT t1.* FROM t1 JOIN t2 ON CAST(t1.a AS DOUBLE)=CAST(t2.b AS DOUBLE); -- case 2
```
The `coalesce(t1.a, t1.b)` or `CAST(t1.a AS DOUBLE)` may generate a lot of null values, which will lead to skew join.
After this pr:
```
== Physical Plan ==
*(5) Project [a#5, b#6, c#7]
+- *(5) SortMergeJoin [coalesce(a#5, b#6)], [a#8], Inner
   :- *(2) Sort [coalesce(a#5, b#6) ASC NULLS FIRST], false, 0
   :  +- Exchange hashpartitioning(coalesce(a#5, b#6), 200), true, [id=#44]
   :     +- *(1) Filter isnotnull(coalesce(a#5, b#6))
   :        +- Scan hive default.t1 [a#5, b#6, c#7], HiveTableRelation `default`.`t1`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [a#5, b#6, c#7], Statistics(sizeInBytes=8.0 EiB)
   +- *(4) Sort [a#8 ASC NULLS FIRST], false, 0
      +- Exchange hashpartitioning(a#8, 200), true, [id=#52]
         +- *(3) Filter isnotnull(a#8)
            +- Scan hive default.t2 [a#8], HiveTableRelation `default`.`t2`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [a#8, b#9, c#10], Statistics(sizeInBytes=8.0 EiB)
```

### Why are the changes needed?

1. Avoid skew join in some cases.
2. [Hive support this optimization](https://github.com/apache/hive/blob/rel/release-3.1.2/ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveJoinAddNotNullRule.java).


### Does this PR introduce _any_ user-facing change?

No.

### How was this patch tested?

Unit test and benchmark test:
Case1:
Before this PR | After this PR
-- | --
![image](https://issues.apache.org/jira/secure/attachment/13003914/13003914_default.png) | ![image](https://issues.apache.org/jira/secure/attachment/13003913/13003913_infer.png)

Case2:
Before this PR | After this PR
-- | --
![image](https://user-images.githubusercontent.com/5399861/128879249-f08c0577-caf7-422f-b25c-f47113cc5793.png) | ![image](https://user-images.githubusercontent.com/5399861/128879432-eff937d2-999b-4ac8-a216-25b40e093b67.png)
",https://api.github.com/repos/apache/spark/issues/28642/timeline,,spark,apache,wangyum,5399861,MDQ6VXNlcjUzOTk4NjE=,https://avatars.githubusercontent.com/u/5399861?v=4,,https://api.github.com/users/wangyum,https://github.com/wangyum,https://api.github.com/users/wangyum/followers,https://api.github.com/users/wangyum/following{/other_user},https://api.github.com/users/wangyum/gists{/gist_id},https://api.github.com/users/wangyum/starred{/owner}{/repo},https://api.github.com/users/wangyum/subscriptions,https://api.github.com/users/wangyum/orgs,https://api.github.com/users/wangyum/repos,https://api.github.com/users/wangyum/events{/privacy},https://api.github.com/users/wangyum/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/28642,https://github.com/apache/spark/pull/28642,https://github.com/apache/spark/pull/28642.diff,https://github.com/apache/spark/pull/28642.patch,,https://api.github.com/repos/apache/spark/issues/28642/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
245,https://api.github.com/repos/apache/spark/issues/28032,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/28032/labels{/name},https://api.github.com/repos/apache/spark/issues/28032/comments,https://api.github.com/repos/apache/spark/issues/28032/events,https://github.com/apache/spark/pull/28032,588181296,MDExOlB1bGxSZXF1ZXN0MzkzOTc3MjM5,28032,[SPARK-31264][SQL] Repartition before writing data source tables/directories,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,46,2020-03-26T06:12:14Z,2021-11-14T07:23:08Z,,MEMBER,,True,"### What changes were proposed in this pull request?

This PR adds a new rule `RepartitionWritingDataSource` to support repartitioning before writing data.  It supports three patterns:
- Repartition by none when writing normal tables/directories to reduce small files.
- Repartition by dynamic partition column when writing dynamic partition tables/directories to reduce small files because a single map task may contains many dynamic partition values.
- Repartition by bucket column with bucket number and sort by sort column when writing bucket tables/directories.

We only support data source tables/directories because [it cannot fully support Hive tables](https://github.com/apache/spark/blob/9f7cdb89f7614ef38ba2d8877d4c0f87ad9b6f5f/sql/hive/src/main/scala/org/apache/spark/sql/hive/execution/InsertIntoHiveTable.scala#L182-L188).

Hive has a similar rule: [SortedDynPartitionOptimizer](https://github.com/apache/hive/blob/917221e8378ec48ea05ef6b6c7d9515609b8ec01/ql/src/java/org/apache/hadoop/hive/ql/optimizer/SortedDynPartitionOptimizer.java).


### Why are the changes needed?
1. To reduce generating small files.
2. To ease pressure on the NameNode and improve insert dynamic partition table/directory performance.
   Spark job failed  because too many data blocks were created | HDFS data block monitor
   -- | --
   ![image](https://user-images.githubusercontent.com/5399861/77612149-62020880-6f62-11ea-8b2f-dfd46d0fc5a6.png) | ![image](https://user-images.githubusercontent.com/5399861/77612239-9bd30f00-6f62-11ea-9178-3bcd65aa4034.png)



### Does this PR introduce any user-facing change?
No.


### How was this patch tested?
Unit test and benchmark test:
Query | Before this PR | After this PR
-- | -- | --
CREATE TABLE t1 USING parquet   PARTITIONED BY (p1, p2) AS (SELECT id, id % 1000 AS p1, id % 10000 AS p2 FROM   range(5000000)) | 15 min | 1.1 min

",https://api.github.com/repos/apache/spark/issues/28032/timeline,,spark,apache,wangyum,5399861,MDQ6VXNlcjUzOTk4NjE=,https://avatars.githubusercontent.com/u/5399861?v=4,,https://api.github.com/users/wangyum,https://github.com/wangyum,https://api.github.com/users/wangyum/followers,https://api.github.com/users/wangyum/following{/other_user},https://api.github.com/users/wangyum/gists{/gist_id},https://api.github.com/users/wangyum/starred{/owner}{/repo},https://api.github.com/users/wangyum/subscriptions,https://api.github.com/users/wangyum/orgs,https://api.github.com/users/wangyum/repos,https://api.github.com/users/wangyum/events{/privacy},https://api.github.com/users/wangyum/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/28032,https://github.com/apache/spark/pull/28032,https://github.com/apache/spark/pull/28032.diff,https://github.com/apache/spark/pull/28032.patch,,https://api.github.com/repos/apache/spark/issues/28032/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
